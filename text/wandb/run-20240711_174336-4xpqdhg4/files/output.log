/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(

Training Text Model:   0%|                     | 5/4114 [00:03<30:18,  2.26it/s]
NaN loss detected at iteration 0
input_ids: tensor([[  101, 10167, 11906,  ...,     0,     0,     0],
        [  101, 15595,   122,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 13483,  ...,     0,     0,     0],
        [  101, 10167, 46503,  ...,     0,     0,     0],
        [  101, 13830, 71007,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.4751,  0.0712,  0.8907,  0.8907,  1.1638, -0.7483, -1.2946, -1.0214,
         1.4370, -0.7483,  0.3443,  0.6175,  1.9833,  0.0712,  1.1638,  0.8907],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 84.3917, 115.2842,  60.8241,  47.5972,  61.4850,  89.6053,  66.2478,
         70.6738,  11.2538,  90.8130, 121.7178,  84.2773,  72.0208,  97.7242,
         71.6320,  82.7975], device='cuda:0')
NaN loss detected at iteration 1
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 63672, 10342,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 15936,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830,   187,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712, -1.2946,  0.3443, -0.4751, -1.2946, -1.2946, -1.2946,  0.8907,
         0.8907,  1.4370, -1.0214, -1.2946, -1.2946,  0.3443, -0.7483,  0.0712],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([131.1357, 123.5231,  60.8241,  66.8451,  51.1162, 105.1368, 115.6987,
         93.8697,  69.1885, 146.8248, 127.0022,  51.1162, 120.4494,  82.3400,
          0.3890,  67.0275], device='cuda:0')
NaN loss detected at iteration 2
input_ids: tensor([[  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 88285, 10136,  ...,     0,     0,     0],
        [  101, 10796, 54963,  ..., 33989,   135,   102],
        ...,
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946, -1.0214,  0.8907,  0.8907, -0.4751, -0.2020,  0.8907, -1.2946,
        -0.4751,  1.4370,  0.3443, -1.2946,  0.3443,  0.6175,  0.0712, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 51.1162,  81.6155,  57.7118,  61.3392,  73.4204, 129.4080,  93.8697,
         59.1710,  50.5625,  35.3669,  63.6273,  95.5450,  38.3240,  54.7379,
         50.1364,  86.4483], device='cuda:0')
NaN loss detected at iteration 3
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 42429, 20565,   102],
        [  101, 15595, 10268,  ...,   120,   135,   102],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 16045, 67099,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443,  0.3443,  0.6175, -1.2946,  0.0712, -1.0214,  0.0712,  0.6175,
        -0.2020,  0.3443,  0.0712,  1.1638,  0.8907,  0.3443, -1.2946, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 99.5648, 111.8594, 119.7666, 107.8155,  78.5566, 101.1675,  91.5499,
         45.2776, 125.5398, 116.5817,  77.1669,  74.3726,  55.7802,  79.9334,
        155.5930,  66.2478], device='cuda:0')
NaN loss detected at iteration 4
input_ids: tensor([[  101, 13209, 11479,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 10329, 27426,   102],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        ...,
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],

Training Text Model:   0%|                    | 12/4114 [00:05<20:57,  3.26it/s]
keyword_count: tensor([-1.0214,  1.1638, -0.2020, -1.2946,  0.0712,  0.3443, -0.2020, -0.4751,
        -0.4751,  0.3443,  0.8907,  0.6175,  0.6175,  1.1638,  1.7101, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([155.2857,  90.6265,  84.2235, 133.1675, 123.0521,  53.9188,  88.8247,
         84.2324, 121.3180,  84.4694,  95.5376, 234.1734,  59.8845,  70.7460,
        104.2360, 116.2032], device='cuda:0')
NaN loss detected at iteration 5
input_ids: tensor([[  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 10745,  ..., 29786,   117,   102],
        [  101, 13338, 49316,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2.5296,  0.3443,  0.0712, -1.0214, -1.2946,  3.3491, -0.2020, -1.2946,
        -0.2020, -1.2946, -0.4751,  0.8907,  0.0712,  0.6175,  3.0759, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 35.3669,  71.4339, 107.3419,  71.3136, 116.3786, 131.9325,  72.1048,
         66.2478,  73.1039,  49.0622, 132.6302,  40.0226, 109.2685,  73.1039,
        131.9325,  88.6680], device='cuda:0')
NaN loss detected at iteration 6
input_ids: tensor([[   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  11500,  11784,  ...,      0,      0,      0],
        [   101,  10167,  10745,  ...,      0,      0,      0],
        ...,
        [   101,  13338,  49316,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443, -1.0214,  0.0712, -0.2020,  0.0712, -1.2946, -0.4751, -1.0214,
        -0.2020,  0.3443,  2.8027, -0.2020, -0.7483, -1.2946, -0.2020, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 64.0918,  84.2324, 118.8928,  63.7042,  89.8892, 106.1893,  72.2873,
         86.4483, 113.6805,  67.3493, 120.5129,  98.7769, 103.2900, 101.9981,
         61.0732, 176.1565], device='cuda:0')
NaN loss detected at iteration 7
input_ids: tensor([[  101, 10167, 95230,  ...,     0,     0,     0],
        [  101, 22034, 10307,  ..., 11446,   119,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907,  0.6175,  1.4370,  1.1638,  0.0712, -1.2946, -0.2020, -1.2946,
         0.0712,  0.0712,  0.8907,  0.3443, -1.2946,  0.6175, -0.4751,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 69.7060,  72.0955,  83.7236, 120.0502,  51.3670,  88.6680, 109.2680,
         46.7206,  61.3055,  79.8238,  99.0409,  71.3136, 109.4364,  80.9500,
         93.8697, 104.8592], device='cuda:0')
NaN loss detected at iteration 8
input_ids: tensor([[  101, 14183, 10118,  ..., 10329, 11583,   102],
        [  101, 10236, 92746,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0],
        [  101, 13209, 11479,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712,  0.0712, -0.4751, -0.7483,  0.3443,  0.6175, -0.2020, -0.7483,
        -1.2946,  0.0712, -0.4751, -0.4751,  0.6175, -0.2020,  1.1638, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 90.5470, 104.2360,  90.6385,  72.0955,  69.8807,  61.0244, 150.2645,
         63.0834, 116.2032,  64.8453,  65.7311,  36.2959,  80.9284,  78.4014,
        119.4443,  81.8661], device='cuda:0')
NaN loss detected at iteration 9
input_ids: tensor([[  101, 10167, 11906,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        ...,
        [  101,   115, 12664,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101,   138, 15864,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.1638,  0.6175,  0.6175, -0.4751, -1.0214, -0.7483, -1.2946,  0.0712,
        -0.4751, -1.0214, -0.4751, -0.7483,  0.6175,  0.0712, -0.4751, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([109.2685,  86.5803,  97.2488,  77.6442,  68.5316,  77.1669,  51.1162,
        116.0685,  84.3917, 126.2878, 125.6128,  71.1369,  60.1338,  92.2559,
        101.8930, 123.5018], device='cuda:0')
NaN loss detected at iteration 10
input_ids: tensor([[  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 23118, 11583,  ...,   182, 13034,   102],
        ...,
        [  101, 11045, 42169,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946,  0.8907,  1.7101,  1.1638,  0.3443, -1.2946,  1.4370,  0.0712,
        -0.4751,  0.0712, -0.4751, -1.2946,  0.0712, -1.2946, -1.0214, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([100.8273, 183.3395,  72.1763, 107.3977,  68.3160, 154.8282, 103.7605,
         44.7106,  63.7042, 106.1335,  73.4204,  94.3045,  84.3884,  59.4358,

Training Text Model:   0%|                    | 19/4114 [00:07<19:46,  3.45it/s]
NaN loss detected at iteration 11
input_ids: tensor([[   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  10313,  77807,  ...,      0,      0,      0],
        [   101,    149,    112,  ...,      0,      0,      0],
        ...,
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  13830,  79427,  ...,      0,      0,      0],
        [   101,  10167,  10599,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946,  0.6175, -1.2946,  0.6175, -0.2020,  0.0712,  0.0712, -0.7483,
         2.2564,  0.3443, -1.2946, -0.7483, -1.2946,  0.0712,  0.8907, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 91.1428,  42.0238, 105.6918, 102.2099, 147.6293, 117.8280, 135.9242,
        105.5885,  97.7242,  89.2187,  31.5023, 133.5628, 133.1675,  52.6674,
         98.4367, 134.4974], device='cuda:0')
NaN loss detected at iteration 12
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 10167, 10118,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907,  0.6175,  0.0712, -0.2020,  3.0759,  1.4370,  0.6175, -1.0214,
         1.9833, -1.2946,  0.3443,  0.3443, -0.7483,  0.6175, -0.2020, -0.4751],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([109.2685,  36.2959,  78.5566,  47.1133,  37.1506,  32.5057, 134.1044,
        116.2032,  38.8275,  60.1122, 135.9242, 135.6691,  46.7206,  58.0071,
         69.8299,  46.8751], device='cuda:0')
NaN loss detected at iteration 13
input_ids: tensor([[  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 10788,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,   148, 20288,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.2020,  0.0712, -0.7483,  0.0712, -0.4751,  1.4370, -0.4751,  0.0712,
         0.6175, -0.2020,  2.2564, -0.2020, -1.2946,  0.0712,  2.8027,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 87.0709,  79.9334,  85.7800,  87.6992, 105.0821,  38.9052,  65.6698,
         72.0955,  79.9334,  73.1039,  66.6077,  49.7804, 133.1675,  53.9188,
        120.5129, 146.7394], device='cuda:0')
NaN loss detected at iteration 14
input_ids: tensor([[   101,  10167,  15826,  ...,      0,      0,      0],
        [   101,  10313,  13483,  ...,      0,      0,      0],
        [   101,  13338,  12028,  ...,      0,      0,      0],
        ...,
        [   101,  30016,  14842,  ...,      0,      0,      0],
        [   101,  13830, 101328,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712,  0.0712, -1.2946,  0.3443, -1.0214,  0.3443, -1.0214, -0.4751,
        -1.2946, -0.7483,  1.9833,  0.3443, -1.2946,  1.4370,  0.8907, -0.4751],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 92.6176,  89.3209,  46.3732,  76.8847,  77.0196, 119.9655, 102.5639,
         91.9994, 164.5873,  48.8803,  35.3669, 134.9760,  61.2797,  56.0026,
         71.4016,  56.8349], device='cuda:0')
NaN loss detected at iteration 15
input_ids: tensor([[  101, 10796, 26130,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        ...,
        [  101, 13209, 11479,  ...,     0,     0,     0],
        [  101, 11583, 10762,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.7483,  1.1638, -1.2946,  1.4370,  1.4370, -1.2946,  3.0759, -0.7483,
        -0.4751,  1.4370, -0.4751,  2.2564,  1.9833, -1.2946, -0.4751, -0.7483],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([157.4288,  72.1763, 103.4801, 109.3474,  60.3016, 190.5930,  37.1506,
         92.2002,  77.0785, 114.1950,  46.9639,  47.6180,  34.8087, 116.3786,
        103.0684,  77.0490], device='cuda:0')
NaN loss detected at iteration 16
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 52208, 24931,  ...,     0,     0,     0],
        [  101, 43690, 47314,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 46503,  ...,     0,     0,     0],
        [  101, 10167, 10118,  ...,     0,     0,     0],
        [  101, 11699, 13207,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712, -1.2946, -1.0214, -1.2946,  0.0712, -0.2020,  0.0712,  1.4370,
         0.0712, -0.4751, -1.2946, -0.7483,  1.9833,  1.7101, -1.0214, -0.4751],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 61.0732, 166.2110,  42.4238, 149.5067,  50.1364, 143.5192,  51.1265,
        109.2685, 102.5554, 134.4974, 105.6918, 108.6525, 119.4386,  47.2084,
        132.6302, 103.7605], device='cuda:0')
NaN loss detected at iteration 17
input_ids: tensor([[  101, 10796, 48480,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 10250,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443, -0.2020, -1.2946, -1.0214,  0.8907, -1.2946, -1.0214,  1.1638,
         1.7101,  0.0712, -0.7483,  1.9833, -0.2020, -0.7483,  0.6175, -0.4751],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 97.3242,  89.3209,  66.3063,  71.3136,  57.1543, 123.5018, 215.5236,
        109.5676, 132.5271,  84.6530, 103.2843,  85.8097,  67.3493,  77.1669,

Training Text Model:   1%|▏                   | 26/4114 [00:09<19:36,  3.48it/s]
NaN loss detected at iteration 18
input_ids: tensor([[  101, 22034, 10307,  ..., 25506, 12244,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        ...,
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 27241, 37135,  ...,   136,   165,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907,  1.7101, -1.2946, -1.2946,  0.3443,  3.6222, -0.7483, -0.2020,
        -0.7483,  0.3443,  1.1638,  0.3443, -0.2020, -1.0214, -0.4751, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 72.0955,  78.4014, 126.1645,  46.3732,  71.4302,  60.9738,  72.0955,
        155.4030,  71.1369,  84.5759, 120.0502,  53.8809,  90.9842,  74.3483,
         74.2130,  67.3493], device='cuda:0')
NaN loss detected at iteration 19
input_ids: tensor([[  101, 10167, 10599,  ...,     0,     0,     0],
        [  101, 10236, 39710,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 12080, 73037,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443,  0.0712, -1.0214, -0.7483,  0.6175, -0.7483,  0.6175,  0.3443,
        -1.0214,  0.0712,  1.1638, -1.2946,  0.0712,  0.8907,  0.3443, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 88.0273, 110.8742,  73.2969,  69.3990,  79.2498,  93.5499,  37.2631,
         32.1372,  87.1285,  43.4819,  35.3669, 102.7199, 113.4881,  66.6077,
        136.8724, 109.9162], device='cuda:0')
NaN loss detected at iteration 20
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 10236, 39710,  ...,     0,     0,     0],
        [  101,   149,   112,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907,  1.4370,  0.3443, -1.2946,  0.6175,  0.8907,  0.3443,  1.1638,
         1.1638, -0.4751,  0.0712, -0.4751, -1.2946,  1.4370, -1.2946, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 38.7441,  38.8275,  52.0002, 154.3134,  68.3160,  79.9334,  88.4059,
         55.7802,  50.4023,  90.6385,  89.8892, 102.5639, 124.6094,  38.8275,
        134.4576, 113.4881], device='cuda:0')
NaN loss detected at iteration 21
input_ids: tensor([[   101,  11982,  66593,  ...,  11583,  10492,    102],
        [   101,  43690,  18460,  ...,      0,      0,      0],
        [   101,  13830,    123,  ...,      0,      0,      0],
        ...,
        [   101,  10167,  46503,  ...,      0,      0,      0],
        [   101,  64766, 109067,  ...,      0,      0,      0],
        [   101,  22034,  10307,  ...,  12908,  12028,    102]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 1.1638, -1.2946, -0.7483,  1.7101,  0.0712, -1.2946, -0.7483, -1.2946,
         1.7101,  0.3443, -0.2020,  0.0712, -0.4751,  1.9833, -1.2946,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 71.8483, 135.2085,  47.3696, 121.8980,  83.4905, 131.2599, 123.1118,
         83.7253,  47.2084,  61.7175,  80.4560, 116.7333,  52.9569,  35.3669,
         47.5635,  72.0955], device='cuda:0')
NaN loss detected at iteration 22
input_ids: tensor([[  101, 13029, 10119,  ...,     0,     0,     0],
        [  101,   138, 33939,  ...,     0,     0,     0],
        [  101, 13830, 46503,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 10599,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946, -1.2946, -1.0214, -0.7483,  1.9833,  0.6175, -1.2946,  0.6175,
         0.8907,  0.6175,  2.2564, -1.2946, -1.0214,  1.4370,  2.2564,  1.1638],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([116.2032, 109.9162,  72.6598,  83.9489,  88.8918,  72.1763,  55.7672,
        113.3749,  43.8032,  74.9864, 103.7605, 101.1622, 162.6413, 109.5676,
         72.1763,  45.2776], device='cuda:0')
NaN loss detected at iteration 23
input_ids: tensor([[  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 39710,  ...,     0,     0,     0],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.4751, -0.7483, -1.0214, -1.2946,  0.3443, -0.7483, -1.2946,  1.7101,
        -0.2020, -0.4751, -0.2020, -0.2020,  2.8027,  0.6175, -1.2946,  1.1638],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 86.4483,  71.1369,  86.4483,  75.0149,  43.8032,   0.3890,  72.6598,
         57.6628,  55.4237, 118.0745, 103.8811,  72.0208,  57.6578, 119.9655,
        128.6350, 120.0502], device='cuda:0')
NaN loss detected at iteration 24
input_ids: tensor([[  101,   146, 29239,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830, 10788,  ..., 14321, 40581,   102],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 52208, 24931,  ...,     0,     0,     0],
        [  101, 11699, 13207,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946,  0.0712,  0.0712, -1.2946, -0.2020, -0.2020, -0.7483, -0.2020,
        -0.7483, -1.0214,  0.6175, -0.2020, -1.2946,  1.1638, -1.2946, -0.4751],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 66.3855,  86.4875,  53.9188,  66.3446,  79.9936, 139.4378, 150.7184,
         90.9842,  83.9489,  68.5316,  74.9581, 103.7950,  67.3036, 183.3395,

Training Text Model:   1%|▏                   | 33/4114 [00:11<19:29,  3.49it/s]
NaN loss detected at iteration 25
input_ids: tensor([[  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 51732, 76002,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 79427,  ...,     0,     0,     0],
        [  101, 13029, 10231,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.6175, -0.2020,  0.8907, -1.2946,  1.7101,  0.3443, -1.0214,  1.1638,
         0.8907,  1.4370,  0.0712, -0.2020, -0.4751, -1.2946, -1.2946,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 86.5803,  68.5316, 134.4974, 128.6350,  57.6628, 130.4891, 119.0336,
         96.0295,  38.3240,  38.8275,  61.3776,  36.2959,  40.4214, 154.8282,
        118.0590,  88.4059], device='cuda:0')
NaN loss detected at iteration 26
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 29224, 21469,  ...,   118, 13077,   102],
        ...,
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 11583, 10762,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,   120,   135,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 0.3443, -0.4751, -0.2020,  0.8907, -1.0214, -1.2946,  0.0712, -0.7483,
         0.3443, -1.2946, -1.2946,  1.1638,  0.8907,  0.3443, -0.4751, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 86.5803,  91.9994,  34.6186,  72.0955, 119.8600,  83.7253, 101.8737,
         77.4418, 140.5139, 156.7790, 101.9981,  26.2954, 108.4150,  52.9115,
         73.3526, 129.1985], device='cuda:0')
NaN loss detected at iteration 27
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10445,   155,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0],
        ...,
        [  101, 10236, 39710,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ..., 10726, 15547,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 0.0712,  0.6175,  0.0712, -1.0214, -1.0214,  1.7101, -0.7483,  0.8907,
         0.6175,  0.0712,  0.0712, -1.0214,  1.1638,  0.0712, -0.4751,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 95.3006, 145.6586, 106.3260,  86.4483, 119.0336,  95.5376, 100.3695,
        139.4378,  61.3055,  94.4898,  89.3209,  80.3005, 107.3977,  40.9595,
         83.9148,  54.7379], device='cuda:0')
NaN loss detected at iteration 28
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10445, 13068,  ...,     0,     0,     0],
        [  101, 10313, 10118,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 22282,  ...,   119, 10672,   102],
        [  101, 30932, 10681,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.0214,  1.1638, -0.2020, -0.4751,  0.6175,  0.3443,  0.8907,  0.6175,
        -1.2946,  0.3443,  3.0759,  0.3443,  1.7101,  0.8907, -0.7483, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([139.0868,  39.1823, 101.1942,  95.3458,  38.9052,  56.1906,  72.0955,
        124.5378,  47.5635,  73.3689, 172.3459, 116.6227,  99.2452,  38.3240,
         65.6698,  64.9748], device='cuda:0')
NaN loss detected at iteration 29
input_ids: tensor([[  101, 10313, 10312,  ...,     0,     0,     0],
        [  101, 25894,   117,  ...,     0,     0,     0],
        [  101, 13830, 39710,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830, 17058,  ..., 11415, 90928,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 0.0712, -1.2946,  1.4370, -1.2946, -0.2020, -0.7483, -1.2946, -1.0214,
        -0.7483,  0.8907,  0.6175,  0.6175,  1.4370, -0.7483, -0.2020,  1.9833],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([162.6413,  88.6680, 119.9655, 101.9981,  98.7767,  72.0955,  89.3835,
         73.0038, 103.2843,  64.0918,  68.9206,  98.3503,  57.6628,  69.4673,
        137.4646,  47.6180], device='cuda:0')
NaN loss detected at iteration 30
input_ids: tensor([[  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 10211, 12226,   102],
        ...,
        [  101, 13029, 10141,  ...,     0,     0,     0],
        [  101, 10167, 11906,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.7483, -1.2946,  1.1638,  2.5296, -1.0214, -1.0214,  0.0712, -1.2946,
        -1.2946, -1.2946, -0.4751,  0.3443, -1.0214, -1.2946,  0.0712, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 62.9688,  59.1710,  85.7659, 120.5129,  88.6680, 108.4471, 104.7439,
        117.2016, 108.6488, 105.1368, 104.6255,  97.2488,  91.0425, 109.3336,
        113.4881,  73.0038], device='cuda:0')
NaN loss detected at iteration 31
input_ids: tensor([[  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 63672, 10342,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 84051,  ...,     0,     0,     0],
        [  101, 10236,   124,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712, -1.2946,  0.6175,  0.3443,  0.6175,  0.0712,  1.7101, -1.2946,
        -1.2946,  0.0712,  0.3443, -0.7483, -0.4751, -0.7483,  0.0712,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([104.8592,  61.3225,  65.8420,  97.2488,  91.2221,  52.6674,  99.9530,
        110.8538, 155.9315, 146.5804, 102.5639,  71.4609,  84.2324,  72.9671,
         71.1369, 169.5490], device='cuda:0')
NaN loss detected at iteration 32
input_ids: tensor([[  101, 13068, 11369,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 71164, 48865,  ...,     0,     0,     0],
        ...,
        [  101, 14563, 10162,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.4370, -0.2020, -1.2946,  0.3443, -0.4751, -0.2020, -1.2946, -0.4751,
        -1.2946, -1.2946, -0.2020,  0.6175,  0.0712,  1.1638, -1.2946, -0.7483],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 60.2439, 147.6666,  86.9333,  68.3160,  89.3209, 105.8216,  59.1710,
        137.1208, 150.0111,  20.5048,  80.4560, 112.7749,  67.3493,  72.1763,

Training Text Model:   1%|▏                   | 40/4114 [00:13<19:27,  3.49it/s]
NaN loss detected at iteration 33
input_ids: tensor([[   101,  43690,  18460,  ...,      0,      0,      0],
        [   101,  10313,  22282,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        ...,
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,  10147, 104278,    102]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([-1.0214,  0.6175, -0.4751, -1.0214, -1.0214,  0.0712, -1.0214,  1.9833,
        -0.2020,  0.3443, -0.7483, -0.4751,  0.6175, -0.4751, -0.7483,  1.9833],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([100.0854,  66.1847,  91.9116, 108.9287,  74.7294,  51.1265, 114.9781,
         42.4782, 123.0521,  99.5648,  72.0955,  65.8420,  62.4533,  39.0712,
         49.0622,  34.8087], device='cuda:0')
NaN loss detected at iteration 34
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 11049,  ...,     0,     0,     0],
        [  101, 10236, 39710,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0],
        [  101, 13830, 61694,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2.2564, -0.2020,  0.0712,  1.9833, -1.0214, -0.2020, -1.0214,  2.5296,
         0.3443,  1.1638, -0.7483,  1.7101,  0.3443,  2.8027,  1.9833,  0.8907],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 56.0026,  69.7060,  40.9595,  85.8097,  60.3315, 109.2680, 135.2085,
        120.5129,  62.9535,  77.0089, 119.1181, 115.3175, 101.7411,  97.7242,
         97.0673, 128.0522], device='cuda:0')
NaN loss detected at iteration 35
input_ids: tensor([[   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  10167,  19985,  ...,      0,      0,      0],
        ...,
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  43690,  18460,  ...,      0,      0,      0],
        [   101,  13830,  20560,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946, -0.7483,  0.3443, -0.4751,  0.3443, -0.4751, -0.7483,  1.1638,
        -0.4751, -1.2946,  0.3443, -1.2946,  0.0712,  0.0712, -1.2946, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([143.0837,  50.5625,  87.0025,  82.0496,  61.3055, 108.7674,  83.7447,
         61.4850, 100.5875, 175.4032,  61.3055,  67.3036,  97.2488,  65.7311,
         59.4358,  81.6155], device='cuda:0')
NaN loss detected at iteration 36
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 10796, 65817,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11982, 10250,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 14465, 26127,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 1.1638,  0.6175,  0.0712,  0.6175, -1.2946,  0.0712,  0.0712, -0.4751,
         1.1638, -1.2946, -1.2946,  1.4370, -0.2020,  1.9833,  0.6175,  1.4370],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 87.7927,  89.2187,  80.2644, 109.2685, 133.7706,  76.1385,  91.9690,
        148.5743, 121.4536,  58.3448, 113.6154,  38.8275,  76.3202,  57.3288,
         86.5803,  38.8275], device='cuda:0')
NaN loss detected at iteration 37
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 10313, 13483,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ..., 42474,   119,   102],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 25894,   117,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907,  0.6175, -0.2020,  0.0712, -0.4751,  0.8907, -0.4751, -0.4751,
        -1.2946,  1.1638, -0.2020, -1.2946, -0.4751,  0.3443,  0.0712, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 73.1039, 119.9655,  87.1531,  36.2959,  78.4963, 121.8980, 162.6413,
        118.0745, 164.5873,  43.7226,  53.4073, 162.6413, 140.9074, 113.4881,
         88.8247,  60.3315], device='cuda:0')
NaN loss detected at iteration 38
input_ids: tensor([[   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  21851,  72894,  ...,      0,      0,      0],
        [   101,  51732,  10762,  ...,      0,      0,      0],
        ...,
        [   101,  13029,  10119,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  13068,  11369,  ...,  10167,  12754,    102]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([-0.7483, -1.0214,  0.0712,  0.0712, -1.0214, -0.4751, -1.2946, -1.0214,
         0.8907,  2.8027, -0.2020,  0.3443,  1.4370, -1.2946, -1.2946,  1.4370],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 92.2002, 149.6065,  11.2538,  87.7494,  68.3160,  42.7689,  88.6680,
        119.0336,  94.7326, 120.5129, 103.8811,  71.6062,  38.9052, 151.5048,
         67.3036,  60.2439], device='cuda:0')
NaN loss detected at iteration 39
input_ids: tensor([[  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0],
        ...,
        [  101, 10236, 61694,  ...,     0,     0,     0],
        [  101, 10236, 20560,  ..., 12765, 14956,   102],
        [  101, 13338, 49316,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.0214,  0.8907, -0.4751, -1.2946, -0.2020,  0.3443,  0.8907, -1.2946,
        -0.2020,  0.8907, -1.2946,  0.6175,  3.0759,  0.8907,  1.4370, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([168.4754,  57.3288,  69.7060, 150.0111,  98.8139,  67.3493,  79.9334,
         46.3732,  69.8299,  79.9334, 142.8494, 112.7749,  37.1506, 113.4881,

Training Text Model:   1%|▏                   | 47/4114 [00:15<19:27,  3.48it/s]
NaN loss detected at iteration 40
input_ids: tensor([[  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 13830, 10788,  ...,     0,     0,     0],
        ...,
        [  101, 46361, 72384,  ...,     0,     0,     0],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712,  0.0712,  2.2564,  1.9833, -0.7483, -1.2946,  1.4370, -1.0214,
        -0.2020,  0.0712,  0.0712, -0.2020,  0.3443, -1.2946, -1.2946,  0.0712],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([102.5554, 119.1181, 103.7605,  35.3669, 121.5787,  46.3732,  60.2439,
        118.7314, 137.4646, 107.3329,  62.9535,  70.2360,  72.0955, 146.3655,
         82.3918, 119.1181], device='cuda:0')
NaN loss detected at iteration 41
input_ids: tensor([[  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 10313, 10118,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.2020,  0.3443,  1.1638,  1.1638,  0.0712,  0.3443,  3.0759, -1.2946,
        -1.2946,  0.0712, -0.4751,  0.3443, -1.2946, -0.4751,  0.0712,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([103.7605,  51.3670, 128.0522,  90.6265,  43.4819,  74.9581,  37.1506,
        132.8490, 101.9981,  37.2631,  77.6442,  69.5174,  83.7253, 112.1855,
        105.6866,  88.8918], device='cuda:0')
NaN loss detected at iteration 42
input_ids: tensor([[  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830,   124,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907, -1.2946, -1.2946,  2.2564,  2.8027, -0.2020,  0.0712,  1.1638,
         1.7101, -0.7483, -1.0214,  0.8907,  1.9833, -0.4751, -0.2020,  1.4370],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 40.0226,  88.6680, 113.6154, 103.7605, 120.5129,  69.0207,  72.3660,
         49.3355, 160.1040,  43.4819,  56.6746, 119.7666,  35.3669, 147.2216,
         80.4560,  38.7441], device='cuda:0')
NaN loss detected at iteration 43
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 13830, 10788,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11583, 10762,  ...,     0,     0,     0],
        [  101, 13830, 79427,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.4751,  0.6175,  0.3443, -0.2020,  1.4370,  0.8907,  0.3443, -1.2946,
         0.6175, -1.2946,  0.3443,  0.8907,  0.6175, -1.2946, -0.4751,  0.6175],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 76.3202, 119.1181, 101.9232,  36.2959,  43.8032, 122.7720,  48.8094,
         59.1710,  85.7659,  61.3225, 127.7731,  61.4287,  71.4016, 162.6413,
         73.3526,  48.9663], device='cuda:0')
NaN loss detected at iteration 44
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13029, 10152,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712, -1.2946,  0.6175,  0.3443,  0.0712, -1.2946, -0.2020,  0.0712,
         0.6175,  1.1638, -0.4751, -0.2020, -1.2946,  0.6175, -0.4751, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 93.8697, 121.1426,  45.0338,  82.3400, 145.2906, 200.6644,  72.2873,
        115.2842, 101.8587,  51.1236, 118.0745, 140.5139, 170.8714,  59.8845,
        124.7100,  69.7060], device='cuda:0')
NaN loss detected at iteration 45
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 10167, 11906,  ..., 10138, 54459,   102],
        ...,
        [  101, 15595, 66593,  ...,     0,     0,     0],
        [  101, 51732, 11049,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.2020,  1.1638,  1.4370, -1.2946,  2.5296, -0.4751, -0.2020,  2.2564,
         0.0712,  0.6175,  0.6175,  0.6175,  0.0712, -0.4751, -0.2020,  1.4370],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 47.1978,  81.6662,  61.4850,  72.8790,  94.6859,  14.1911,  87.9878,
         97.0673,  40.9595,  62.4533, 124.5391, 121.8980,  73.1039,  78.4963,
        102.5639, 109.5676], device='cuda:0')
NaN loss detected at iteration 46
input_ids: tensor([[   101,    150,  14902,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        ...,
        [   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  13830,  39710,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712, -0.4751, -1.0214, -1.2946, -0.4751,  1.7101,  1.9833, -0.7483,
         1.1638, -1.2946,  0.6175,  3.0759, -0.7483, -1.2946,  1.7101, -0.7483],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 51.7564,  71.1369,  67.8071,  62.9650,  61.8514,  92.0227, 132.5271,
         85.7035, 103.5517,  74.7294, 117.3433,  37.1506,  26.5983,  82.5600,

Training Text Model:   1%|▎                   | 54/4114 [00:17<19:38,  3.44it/s]
NaN loss detected at iteration 47
input_ids: tensor([[   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  71164,  48865,  ...,      0,      0,      0],
        [   101,  51732,  10762,  ...,      0,      0,      0],
        ...,
        [   101,  13209,  11170,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.7483, -1.2946,  1.1638,  0.6175,  0.0712, -1.2946, -0.4751,  0.6175,
         0.3443, -1.2946, -1.2946, -0.2020,  0.3443, -1.0214, -1.2946,  0.6175],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 61.0732, 164.5873,  35.3669,  59.8845,  51.9000, 135.2085,  94.3206,
        138.7497,  54.7379,  99.5779, 170.2543, 100.7400, 120.0502, 118.8089,
         59.1710,  59.8845], device='cuda:0')
NaN loss detected at iteration 48
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10313, 31300,  ...,     0,     0,     0],
        [  101, 10236, 79427,  ...,     0,     0,     0],
        ...,
        [  101, 13029, 18734,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13068, 11369,  ..., 52455, 10112,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 0.3443,  0.0712, -0.7483, -0.7483, -1.2946,  0.6175,  2.5296, -1.2946,
        -0.2020, -0.2020,  0.3443, -0.2020, -0.4751, -1.2946, -0.2020,  2.2564],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([107.6159,  26.8834,  88.0273,  84.2324, 110.4757, 119.9655, 120.5129,
        124.9767, 132.4450, 141.7580,  94.3206, 102.5639,  84.4694,  67.3036,
         74.9484,  60.2439], device='cuda:0')
NaN loss detected at iteration 49
input_ids: tensor([[  101, 10236, 39710,  ...,   120,   135,   102],
        [  101, 15595,   122,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ..., 46189, 46503,   102],
        ...,
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 13830, 39710,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.7101, -0.2020,  0.3443, -1.2946,  0.3443, -1.0214, -1.2946, -1.0214,
         2.2564,  0.3443, -1.2946,  0.8907, -0.4751,  0.0712, -1.2946,  0.0712],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 77.9310, 143.9861, 119.4386, 119.5634, 173.7271, 128.2466,  61.2797,
        119.0336,  77.6729,  61.0732,  86.7073, 134.5203,  77.1669, 129.2656,
        121.3755,  92.4465], device='cuda:0')
NaN loss detected at iteration 50
input_ids: tensor([[  101, 10313, 13483,  ...,     0,     0,     0],
        [  101, 24338, 10118,  ...,     0,     0,     0],
        [  101,   115, 24864,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 10762,  ..., 11157, 22331,   102],
        [  101, 10167, 32218,  ...,     0,     0,     0],
        [  101, 13068, 11369,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.2020,  0.3443, -0.2020,  1.4370,  1.1638,  0.3443,  0.3443,  0.3443,
        -0.4751, -0.7483, -0.7483, -1.0214,  2.2564,  2.5296, -0.4751,  1.7101],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([117.8280,  61.3055, 107.2845,  85.7659, 130.4891, 111.8594,  62.3935,
         71.9936,  34.9573, 116.2469,  65.6932, 121.3180,  35.3669,  52.9115,
         40.5938,  60.2439], device='cuda:0')
NaN loss detected at iteration 51
input_ids: tensor([[   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  11505,  19716,  ...,      0,      0,      0],
        ...,
        [   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  13029,  10119,  ...,      0,      0,      0],
        [   101,  61916,  41995,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.2020,  0.8907,  0.0712,  0.3443,  0.0712,  0.8907,  0.8907,  1.1638,
        -0.4751, -0.7483,  0.8907,  0.6175, -0.4751, -1.2946, -1.2946,  0.0712],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 97.3965, 172.9989, 126.7494,  67.3493,  92.4465,  79.9334,  89.9637,
         55.7802,  52.9569,  50.5625, 101.9698,  61.0732, 147.6293,  66.3063,
        147.8677, 124.5378], device='cuda:0')
NaN loss detected at iteration 52
input_ids: tensor([[  101, 51732, 16719,  ..., 13692,   117,   102],
        [  101, 51732, 16719,  ...,   119, 12468,   102],
        [  101, 10313, 13483,  ...,     0,     0,     0],
        ...,
        [  101, 13830,   125,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.7101,  1.1638,  0.8907,  1.4370,  0.3443,  0.3443, -1.0214, -0.7483,
         0.3443,  0.6175,  0.6175, -0.4751,  1.1638,  0.8907, -0.2020, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 57.6628,  83.7236,  56.1906,  38.8275,  72.2873,  57.6628,  85.9795,
         61.3055,  88.4059,  86.1742,  40.0226,  52.9569, 118.6371,  67.0071,
         69.0207,  93.1135], device='cuda:0')
NaN loss detected at iteration 53
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 22034, 10307,  ...,   113,   254,   102],
        [  101, 51732, 16719,  ..., 33989,   120,   102],
        ...,
        [  101, 10313, 13483,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ..., 26347, 10496,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 0.6175,  0.3443,  1.9833,  1.9833, -1.0214,  0.0712,  1.1638,  2.5296,
         0.0712,  0.3443, -1.0214,  0.0712, -1.0214, -0.4751,  0.6175,  1.9833],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([169.9366,  72.0955,  34.8087,  57.6628,  98.7408, 103.1871, 103.5517,
        121.4536,  56.4529,  71.4191, 135.2085,  86.5803, 168.4754,  77.6442,

Training Text Model:   1%|▎                   | 61/4114 [00:19<19:23,  3.48it/s]
NaN loss detected at iteration 54
input_ids: tensor([[  101, 13830, 20560,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.4751, -1.2946,  0.0712,  0.6175, -1.2946,  0.8907, -1.2946,  1.4370,
         0.6175, -1.2946,  0.8907, -1.2946, -1.2946, -1.2946,  0.3443, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 81.6155, 124.2625,  71.4191,  72.0955,  66.2478,  84.2773, 142.2642,
         72.1763, 122.6961, 121.6379, 119.9655, 164.5873,  60.1122,  78.4597,
        101.9668,  55.4237], device='cuda:0')
NaN loss detected at iteration 55
input_ids: tensor([[   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  13830, 101328,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        ...,
        [   101,  11505,  19716,  ...,      0,      0,      0],
        [   101,  10313,  77807,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907, -0.4751, -0.2020,  2.2564,  1.1638, -0.2020, -1.2946,  0.6175,
         1.1638,  0.0712, -0.2020, -0.2020, -1.2946,  0.8907,  0.0712,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 43.4819,  91.8406,  87.1531, 101.6304,  35.3669,  80.4560,  88.6680,
         74.9581,  85.7659,  42.4251,  77.7402,  69.0207, 115.6600, 119.1181,
        131.3147,  54.5772], device='cuda:0')
NaN loss detected at iteration 56
input_ids: tensor([[  101,   149,   112,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 51732, 11049,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.0214, -1.2946,  1.7101,  0.0712, -0.7483,  0.8907, -0.2020,  0.3443,
        -1.2946, -0.4751, -1.2946, -0.4751, -0.2020, -1.2946, -0.7483,  0.6175],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([113.6154, 135.2085,  48.8803,  47.1978,  87.6992,  40.0226, 101.9668,
         92.3754,  66.2478, 131.1357,  92.7448, 150.2645,  76.2296,  82.5600,
        151.8966,  68.3160], device='cuda:0')
NaN loss detected at iteration 57
input_ids: tensor([[  101, 11982,   122,  ..., 80206, 10136,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 27426, 15527,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,   133, 33989,   102],
        [  101, 51732, 10762,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2.8027, -0.7483,  0.3443, -1.2946,  0.0712, -0.2020,  0.3443,  1.1638,
        -0.2020,  1.1638, -1.2946,  1.1638,  0.0712,  0.8907,  0.6175, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([120.5129, 124.6951,  79.9334,  99.5081,  80.4560, 111.9527, 107.0300,
        111.8482,  67.3784,  81.6662, 121.1426,  72.1763,  84.3917, 126.7494,
         74.9581, 123.8212], device='cuda:0')
NaN loss detected at iteration 58
input_ids: tensor([[  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,   182, 13034,   102],
        ...,
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 10236, 10151,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.7483, -1.2946,  1.7101, -1.2946, -1.2946, -1.2946, -1.2946,  1.1638,
         2.2564,  0.3443, -1.0214,  0.3443, -0.4751, -0.4751,  0.0712,  0.0712],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 26.2954, 159.8013,  26.2954, 112.9437, 178.6319, 108.2465,  56.6746,
         57.7759,  35.3669,  99.9530,  64.9807, 120.1339,  94.9230, 156.7790,
         75.3021,  64.9748], device='cuda:0')
NaN loss detected at iteration 59
input_ids: tensor([[  101, 13830, 79427,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830,   124,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,   182, 11517,   102],
        [  101, 51732, 16719,  ..., 10240, 77574,   102],
        [  101, 13830, 61694,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907,  0.3443, -0.2020,  1.1638,  0.3443, -0.4751, -1.2946,  1.7101,
        -1.2946, -0.7483,  1.7101, -0.7483,  0.6175,  0.3443,  1.1638,  1.4370],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 80.3428, 129.8458,  80.4560, 129.1985,  97.2488,  91.5499,  46.3732,
         47.2084, 130.4265,  97.2573,  34.9573, 129.4080,  77.6729, 119.9655,
         68.3160,  55.7802], device='cuda:0')
NaN loss detected at iteration 60
input_ids: tensor([[  101, 10557, 59841,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 10167, 11906,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982,   122,  ..., 10726, 10130,   102],
        [  101, 11982,   122,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443,  1.1638,  0.3443, -0.7483,  1.4370,  2.2564,  0.3443, -1.2946,
         0.0712,  1.1638,  0.6175, -1.0214,  0.6175, -0.7483,  0.6175, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 49.7804,  54.7379, 173.9604,  47.3696, 103.7605,  60.2439,  69.5174,
        107.4873,  80.0408, 100.8525,  86.5803,  86.4483, 119.9655,  92.2002,

Training Text Model:   2%|▎                   | 68/4114 [00:21<19:21,  3.48it/s]
NaN loss detected at iteration 61
input_ids: tensor([[  101, 10167, 15826,  ...,     0,     0,     0],
        [  101, 15595, 66593,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        ...,
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 10167, 46503,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.2020,  0.3443,  0.6175,  0.0712, -1.2946, -1.0214,  0.0712,  0.8907,
         1.1638,  0.8907,  0.3443, -1.2946, -1.0214, -1.2946, -0.4751, -0.4751],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([112.7749,  97.2488,  26.0435,  52.7144, 117.3248,  80.3005,  77.9067,
         87.1531,  62.9847,  73.2870,  76.8517, 132.3790,  63.3525,  82.3918,
        118.2129, 111.0106], device='cuda:0')
NaN loss detected at iteration 62
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 51732, 11049,  ...,     0,     0,     0],
        ...,
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0],
        [  101, 27426, 15527,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443, -1.2946,  1.4370, -0.7483,  0.3443,  1.4370,  0.6175,  0.0712,
         0.6175,  2.5296,  0.6175, -0.7483, -0.2020, -1.2946,  1.1638,  0.8907],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 91.6565, 135.2085,  35.3669,  68.9206,  99.5648,  83.7236,  42.0238,
        124.5378, 102.0662,  47.5972,  37.5456, 162.6413, 112.9181,  90.9860,
         89.4286, 126.7494], device='cuda:0')
NaN loss detected at iteration 63
input_ids: tensor([[  101, 10167, 10745,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,   117, 10293,   102],
        [  101, 10167, 10118,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 11906,  ...,     0,     0,     0],
        [  101, 11583, 11522,  ...,     0,     0,     0],
        [  101, 13830, 92746,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.4370,  1.4370, -0.4751,  0.0712,  0.6175,  0.3443, -0.2020,  0.8907,
         0.0712,  0.8907,  0.0712,  1.7101,  0.0712,  1.1638,  0.8907,  0.0712],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([110.7243,  90.6265,  46.9639, 119.1181,  26.2954, 101.8930,  86.4875,
         57.3288,  80.2644,  56.1906,  41.6707,  97.0673,  62.3296,  35.3669,
         61.4287,  78.0520], device='cuda:0')
NaN loss detected at iteration 64
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ...,     0,     0,     0],
        [  101, 10236, 39710,  ..., 12337, 12210,   102],
        ...,
        [  101, 10313, 84051,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ..., 55421,   117,   102],
        [  101, 51732, 16719,  ...,   163, 54577,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([-0.4751,  1.1638,  1.4370, -0.2020, -1.2946, -1.2946, -0.4751,  1.1638,
         0.8907, -1.0214,  0.3443, -0.7483, -1.2946, -0.4751,  1.7101,  1.7101],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 52.2181,  72.1763,  38.8275,  95.3006,  88.6680, 126.4164,  61.2797,
         71.4339, 134.5203,  43.4819,  83.7447,  94.3045, 106.5418, 134.9736,
         72.1763,  57.6628], device='cuda:0')
NaN loss detected at iteration 65
input_ids: tensor([[  101, 13830, 65817,  ...,     0,     0,     0],
        [  101, 13830,   124,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 15936,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 79427, 10136,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.7483, -1.0214, -1.2946,  0.0712, -0.2020,  0.8907, -0.2020, -1.2946,
        -0.4751, -0.2020,  0.0712,  0.3443, -0.7483, -0.4751,  0.6175,  1.1638],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 94.3045,  86.4483, 126.3394,  37.2631, 113.4881,  83.7236,  61.2797,
        200.6644, 129.4080,  97.2488,  35.2528, 101.8930,  68.1394,  93.5499,
         11.2538, 103.5517], device='cuda:0')
NaN loss detected at iteration 66
input_ids: tensor([[  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 13830, 92746,  ...,     0,     0,     0],
        [  101, 10313, 10118,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.0214,  0.0712,  0.0712,  1.9833,  0.6175, -0.4751,  0.3443,  0.0712,
        -1.2946,  0.0712, -1.2946,  1.4370,  0.6175,  0.8907,  0.6175,  1.1638],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 59.4358,  67.0275, 104.3588,  35.3669,  59.8845,  77.0490,  84.4694,
        115.2842, 126.1645, 103.1871, 136.9492, 112.6848, 162.8789, 100.8525,
        112.2644,  44.7106], device='cuda:0')
NaN loss detected at iteration 67
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 13830, 15936,  ...,   119,   102,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.4751,  1.1638,  0.0712,  0.8907, -0.2020, -1.2946,  0.0712,  1.7101,
         0.8907,  0.0712, -1.2946, -0.4751,  0.0712, -0.4751,  0.3443,  0.0712],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 78.4963,  76.9940,  51.7564, 111.5787,  80.9284, 110.8538, 139.1182,
         61.4850,  66.5990,  52.7144, 118.6973,  52.2181, 102.2099, 111.0106,

Training Text Model:   2%|▎                   | 75/4114 [00:23<19:20,  3.48it/s]
NaN loss detected at iteration 68
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13029, 10231,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 10236, 39710,  ..., 88226, 11583,   102],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443, -1.2946,  0.0712, -0.2020,  1.1638,  0.6175, -0.7483,  0.0712,
         0.6175, -1.2946,  0.0712, -0.7483,  0.3443,  0.0712, -1.2946,  0.8907],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([103.7950, 107.4873,  63.6273, 176.7817,  85.7659, 146.7394,  86.1275,
         79.9334,  51.1236, 103.4801,  41.5419, 124.3743,  96.0295,  44.4890,
         83.7253,  72.1763], device='cuda:0')
NaN loss detected at iteration 69
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 11906,  ..., 10965, 33963,   102],
        [  101, 11505, 19716,  ..., 19319, 10129,   102],
        [  101, 11982, 10250,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.4751,  0.0712,  1.4370, -0.4751,  1.1638, -1.2946,  0.3443,  0.3443,
        -1.2946,  1.9833,  1.1638,  0.0712, -0.2020,  1.4370,  0.3443, -0.4751],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 52.2181,  51.9000,  76.9940,  73.2969,  85.7659, 153.9068,  64.1885,
         88.4059,  90.9860,  35.3669,  87.5467,  81.6155,  98.2838,  61.4850,
        119.9655,  98.9470], device='cuda:0')
NaN loss detected at iteration 70
input_ids: tensor([[  101, 10167, 11906,  ..., 59424, 13979,   102],
        [  101, 12664, 25239,  ..., 12577, 25028,   102],
        [  101, 13068, 11369,  ..., 15826, 10329,   102],
        ...,
        [  101, 10236,   158,  ...,     0,     0,     0],
        [  101, 20508, 53907,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.6175,  1.9833,  1.7101, -1.2946, -0.7483, -0.4751, -1.2946, -1.2946,
        -0.2020,  0.6175, -0.2020, -1.2946, -0.7483, -0.2020,  1.1638,  0.3443],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 80.9284,  35.3669,  60.2439, 121.3755,  82.3855,  51.1236, 148.3293,
        136.9492,  94.3206, 132.4450, 174.2724, 200.6644, 104.3588,  98.9470,
         38.9052,  78.3662], device='cuda:0')
NaN loss detected at iteration 71
input_ids: tensor([[  101, 10167, 10599,  ...,     0,     0,     0],
        [  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        ...,
        [  101, 10236, 20560,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ..., 21485, 10162,   102],
        [  101, 11982, 13415,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.2020, -1.2946, -0.4751,  0.8907,  0.8907, -0.4751, -1.2946, -1.2946,
        -0.2020,  0.8907, -1.0214,  0.6175,  1.4370,  1.9833,  1.9833, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 64.9748, 100.8273,  40.5938,  38.3240,  46.4092,  92.4465, 127.4277,
        150.5116,  47.1978, 116.7333,  66.1847,  62.6151,  67.6719, 111.9007,
         11.2538, 107.6476], device='cuda:0')
NaN loss detected at iteration 72
input_ids: tensor([[   101, 106482,  45522,  ...,      0,      0,      0],
        [   101,  11505,  19716,  ...,      0,      0,      0],
        [   101,  10313,  13483,  ...,      0,      0,      0],
        ...,
        [   101,  13830,  15936,  ...,      0,      0,      0],
        [   101,  13830, 101328,  ...,      0,      0,      0],
        [   101,  17605,  32650,  ...,  13935,  18989,    102]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([-1.0214,  0.6175,  0.6175,  1.4370, -0.4751, -1.0214, -0.2020,  0.0712,
         2.2564, -0.2020,  0.6175,  0.0712, -1.2946, -0.2020, -0.4751, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([126.2878, 120.4043,  53.8809,  92.2002,  47.1978,  52.1400,  84.4694,
         69.8807, 113.5054, 145.6586,  59.8845, 135.0588, 164.5873,  93.5499,
         63.7607,  66.2191], device='cuda:0')
NaN loss detected at iteration 73
input_ids: tensor([[  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 10313, 84051,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        ...,
        [  101, 27426, 15527,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ...,     0,     0,     0],
        [  101, 51732, 11049,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946,  0.8907, -1.2946, -1.0214, -0.4751, -0.2020,  0.0712,  1.4370,
         1.1638, -0.4751,  0.0712, -1.2946, -0.7483,  0.8907,  1.4370,  1.4370],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 68.3333,  56.1906,  88.6680,  86.4483,  65.6698, 136.0320,  42.4251,
         57.6628,  68.9206,  37.2631, 143.0850, 113.6154,  50.5625, 126.7494,
         72.1763,  69.1885], device='cuda:0')
NaN loss detected at iteration 74
input_ids: tensor([[  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 10762,  ...,   100, 25569,   102],
        [  101, 51732, 16719,  ..., 33903,   180,   102],
        [  101, 11982, 66593,  ..., 20856, 10307,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([-0.7483,  1.4370, -0.4751,  1.4370,  0.3443, -0.4751, -0.7483,  1.1638,
        -0.2020,  0.6175,  0.8907, -1.0214,  0.8907,  1.4370,  2.8027,  1.1638],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([127.0022, 121.4536,  90.6385,  39.0712, 109.2685,  54.1578,  93.5499,
         61.5721, 100.5144,  95.3295,  76.7720,  42.4238, 130.6077,  38.8275,

Training Text Model:   2%|▍                   | 82/4114 [00:25<19:17,  3.48it/s]
NaN loss detected at iteration 75
input_ids: tensor([[  101, 10313, 13483,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ..., 32854, 48463,   102],
        [  101, 13186,   191,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 39710,  ..., 18472,   107,   102],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.4751,  1.7101,  1.1638,  1.9833,  0.8907, -1.2946,  0.6175,  0.3443,
        -1.0214,  0.0712, -1.0214, -0.2020, -1.2946,  0.6175, -1.2946, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 82.0496,  56.0026,  56.1906,  34.8087,  61.4287,  59.1710, 121.5787,
         55.7802,  65.6932,  26.0435, 119.8600,  69.0207,  86.8354, 119.9655,
         82.3918,  63.1706], device='cuda:0')
NaN loss detected at iteration 76
input_ids: tensor([[  101, 15595, 66593,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 13483,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 67961, 24125,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443, -0.4751, -0.4751, -1.2946, -0.7483,  0.0712, -0.7483, -0.4751,
         0.6175, -1.2946,  1.4370,  1.1638,  0.0712, -1.0214,  2.2564,  0.6175],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 94.6641,  60.9703,  86.4875, 101.9981,  78.4963,  85.7800, 100.8900,
         60.9703,  74.9581, 103.2843,  72.1763,  55.7802, 115.2842,  69.3990,
        101.0048, 116.7202], device='cuda:0')
NaN loss detected at iteration 77
input_ids: tensor([[  101, 52208, 10406,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 29224, 45243,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 61694,  ...,     0,     0,     0],
        [  101, 10167, 32218,  ...,     0,     0,     0],
        [  101, 10236, 61694,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.0214,  1.1638, -1.0214,  1.4370, -0.4751, -0.7483,  0.3443, -0.2020,
        -1.0214, -0.4751,  0.0712, -1.0214,  0.0712,  1.4370,  0.8907, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 82.3790,  50.4023, 126.2878,  42.4782,  92.7580, 142.7595,  79.9334,
         84.2324,  43.4819,  71.3136,  58.0071,  51.7564, 106.3260, 132.4450,
         76.7720,  55.4237], device='cuda:0')
NaN loss detected at iteration 78
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 46503,  ...,     0,     0,     0],
        [  101, 10796, 15646,  ...,     0,     0,     0],
        ...,
        [  101, 10796, 65817,  ...,     0,     0,     0],
        [  101, 13830,   187,  ...,     0,     0,     0],
        [  101, 10167, 10118,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-0.2020,  1.1638,  1.4370,  0.0712,  0.3443, -0.7483,  0.3443,  0.6175,
         1.9833,  1.7101, -1.2946,  0.6175,  0.8907, -0.2020, -0.7483, -0.7483],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 67.3493, 109.2685,  72.1763,  51.3670, 119.9655,  96.5530,  82.3400,
         86.5803,  41.5419,  97.0673, 113.2156, 110.6995,  79.7499, 110.0655,
        104.6165,  68.9206], device='cuda:0')
NaN loss detected at iteration 79
input_ids: tensor([[  101, 10167, 10599,  ...,   117, 10293,   102],
        [  101, 10167, 46503,  ...,     0,     0,     0],
        [  101, 13209, 11479,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 10788,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.3443,  1.7101, -1.2946,  0.3443, -0.4751,  1.9833, -0.7483,  1.7101,
        -1.2946, -0.2020, -0.4751, -1.2946,  1.9833,  1.7101,  1.4370,  1.1638],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([136.1487,  47.2084, 116.3786, 122.7720,  86.5573,  35.3669, 113.8676,
         66.6077,  66.2478,  87.1531,  95.3458,  88.6680,  35.3669,  11.2538,
         90.6265,  72.1763], device='cuda:0')
NaN loss detected at iteration 80
input_ids: tensor([[  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 38050, 11643,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946, -1.2946,  0.3443, -0.4751, -1.0214, -1.2946, -1.0214,  0.8907,
        -0.4751, -1.0214,  0.6175,  0.0712, -0.4751,  1.1638, -1.2946,  0.0712],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 90.9860, 124.2625,  87.7927,  55.4237,  90.8694, 124.0893,  59.4358,
        102.2099,  81.6155,  87.1445,  99.2452,  71.4191,  71.9936,  69.1885,
        133.1675,  90.6385], device='cuda:0')
NaN loss detected at iteration 81
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10313, 84051,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 15826,  ...,     0,     0,     0],
        [  101, 10445,   107,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.4370,  0.3443,  0.0712,  1.1638,  1.9833, -0.4751,  1.9833,  1.1638,
         0.8907,  1.7101,  0.0712, -1.2946,  0.8907, -0.2020,  0.3443,  0.8907],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 42.4782, 101.7411, 125.6128,  39.0712, 111.9007, 100.3695,  35.3669,
         53.9188,  53.3545,  61.5721, 102.5554, 133.2294,  26.2954, 128.9019,

Training Text Model:   2%|▍                   | 89/4114 [00:27<19:14,  3.49it/s]
NaN loss detected at iteration 82
input_ids: tensor([[  101, 10236, 39710,  ..., 81692, 79427,   102],
        [  101, 12699, 10268,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 12786, 11044,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712,  0.3443,  0.6175, -0.2020,  0.3443,  0.6175,  1.1638,  0.6175,
        -1.0214,  0.6175, -1.0214, -1.2946, -0.7483,  0.6175, -1.2946, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 44.4890,  82.3400, 103.1871,  84.2324,  56.1906,  86.5803,  39.6696,
         96.0686, 170.8714,  73.2870,  20.9272,  88.6993,  96.9368, 129.1985,
         54.9834, 101.7411], device='cuda:0')
NaN loss detected at iteration 83
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 10250,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ..., 33989,   120,   102],
        ...,
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 13937, 10131,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712,  0.8907,  1.4370,  0.3443,  0.3443, -1.2946, -1.2946,  0.3443,
        -1.0214,  1.1638,  0.0712,  0.3443,  1.9833, -1.2946,  0.6175, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([149.0346,  83.4928,  72.1763, 105.3510, 106.0700, 128.3996,  82.3918,
        145.2906, 157.4288,  90.6265,  40.9595,  72.0955,  47.6180, 149.5067,
        117.3433, 109.9162], device='cuda:0')
NaN loss detected at iteration 84
input_ids: tensor([[   101,  64766, 109067,  ...,      0,      0,      0],
        [   101,  51732,  10762,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        ...,
        [   101,  11982,  66593,  ...,      0,      0,      0],
        [   101,  10313,  84051,  ...,      0,      0,      0],
        [   101,  67961,  24125,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.0214,  1.4370, -1.2946, -1.0214,  0.3443, -0.7483,  0.0712, -0.4751,
        -0.4751, -1.2946, -1.2946, -0.4751, -0.4751, -0.4751,  0.0712, -0.7483],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 89.2359,  41.6707,  71.1369,  80.6542, 104.6255,  92.8267,  84.4694,
        162.6413, 102.5639, 109.9162,  71.1369,  82.0496,  67.3784,  47.1978,
        117.1787,  97.3259], device='cuda:0')
NaN loss detected at iteration 85
input_ids: tensor([[  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 10167, 30518,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        ...,
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 10445, 13068,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.7101, -0.2020, -0.2020,  1.9833, -0.2020,  0.8907,  0.3443, -0.4751,
         0.6175, -1.2946,  0.0712,  0.0712,  0.8907,  0.3443, -1.2946,  1.1638],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 47.2084,  83.7236,  44.6396,  97.7242,  94.3206, 122.2209,  68.3160,
         69.3990,  59.8845, 126.3394,  91.8406, 108.7571,  40.0226, 119.4386,
         47.5635,  39.1823], device='cuda:0')
NaN loss detected at iteration 86
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830, 79427,  ...,     0,     0,     0],
        [  101, 71164, 48865,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712,  0.0712, -0.4751,  0.0712, -1.0214,  0.3443, -1.2946,  0.0712,
         0.3443, -0.7483, -0.7483,  0.6175,  0.8907,  1.7101, -0.4751, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([105.3510, 102.5554, 114.3142,  92.4465,  77.0196,  64.1393, 166.2110,
         40.9595,  54.7379,  77.4132,  92.2559,  88.4059,  40.0226,  83.6796,
         85.7496, 116.4114], device='cuda:0')
NaN loss detected at iteration 87
input_ids: tensor([[  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.8907,  0.8907, -0.2020, -1.2946, -1.2946,  0.6175, -0.7483,  0.3443,
        -1.2946,  0.8907, -1.0214,  0.3443, -1.2946,  0.3443, -1.0214, -0.7483],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 26.0435, 104.8592, 125.6128,  54.9834, 119.0336,  73.3689,  90.8694,
         92.3754,  54.9834,  43.2749,  91.9994,  97.2488, 149.5067,  67.3493,
        124.3743,  43.4819], device='cuda:0')
NaN loss detected at iteration 88
input_ids: tensor([[  101, 51732, 16719,  ..., 42429, 10130,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 11906,  ...,     0,     0,     0],
        [  101, 13830,   123,  ...,     0,     0,     0],
        [  101, 13029, 10141,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.4370,  0.3443, -0.2020,  0.3443, -0.7483, -0.2020, -1.0214,  0.6175,
        -1.2946, -0.2020, -0.4751, -1.0214,  0.0712,  0.6175, -0.2020, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 38.8275,  64.1885,  82.9962,  92.3309, 130.3151,  74.2130,  43.4819,
        111.8594, 120.4494,  52.9569,  43.4819, 128.2466, 108.7571,  78.5566,

Training Text Model:   2%|▍                   | 94/4114 [00:28<19:12,  3.49it/s]
NaN loss detected at iteration 89
input_ids: tensor([[   101,  25894,    117,  ...,      0,      0,      0],
        [   101,  11982,  10669,  ...,      0,      0,      0],
        [   101, 102204,  15797,  ...,      0,      0,      0],
        ...,
        [   101,  51732,  16719,  ...,    117,  32650,    102],
        [   101,  10236,  38002,  ...,      0,      0,      0],
        [   101,  13338,  13093,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.0214,  1.4370, -0.2020, -0.7483, -1.2946, -0.7483, -1.2946, -1.2946,
        -0.7483, -0.4751, -0.7483, -1.2946, -0.7483,  2.8027, -0.7483, -1.2946],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 46.3732, 119.9536,  40.2379,  32.5057, 164.5873,  67.8071, 130.6880,
         97.3201, 141.7105, 119.1305,  92.0074, 117.2596,  43.4819, 120.5129,
         65.6698, 123.4494], device='cuda:0')
NaN loss detected at iteration 90
input_ids: tensor([[   101,  13830,    187,  ...,      0,      0,      0],
        [   101,  11982,  10669,  ...,      0,      0,      0],
        [   101,  15595,    122,  ...,      0,      0,      0],
        ...,
        [   101,  13830,  92746,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  10313,  31300,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.6175,  0.3443,  0.0712,  1.1638,  0.0712,  0.6175,  0.3443, -0.4751,
        -0.2020, -1.2946, -1.2946, -0.2020, -0.4751,  0.0712, -1.2946,  0.6175],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 90.0999,  71.4191, 115.2842,  72.1763, 156.7790,  43.1811,  52.0002,
         96.5530, 101.9668, 103.1518, 197.6317, 101.1942, 127.0168,  71.3136,
        124.2625,  47.5972], device='cuda:0')
NaN loss detected at iteration 91
input_ids: tensor([[  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 10796, 39284,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ..., 22667, 10115,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.0712,  0.0712, -1.2946, -1.2946, -1.0214,  0.0712,  1.7101, -0.4751,
        -0.7483, -0.4751, -0.4751,  1.4370, -1.2946,  1.4370, -0.2020, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 51.7282,  48.8803, 132.8490, 133.1675,  69.4673, 145.6586,  91.2221,
         95.6271,  50.5625, 113.8676,  83.4928, 125.4000,  74.8993,  38.8275,
        122.9195, 113.3975], device='cuda:0')
NaN loss detected at iteration 92
input_ids: tensor([[  101, 10236, 39710,  ..., 10874, 18913,   102],
        [  101, 13029, 10231,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10236, 39710,  ..., 15499, 61355,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.4370, -1.2946, -0.7483,  0.8907, -0.4751,  0.8907,  0.8907,  0.3443,
         0.0712,  0.0712, -1.2946, -0.2020,  0.6175,  1.4370, -1.2946, -0.7483],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 38.8275, 124.0893,  61.2797, 152.9082,  90.8694, 143.0850,  40.0226,
         32.1372,  92.3754,  96.9368, 126.1645, 116.2469,  59.8845,  38.8275,
         71.1369, 130.3151], device='cuda:0')
NaN loss detected at iteration 93
input_ids: tensor([[  101, 10796,   187,  ...,     0,     0,     0],
        [  101,   119,   119,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 10347,  ...,     0,     0,     0],
        [  101, 13830, 35350,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ..., 13034, 10835,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([-0.7483, -1.0214, -0.2020,  0.0712, -1.2946, -0.2020, -0.4751, -0.2020,
        -1.2946, -1.2946,  0.3443, -0.2020,  1.1638,  0.0712,  1.1638,  0.6175],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([166.7724,  65.6698,  84.2773, 143.3086,  47.5635,  71.1369, 116.0713,
         88.1533,  68.3333,  82.3918,  42.4251,  36.2959,  38.9052, 106.7413,
        101.8587,  38.3240], device='cuda:0')
NaN loss detected at iteration 94
input_ids: tensor([[  101, 12685, 15936,  ..., 22882, 42182,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 66188,   136,   102],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.6175,  1.1638, -0.2020,  0.0712,  1.4370,  0.0712,  1.4370, -1.2946,
         0.0712,  0.0712, -0.2020, -0.7483,  0.3443,  0.0712,  0.3443, -1.0214],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 60.1338,  85.7659,  87.1531,  40.9595, 123.8212,  78.5566,  43.8032,
         68.3333,  90.5470,  67.0275, 113.6805, 121.4536,  84.4694, 117.4333,
         38.3240,  64.1393], device='cuda:0')
NaN loss detected at iteration 95
input_ids: tensor([[  101, 11982, 11165,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 10236, 39710,  ...,     0,     0,     0],
        [  101, 10313, 13483,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.2946,  0.8907,  1.1638, -0.2020, -1.2946,  0.3443,  0.3443,  0.8907,
         1.1638,  0.0712, -1.2946,  1.7101,  0.8907,  1.4370,  0.0712, -0.2020],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([119.5634,  45.2776,  72.1763, 129.2656,  86.9333, 119.9655, 147.6293,
         73.2870,  61.4850,  41.6707,  91.4785,  53.4073, 119.1181,  37.2631,
Training Text Model:   2%|▍                   | 96/4114 [00:29<20:39,  3.24it/s]
Traceback (most recent call last):
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/combined_model.py", line 250, in <module>
    train_embeddings, test_embeddings = train_text_model(model, train_loader, test_loader, criterion, optimizer, scaler, scheduler, device, EPOCHS, save_path="model/text_model_full.pth")
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/combined_model.py", line 210, in train_text_model
    train_loss, train_mse, train_embeddings = train_epoch_text(model, train_loader, criterion, optimizer, scaler, device)
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/combined_model.py", line 154, in train_epoch_text
    scaler.scale(loss).backward()
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: an illegal memory access was encountered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
NaN loss detected at iteration 96
input_ids: tensor([[   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  13830,  20560,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        ...,
        [   101,  11982,  10669,  ...,    118,    118,    102],
        [   101,  10445,    155,  ...,      0,      0,      0],
        [   101,  13830,    187,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([-1.0214, -0.4751, -0.2020,  0.6175,  0.6175,  0.3443,  0.3443, -1.2946,
        -0.4751,  0.0712, -0.4751,  1.4370,  0.0712,  0.8907,  0.6175,  1.4370],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 47.5635,  82.0496, 176.7817, 100.8525,  65.8420,  78.3662,  74.9581,
         88.4139,  90.1144, 123.7856,  43.4819,  53.8809,  92.3754, 122.6961,
        145.6586, 123.8212], device='cuda:0')