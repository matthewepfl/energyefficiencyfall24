/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(

Training Text Model:   0%|                   | 1/4114 [00:02<2:58:36,  2.61s/it]
NaN loss detected at iteration 0
input_ids: tensor([[  101, 14453, 10109,  ...,     0,     0,     0],
        [  101, 77034, 10115,  ...,     0,     0,     0],
        [  101, 13830, 79427,  ...,     0,     0,     0],
        ...,
        [  101, 12685, 15936,  ...,     0,     0,     0],
        [  101, 10236, 85695,  ...,     0,     0,     0],
        [  101, 48503, 10107,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  4.,  3.,  1.,  6.,  4.,  9.,  6.,  3.,  7., 15.,  5.,  3.,  9.,
         8., 11.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 60.1122,  30.1471,  82.0496,  86.4483,  80.4560, 133.0278, 109.2685,
        135.0588, 106.3592, 119.9655,  57.6578,  88.2710, 116.5817,  60.1338,
        122.7127,  42.4782], device='cuda:0')
NaN loss detected at iteration 1
input_ids: tensor([[  101, 11500,   187,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 10250,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 79427,  ...,     0,     0,     0],
        [  101, 13830, 12364,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2.,  1.,  6.,  3.,  3.,  2.,  0.,  8.,  1.,  2.,  3.,  6.,  0.,  6.,
         4., 11.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([107.2845,  60.9703, 134.5203,  58.9034, 116.2469,  76.8135, 147.7813,
         81.6662, 108.5696, 162.6413, 147.6293,  38.3240, 118.8089,  68.3160,

Training Text Model:   0%|                     | 7/4114 [00:04<26:49,  2.55it/s]
NaN loss detected at iteration 2
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 39663,  ...,     0,     0,     0],
        [  101, 11583, 11522,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([8., 0., 5., 5., 9., 8., 7., 5., 2., 2., 4., 7., 0., 1., 7., 6.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([144.3342,  86.4483,  61.4287, 101.5677, 110.7243,  94.0071,  78.3662,
         48.8803,  92.3389,  84.2324,  71.3895,  59.8845,  86.9333,  90.8694,
         52.8775,  72.1763], device='cuda:0')
NaN loss detected at iteration 3
input_ids: tensor([[  101, 10167, 19985,  ...,     0,     0,     0],
        [  101, 43690, 47314,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ..., 10304, 19453,   102],
        [  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 21851, 72894,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 6.,  0.,  7., 10.,  5.,  5., 11.,  0.,  2.,  9.,  1., 12.,  8.,  8.,
         0.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 87.0025, 132.8490,  69.7060,  79.3868,  52.6674, 162.6413,  44.7106,
         86.9333, 137.0422,  72.0955,  95.3458,  35.3669,  11.2538,  67.0071,
        138.2969, 101.1622], device='cuda:0')
NaN loss detected at iteration 4
input_ids: tensor([[   101,  10167,  46503,  ...,      0,      0,      0],
        [   101,  11982,    122,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,  10726,  15547,    102],
        ...,
        [   101,  10236,  39710,  ...,    165,    182,    102],
        [   101,  12395,  28713,  ...,    135,  13740,    102],
        [   101,  73784, 104046,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([10.,  3.,  6., 10.,  4.,  6.,  6.,  0.,  0.,  1.,  8.,  0.,  5., 11.,
         4.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 43.8032, 113.8676, 122.6961,  73.1039,  52.6674,  48.8803, 109.2685,
         97.5631, 109.9162,  98.9470,  64.1393, 126.0625,  40.2379,  38.8275,
         51.1265, 118.6783], device='cuda:0')
NaN loss detected at iteration 5
input_ids: tensor([[  101, 13186,   191,  ...,     0,     0,     0],
        [  101, 23837, 42031,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 6.,  0.,  0.,  0.,  1.,  3.,  9.,  5., 12.,  5.,  0.,  5.,  0.,  6.,
         5.,  7.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 56.1906,  68.3333, 132.8490, 108.9257,  73.0038,  63.7042, 107.3977,
         34.9573,  34.8087,  77.6729,  86.7073, 106.3260, 116.4114,  37.5456,
         45.2776,  79.9334], device='cuda:0')
NaN loss detected at iteration 6
input_ids: tensor([[  101, 13830, 79427,  ...,     0,     0,     0],
        [  101, 10167, 10118,  ..., 47145, 10305,   102],
        [  101, 11982, 10669,  ..., 10304, 19453,   102],
        ...,
        [  101, 10796,   187,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 4.,  6.,  9.,  9., 10.,  8.,  0.,  4.,  0.,  5.,  0.,  0.,  4.,  4.,
         2.,  4.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 84.2235,  72.0955,  90.6038, 128.0522,  74.8969,  78.2945, 126.1645,
        102.2099,  66.2478,  44.4890, 119.5634,  46.3732,  62.3935, 107.2845,
         92.2559,  97.3259], device='cuda:0')
NaN loss detected at iteration 7
input_ids: tensor([[  101, 52208, 24931,  ...,     0,     0,     0],
        [  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 10167, 46503,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  0.,  6.,  0.,  4.,  9., 10.,  1.,  9.,  2.,  3.,  3.,  7.,  5.,
         3.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([133.1675, 129.4433,  69.2053,  88.6993,  97.2488,  40.0226,  72.1763,
         86.9543,  45.2776,  65.6698, 105.0821,  83.9148, 132.6190, 127.0022,
         84.2773, 132.8490], device='cuda:0')
NaN loss detected at iteration 8
input_ids: tensor([[  101, 51732, 16719,  ..., 12127, 16831,   102],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 63672, 10342,  ...,     0,     0,     0],
        [  101, 52208, 24931,  ...,     0,     0,     0],
        [  101, 43690, 47314,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([10.,  0.,  3., 12.,  7.,  0.,  0., 13.,  3.,  0.,  8.,  1.,  1.,  0.,
         0.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 38.8275, 128.6350,  92.3389,  35.3669,  51.1236, 103.4801, 126.3394,
         26.2954,  72.4151, 116.4114, 120.5129,  85.9795,  76.5133,  60.3315,

Training Text Model:   0%|                    | 14/4114 [00:06<20:48,  3.28it/s]
NaN loss detected at iteration 9
input_ids: tensor([[  101, 80433, 10161,  ...,     0,     0,     0],
        [  101, 10167, 10118,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,   165,   182,   102],
        ...,
        [  101, 10313, 10118,  ...,     0,     0,     0],
        [  101, 10445, 13068,  ...,     0,     0,     0],
        [  101, 13068, 11369,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 7.,  1.,  6.,  2.,  3.,  9.,  6.,  0., 13.,  0.,  2.,  0.,  5.,  7.,
        10., 11.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 58.0071,  87.7494,  74.9581, 133.5628, 130.3151,  26.2954,  40.2379,
         59.1710,  97.7242, 121.3755,  71.1369,  98.5815, 145.6586,  71.4016,
         39.1823,  60.2439], device='cuda:0')
NaN loss detected at iteration 10
input_ids: tensor([[  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 71164, 48865,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ..., 15826, 10329,   102],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 10236,   125,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  0.,  0.,  0.,  3.,  4.,  0., 11.,  0.,  2.,  5.,  9.,  0.,  9.,
         0.,  9.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([200.6644, 108.9287,  99.5081, 110.4757,  98.7767, 125.6128,  59.4358,
         39.8819,  82.5600,  69.8299, 117.1787,  26.2954, 122.9194,  41.4272,
        103.4801,  38.9052], device='cuda:0')
NaN loss detected at iteration 11
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 46503,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 10118,  ...,     0,     0,     0],
        [  101,   146, 29239,  ...,     0,     0,     0],
        [  101, 10167, 38657,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 4.,  5.,  3., 15.,  8.,  6.,  4.,  5.,  5.,  3., 12.,  9.,  7.,  4.,
         1.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 99.5648, 111.8594,  52.2181, 120.5129, 139.1182,  71.4302,  97.2488,
        101.9698,  53.8809, 182.5036,  42.4782,  94.0071,  40.2379, 135.8302,
        101.9981,  84.2324], device='cuda:0')
NaN loss detected at iteration 12
input_ids: tensor([[  101, 10167, 10118,  ...,     0,     0,     0],
        [  101, 43690, 47314,  ...,     0,     0,     0],
        [  101, 10236, 14483,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 11982, 11165,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([10.,  0.,  3.,  1.,  6.,  0.,  3.,  5.,  8.,  7.,  4.,  2.,  2.,  9.,
         7.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([111.9007,  59.4358,  91.9116, 124.2625,  82.3400, 116.4114,  63.6273,
         34.9573,  47.5972,  73.1039, 174.2724, 182.5036, 116.0713, 134.9760,
        108.6525, 119.5634], device='cuda:0')
NaN loss detected at iteration 13
input_ids: tensor([[   101,  43690,  18460,  ...,      0,      0,      0],
        [   101, 102204,  15797,  ...,      0,      0,      0],
        [   101,  11982,  10250,  ...,      0,      0,      0],
        ...,
        [   101,  13830,  79427,  ...,      0,      0,      0],
        [   101,  11982,    122,  ...,      0,      0,      0],
        [   101,  10178,  66593,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  9.,  6., 10.,  1.,  0.,  3.,  6.,  0.,  6.,  9.,  6.,  1.,  7.,
         7.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([120.4155,  38.9052,  93.0223,  70.3214,  66.0324, 119.0336,  71.9936,
         92.6176, 116.2032,  82.3400,  56.1906, 100.8525,  69.0207,  62.9535,
        117.3433, 108.9257], device='cuda:0')
NaN loss detected at iteration 14
input_ids: tensor([[  101, 10313, 13483,  ...,     0,     0,     0],
        [  101, 13830, 10788,  ..., 10151, 20409,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 10762,  ..., 18472, 11583,   102],
        [  101, 10445,   155,  ...,     0,     0,     0],
        [  101, 30932, 10681,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([3., 6., 6., 2., 8., 6., 5., 3., 6., 9., 0., 2., 6., 5., 5., 3.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([135.8302,  53.9188,  26.2954,  72.4151,  51.1236,  54.7379,  52.7144,
         37.3637,  58.2135,  47.2242,  47.5635,  77.1669, 152.9082,  51.1265,
        110.8742,  65.6698], device='cuda:0')
NaN loss detected at iteration 15
input_ids: tensor([[  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 29224, 45243,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([2., 5., 3., 3., 0., 1., 1., 8., 3., 2., 4., 4., 3., 9., 5., 7.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([104.6165,  47.5118, 129.2656,  98.7767,  82.5600, 119.0336,  94.3045,
         94.7326,  79.8238,  82.3855, 132.6190,  87.6992,  67.4676,  76.9940,

Training Text Model:   1%|                    | 21/4114 [00:08<19:49,  3.44it/s]
NaN loss detected at iteration 16
input_ids: tensor([[  101, 10672, 13483,  ..., 10762, 10106,   102],
        [  101, 11982, 10250,  ...,     0,     0,     0],
        [  101,   118, 92746,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13209, 11479,  ...,     0,     0,     0],
        [  101, 13830, 61694,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([12.,  5.,  3.,  2., 14., 11.,  8., 12.,  4.,  8.,  7.,  4.,  4.,  2.,
         0.,  6.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 53.9188,  77.1669, 121.8980,  48.8803,  35.3669,  92.0227, 131.1357,
         47.6180, 131.3643,  74.3726,  51.1236, 197.7153,  93.8469,  61.2797,
        128.6350, 121.8980], device='cuda:0')
NaN loss detected at iteration 17
input_ids: tensor([[  101, 10313, 13483,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ..., 15826, 10329,   102],
        [  101, 11982, 66593,  ...,   119,   133,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 6.,  5.,  7., 10., 10.,  7.,  0.,  1., 10.,  0.,  6.,  6.,  0.,  9.,
         8.,  5.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([111.8594,  78.5566,  59.8845, 146.8248, 123.8212,  83.7236, 116.4114,
        103.4801,  38.8275, 151.7125, 101.7411, 120.0502,  82.3918,  80.9500,
         39.0712,  92.2559], device='cuda:0')
NaN loss detected at iteration 18
input_ids: tensor([[  101, 43690, 47314,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        ...,
        [  101, 13068, 11369,  ..., 10167, 12754,   102],
        [  101, 25894,   117,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  2.,  9.,  6.,  6.,  6.,  8.,  9.,  1.,  2.,  6.,  5.,  2., 11.,
         0.,  2.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 59.4358, 100.1747, 130.4891, 102.4977,  38.3240,  97.2488,  80.4802,
         39.1823,  81.0708, 108.6525,  39.1823,  87.6222,  77.1669,  60.2439,
         86.8354,  48.8803], device='cuda:0')
NaN loss detected at iteration 19
input_ids: tensor([[  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830,   191,  ...,     0,     0,     0],
        [  101, 10313, 13483,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  0.,  0.,  0.,  3., 10.,  7.,  0.,  7.,  0.,  9.,  3.,  0.,  4.,
         9.,  3.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 56.6746, 187.4622, 143.7645,  66.3063,  80.1944,  83.7236, 109.2685,
         67.3493,  96.0686, 149.5067, 107.3419,  83.9148, 118.8089, 150.7184,
         56.1906, 144.3217], device='cuda:0')
NaN loss detected at iteration 20
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 14977, 10372,   102],
        ...,
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 10167, 22282,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 3.,  9., 13.,  6.,  9.,  0., 14.,  7.,  4.,  3.,  0., 10.,  0.,  9.,
         6., 10.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 77.0785, 132.6190,  78.4014,  47.1133, 183.3395, 170.2543,  37.1506,
        111.9557,  65.6698,  87.5872, 125.9277,  72.1763,  89.3187,  40.0226,
         74.9864,  53.4073], device='cuda:0')
NaN loss detected at iteration 21
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 11165,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 10167, 10599,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([10.,  6.,  1.,  4., 12.,  0.,  2.,  1.,  0.,  1.,  7.,  8.,  1.,  0.,
         9.,  5.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([135.6367,  97.2488, 138.6623,  80.4802,  61.4850, 108.2465, 147.6666,
        122.9194, 175.4032,  59.1710, 111.9007, 119.1181, 100.3695, 116.4114,
        120.4043, 139.4378], device='cuda:0')
NaN loss detected at iteration 22
input_ids: tensor([[  101, 10167, 10599,  ..., 14977, 10372,   102],
        [  101, 11505, 19716,  ..., 29424, 16719,   102],
        [  101, 11982,   122,  ...,     0,     0,     0],
        ...,
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 10150,   144,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([12.,  7.,  3.,  3.,  5.,  2.,  0.,  8.,  3.,  0., 14.,  9.,  8.,  0.,
         6., 13.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([107.3419, 119.9655, 132.6302,  84.3917,  72.0955,  96.1158, 118.6973,
         61.3055, 122.6961,  88.6680, 131.9325, 128.0522,  64.0918, 101.9981,

Training Text Model:   1%|▏                   | 28/4114 [00:10<19:47,  3.44it/s]
NaN loss detected at iteration 23
input_ids: tensor([[   101,  11982,    122,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        ...,
        [   101,  13338,  49316,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2.,  6.,  4.,  5., 13., 10.,  6.,  3.,  5.,  8.,  6.,  4.,  0.,  0.,
         1.,  4.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 56.1245,  70.2360, 134.5203, 108.7571, 103.7605, 146.8248,  82.7975,
        112.9181, 143.3086,  72.0955,  87.5467, 117.1787,  66.2478, 158.3184,
         20.9272,  91.8406], device='cuda:0')
NaN loss detected at iteration 24
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10796, 48480,  ..., 10136, 10106,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 5.,  5.,  4.,  9.,  4.,  5.,  0.,  5.,  5.,  1.,  3.,  4.,  5., 12.,
         9.,  9.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([123.0521, 123.8212,  95.3191,  91.0322, 129.2656,  71.4302,  66.2478,
         84.4694,  84.2324, 124.9767,  84.2235,   0.3890,  92.3754, 131.3255,
         60.3016,  76.9940], device='cuda:0')
NaN loss detected at iteration 25
input_ids: tensor([[  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 24338, 10268,  ...,     0,     0,     0],
        [  101,   138, 41357,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([0., 1., 4., 4., 9., 9., 5., 2., 0., 6., 2., 1., 5., 3., 6., 0.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([108.2465, 159.8161, 125.6128,  84.2773, 120.4043, 130.6077, 116.4669,
         65.6698, 106.9028,  91.6565,  65.6698,  92.8500,  40.9595,  47.1978,
         61.3055, 133.1675], device='cuda:0')
NaN loss detected at iteration 26
input_ids: tensor([[  101, 10167, 10118,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 10118,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([14.,  0.,  6.,  4.,  0., 12., 12.,  9.,  1.,  8.,  0.,  0.,  3.,  3.,
         6.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 97.7242, 133.7706,  69.7060, 104.6165,  51.1162,  71.8204,  57.6628,
        135.9242,  93.5499,  64.0918,  54.9834,  88.6993, 100.3695, 104.3588,
         84.4694, 121.1426], device='cuda:0')
NaN loss detected at iteration 27
input_ids: tensor([[  101, 61916, 41995,  ...,     0,     0,     0],
        [  101, 10313,   187,  ...,     0,     0,     0],
        [  101,   118, 13209,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([5., 4., 0., 3., 6., 0., 3., 5., 0., 0., 8., 3., 2., 2., 5., 3.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([124.5378, 110.2670, 125.9277, 133.1675,  92.3754, 173.5324,  68.3160,
         51.1265,  89.3835, 134.9945,  48.9663,  77.8959, 112.9181,  56.1245,
         71.1320, 101.9668], device='cuda:0')
NaN loss detected at iteration 28
input_ids: tensor([[   101,  13830,    124,  ...,      0,      0,      0],
        [   101,  11505,  19716,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        ...,
        [   101,  51732,  16719,  ...,  14321,  40581,    102],
        [   101,  11982,  66593,  ...,      0,      0,      0],
        [   101,  11982,    122,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([1., 7., 1., 6., 5., 6., 7., 4., 9., 8., 5., 8., 5., 5., 5., 5.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 86.4483,  87.7927, 125.1736, 130.4891,  72.2873,  92.3754,  85.2361,
        104.8592,  89.2187,  51.3670, 101.9698,  44.3771,  57.6628,  80.4560,
        119.1181,  97.1703], device='cuda:0')
NaN loss detected at iteration 29
input_ids: tensor([[  101, 63672, 10342,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10236,   124,  ..., 33963, 29224,   102],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 25894,   117,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  8.,  9.,  3.,  7.,  4.,  8.,  7.,  5., 11., 10.,  3.,  7.,  2.,
         4.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 61.3225,  80.9500,  39.6696,  51.7282,  59.8845,  90.6385,  31.9621,
        117.3433,  34.7706, 110.7243, 109.2685,  34.9573,  26.8834, 140.9074,

Training Text Model:   1%|▏                   | 35/4114 [00:12<19:36,  3.47it/s]
NaN loss detected at iteration 30
input_ids: tensor([[  101, 43690, 18460,  ...,     0,     0,     0],
        [  101,   163, 24820,  ..., 10307, 12750,   102],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 31300,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,   117, 10143,   102],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  9.,  3.,  7.,  6., 15.,  5.,  7.,  1.,  8., 14.,  7.,  2., 10.,
         6.,  4.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 87.1445,  61.4850,  48.8094, 134.5203,  79.9334, 120.5129, 134.1044,
        150.1677,  72.4151, 116.4137, 131.9325, 185.8632, 107.6159, 122.7720,
        111.8594,  72.1048], device='cuda:0')
NaN loss detected at iteration 31
input_ids: tensor([[  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830, 79427,  ...,     0,     0,     0],
        [  101, 32560, 11624,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0., 10.,  6.,  8.,  0., 11.,  3.,  7.,  7.,  9.,  0.,  7.,  9.,  6.,
         7.,  9.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 90.2836,  38.8275, 106.1335,  98.0629, 121.1426, 116.5817, 217.8111,
        117.3433,  68.3160, 119.9655, 164.7993, 137.6797,  55.8393, 135.0588,
         91.5499,  88.1533], device='cuda:0')
NaN loss detected at iteration 32
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 10236, 20560,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 6.,  0., 11.,  5.,  0.,  4., 12.,  8.,  0.,  3.,  6.,  6.,  5.,  2.,
         2.,  8.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([144.2842,  46.3732, 114.7136,  62.9688,  78.7654, 101.7411, 103.7605,
         67.6765, 121.9276,  77.8959,  88.4059, 101.9698,  41.4272,  83.4905,
         51.3670, 128.0522], device='cuda:0')
NaN loss detected at iteration 33
input_ids: tensor([[  101, 10236, 39710,  ...,   133, 33989,   102],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 10313, 84051,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 18080, 10115,  ...,     0,     0,     0],
        [  101, 12664, 25239,  ..., 10390, 19282,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 5.,  7.,  5.,  8.,  7.,  5.,  4.,  7.,  0.,  2., 11.,  3.,  6.,  3.,
         8., 11.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 61.7175,  91.0425,  67.2432,  39.0712,  88.4059, 134.4974,  70.1426,
        113.3749, 115.6987, 162.6413, 128.0522,  94.4898,  62.3935,  89.3209,
        124.5391, 128.0522], device='cuda:0')
NaN loss detected at iteration 34
input_ids: tensor([[  101, 30932, 10681,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ..., 10136, 11583,   102],
        [  101, 10167, 10599,  ...,   133, 33989,   102],
        ...,
        [  101, 10167, 11906,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 71164, 48865,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 3.,  7.,  7.,  1.,  3.,  2.,  6., 12.,  0.,  7.,  5.,  0., 10.,  9.,
         0.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 65.6698,  79.6732,  93.7358,  60.3315,  46.8751,  55.4237, 119.9655,
        111.8482, 160.9451,  80.4560,  96.0969,  99.5081,  71.6320,  84.5759,
        126.3394, 105.6918], device='cuda:0')
NaN loss detected at iteration 35
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 15936,  ...,     0,     0,     0],
        [  101, 13830, 39710,  ...,     0,     0,     0],
        [  101, 10313, 84051,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([13.,  3.,  6.,  0.,  4., 12.,  1.,  6.,  3.,  6., 13.,  0.,  9.,  6.,
         4., 10.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([101.0048, 125.6128, 134.5203,  96.6987,  46.0141,  60.2439,  68.6087,
         84.2773,  68.3160,  72.1763,  72.1763, 105.6918,  26.2954,  62.3935,
        100.0695,  56.1906], device='cuda:0')
NaN loss detected at iteration 36
input_ids: tensor([[  101, 51732, 10762,  ..., 55732, 25547,   102],
        [  101, 13209, 11479,  ...,     0,     0,     0],
        [  101, 13830,   187,  ...,     0,     0,     0],
        ...,
        [  101, 13209, 11479,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ..., 40581, 24787,   102],
        [  101, 10445,   155,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([15.,  1.,  7., 11.,  6.,  1., 11.,  6.,  0.,  3.,  7.,  1.,  1.,  0.,
         8.,  3.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([120.5129, 111.2677,  51.1236,  57.6628, 104.0875, 119.8600, 107.5429,
         68.9206, 121.9072,  72.4151,  98.3834,  75.8853,  34.9573,  65.7572,

Training Text Model:   1%|▏                   | 42/4114 [00:14<19:36,  3.46it/s]
NaN loss detected at iteration 37
input_ids: tensor([[  101, 52208, 50096,  ...,     0,     0,     0],
        [  101, 29224, 45243,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 84051,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  7., 14.,  9., 15.,  0.,  8.,  2.,  6.,  7.,  8.,  0., 14.,  9.,
         6.,  6.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([125.9277, 136.0320,  94.6859,  72.1763, 120.5129, 113.6154, 124.7353,
         61.7175,  67.0455,  60.9823,  56.1906,  66.2478,  72.0208,  56.1906,
         83.3201,  84.2773], device='cuda:0')
NaN loss detected at iteration 38
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 30518,  ...,     0,     0,     0],
        [  101, 10167, 35052,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 7.,  9.,  3.,  9.,  0., 12.,  1.,  6.,  2.,  5.,  3.,  6.,  6.,  7.,
         0.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 39.0712,  81.6662,  62.9688,  50.4023,  78.7654,  97.0673,  68.5316,
        116.4669, 140.9074,  51.1265,  71.5262,  92.2002, 126.7494,  38.7441,
        164.5873,  57.1406], device='cuda:0')
NaN loss detected at iteration 39
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        ...,
        [  101,   138, 15533,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([3., 1., 3., 5., 1., 4., 5., 5., 7., 6., 0., 0., 6., 0., 2., 5.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 93.1135, 117.2596,  48.8094,  84.3884,  78.4597, 139.0868, 111.9527,
         61.1053,  40.0226,  77.4418,  68.3333, 117.2596,  97.2488, 105.9855,
         82.3855,  80.1944], device='cuda:0')
NaN loss detected at iteration 40
input_ids: tensor([[  101, 13209, 11479,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 10796, 13483,  ...,     0,     0,     0],
        ...,
        [  101, 90834, 10216,  ..., 12128, 10425,   102],
        [  101, 10796, 48480,  ...,   182, 13034,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  0.,  3., 11.,  5.,  1.,  1.,  0.,  5.,  6., 14.,  2.,  6.,  0.,
        10.,  3.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([128.6350, 115.2172,  96.5530,  85.8097,  44.4890, 101.7443,  87.7494,
        110.8538, 116.2469,  79.6732,  26.2954, 100.5144,  82.4245,  66.2191,
         72.1763, 209.6592], device='cuda:0')
NaN loss detected at iteration 41
input_ids: tensor([[  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 10313, 30518,  ...,     0,     0,     0],
        [  101, 10313, 84051,  ...,     0,     0,     0],
        ...,
        [  101, 13830,   125,  ...,     0,     0,     0],
        [  101, 10313, 34665,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 7.,  1.,  2.,  4.,  6.,  6., 16.,  9.,  5.,  4.,  8.,  9.,  2.,  4.,
         4.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 68.9206, 147.6293, 104.3588,  78.6658,  53.3545,  78.5566,  37.1506,
        103.5517,  52.7144, 137.4646,  53.9188,  72.1763,  69.2053, 112.9181,
         54.7379,  82.5600], device='cuda:0')
NaN loss detected at iteration 42
input_ids: tensor([[   101,  10167,  10599,  ...,      0,      0,      0],
        [   101,  10236,  39710,  ...,  28647,  10762,    102],
        [   101,  11982,  10596,  ...,      0,      0,      0],
        ...,
        [   101,  11505,  19716,  ...,  16831,  10136,    102],
        [   101,  11982,  10669,  ...,  10147, 104278,    102],
        [   101,  13338,  49316,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 3.,  4.,  3.,  5.,  1.,  0.,  5.,  2.,  0.,  0.,  0., 15.,  5., 11.,
        13.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([116.0713,  93.8469,  60.9703,  89.3209, 113.6154,  77.0196,  41.6707,
         74.3483,  66.3063,  87.1445, 123.5018,  97.7242,  40.2379,  39.0712,
         34.8087,  88.6680], device='cuda:0')
NaN loss detected at iteration 43
input_ids: tensor([[  101, 24338, 10118,  ..., 12127, 16831,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 10118,  ..., 40581, 24787,   102],
        ...,
        [  101, 13830,   187,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10236, 79427,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2.,  7., 12.,  1.,  0.,  0.,  0.,  7.,  3.,  0.,  0.,  1.,  6.,  1.,
         3.,  3.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 61.3055, 101.9698, 111.9007,  62.9688,  95.2489,  88.6680, 130.3642,
        121.5787,  51.2862,  86.9333,  87.1445,  71.7857,  86.1193, 100.3695,

Training Text Model:   1%|▏                   | 49/4114 [00:16<19:39,  3.45it/s]
NaN loss detected at iteration 44
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101,   146, 29239,  ...,     0,     0,     0],
        [  101, 21851, 10211,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 51732, 11049,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 7.,  1.,  0.,  6.,  8.,  4.,  5.,  6., 10.,  9.,  9., 11.,  3.,  5.,
        10.,  6.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([106.3260,  66.3063, 110.8538,  38.3240, 101.9698,  73.4204, 150.1677,
        162.6597,  72.1763,  11.7240,  39.1823,  47.2084, 101.9668,  78.5566,
         35.3669, 108.7571], device='cuda:0')
NaN loss detected at iteration 45
input_ids: tensor([[   101,  25894,    117,  ...,      0,      0,      0],
        [   101,  13029,  10119,  ...,      0,      0,      0],
        [   101,  50611,  22913,  ...,      0,      0,      0],
        ...,
        [   101,  15595,  10669,  ...,      0,      0,      0],
        [   101,  80433,  10161,  ...,      0,      0,      0],
        [   101, 102204,  40405,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  1.,  0.,  7.,  1.,  5.,  7.,  4.,  1.,  1.,  7.,  5.,  5., 10.,
         7.,  2.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([101.9981,  83.7253, 128.6350,  72.3660,  87.7494, 124.5378,  68.3160,
         85.3986,  69.0207, 127.8508, 141.1521, 115.2842,  40.0226,  81.6662,
         58.0071,  72.0955], device='cuda:0')
NaN loss detected at iteration 46
input_ids: tensor([[  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 10167, 11134,  ...,     0,     0,     0],
        [  101, 11982, 10250,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 13830, 10211,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 3.,  9.,  6.,  8.,  1.,  9., 10.,  6.,  3.,  0.,  0.,  6.,  3., 10.,
         9.,  2.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 71.1369,  53.4073,  73.4928,  66.5990,  69.5174,  74.3726,  90.6265,
        135.8302,  67.4676, 146.3655, 137.0114,  51.1236,  38.1963,  50.4023,
         81.6662, 105.6866], device='cuda:0')
NaN loss detected at iteration 47
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 12664, 10745,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        ...,
        [  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ..., 33989,   120,   102],
        [  101, 10313, 13483,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  9.,  1.,  1.,  0.,  0.,  5.,  3.,  6.,  6.,  0.,  1.,  8.,  0.,
        10.,  7.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([204.5010,  40.0226,  92.0374, 170.2543, 101.3964, 119.5634,  84.4694,
         74.9484, 124.5378,  68.9206, 108.4471,  68.6087,  94.0071,  86.7073,
        100.0695, 151.0934], device='cuda:0')
NaN loss detected at iteration 48
input_ids: tensor([[   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  10313,  10118,  ...,      0,      0,      0],
        [   101,  10236, 101328,  ...,      0,      0,      0],
        ...,
        [   101,  14619,  12750,  ...,      0,      0,      0],
        [   101,  11038,  34693,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 4.,  3., 11.,  3.,  3., 12.,  7.,  0.,  0.,  9.,  5.,  9., 10.,  6.,
         5.,  8.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 68.3160, 135.8302, 123.8212, 125.3827,  99.5779,  77.6729, 107.0550,
        105.6866,  88.6680,  70.7460,  51.9000, 100.8525,  42.4782,  66.5990,
         92.0227,  68.5149], device='cuda:0')
NaN loss detected at iteration 49
input_ids: tensor([[  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 13830, 30016,  ..., 28647, 17256,   102],
        ...,
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ..., 40581, 24787,   102],
        [  101, 10313, 13483,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  5.,  9.,  0.,  0.,  3.,  5.,  5.,  4.,  8.,  4.,  5.,  5.,  7.,
        11.,  2.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 95.8782,  88.8247,  39.6696, 122.9194,  59.1710, 135.8302,  92.3754,
         72.0955,  85.2361,  57.1543, 128.0522,  48.8803,  81.7370,  97.2488,
         72.1763,  69.3990], device='cuda:0')
NaN loss detected at iteration 50
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 50557, 10305,  ...,     0,     0,     0],
        ...,
        [  101, 13830,   187,  ...,     0,     0,     0],
        [  101, 10796, 46437,  ...,     0,     0,     0],
        [  101, 11699, 13207,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 3., 10.,  4.,  8.,  5.,  0.,  2.,  0.,  3.,  9.,  6.,  0.,  5.,  8.,
         4.,  4.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([115.2922,  83.7236,  67.3784,  84.5759,  61.1053,  66.2478, 140.7969,
         96.8331,  91.9994,  44.7106,  79.9334,  66.2478,  73.4204, 146.0798,

Training Text Model:   1%|▎                   | 56/4114 [00:18<19:35,  3.45it/s]
NaN loss detected at iteration 51
input_ids: tensor([[  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 13830, 79427,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 61694,  ...,     0,     0,     0],
        [  101, 10313, 10118,  ...,     0,     0,     0],
        [  101, 13830,   187,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 9.,  3.,  1.,  6.,  9., 10.,  2.,  7.,  3.,  7.,  8.,  1.,  6.,  6.,
         4.,  4.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 40.0226,  84.4694, 133.5628,  86.5803, 112.2400,  72.1763,  67.2432,
         39.1823,  68.3160,  93.7358,  47.5972, 166.7724, 119.9655, 121.8980,
        104.3588,  90.9842], device='cuda:0')
NaN loss detected at iteration 52
input_ids: tensor([[  101, 10236,   125,  ...,     0,     0,     0],
        [  101, 23837, 42031,  ...,   112, 27439,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([9., 0., 6., 3., 8., 4., 4., 7., 6., 0., 7., 6., 6., 5., 1., 0.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 38.9052,  87.0876, 115.2842,  93.8697, 122.6961,  76.3202,   0.3890,
         80.9500, 132.6190,  51.1162,  43.4819,  73.2870,  73.1039,  78.5566,
        210.5288,  47.5635], device='cuda:0')
NaN loss detected at iteration 53
input_ids: tensor([[  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 11982, 10250,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  4.,  3.,  6.,  3.,  1.,  0., 11.,  7.,  0.,  9.,  5., 11.,  8.,
         7.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([107.4737,  26.8834, 128.6153,  54.5772, 116.2469,  68.5316, 109.8450,
        115.3175,  82.1596, 130.6647,  71.4339, 147.5729,  40.4667,  65.8420,
         59.8845, 109.3336], device='cuda:0')
NaN loss detected at iteration 54
input_ids: tensor([[  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 11038, 81470,  ...,     0,     0,     0],
        [  101, 43690, 18460,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 32560, 11624,  ...,     0,     0,     0],
        [  101, 10167, 11906,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  2.,  0.,  6.,  2., 11.,  6.,  3.,  5.,  0.,  7.,  0.,  7.,  6.,
         1.,  9.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 93.5499, 108.6525,  59.4358, 173.7271,  87.7494,  38.9052,  88.4059,
         77.0785,  77.6442, 135.2085,  88.4059, 116.3202,  38.7441,  97.2488,
         75.4086,  94.7326], device='cuda:0')
NaN loss detected at iteration 55
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 25808, 10115,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10243, 37695,  ...,     0,     0,     0],
        [  101, 10236, 79427,  ...,     0,     0,     0],
        [  101, 10313, 13483,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2., 11.,  3.,  2.,  0.,  3.,  3., 12.,  5.,  1.,  6.,  0.,  2.,  0.,
         1.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 83.4905, 128.0522, 129.2656,  64.1393, 116.0514, 154.7226,  90.1144,
         35.3669,  44.6396,  88.6680, 115.2842, 125.9277,  84.2324, 108.4471,
         81.6155, 126.3493], device='cuda:0')
NaN loss detected at iteration 56
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 10599,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 59482, 13439,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([6., 0., 4., 7., 0., 5., 0., 6., 0., 6., 3., 7., 4., 7., 6., 0.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 79.9334,  81.3951, 182.5036,  40.0226, 135.2085,  53.9188, 107.8712,
         73.3689, 109.8450,  54.7379,  90.8130, 102.4977, 103.8811,  88.0273,
         69.4673, 149.5067], device='cuda:0')
NaN loss detected at iteration 57
input_ids: tensor([[  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 10685, 11643,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 36520, 11583,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 0.,  6., 10., 10.,  0.,  9., 12., 11.,  7.,  6.,  6.,  8.,  0.,  1.,
         0.,  9.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([135.2085,  94.6548, 123.8212, 134.4974, 123.5018,  81.6662,  52.7542,
         26.2954,  69.2053,  61.3055,  40.4214,  86.5803,  82.5600, 103.2900,

Training Text Model:   2%|▎                   | 63/4114 [00:20<19:33,  3.45it/s]
NaN loss detected at iteration 58
input_ids: tensor([[  101, 51732, 16719,  ..., 10242, 23368,   102],
        [  101, 13209, 11170,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10796, 10347,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 10167, 10745,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([15.,  0.,  3.,  4.,  2.,  2.,  3.,  1., 13.,  6.,  7.,  5.,  5.,  3.,
         3., 16.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([120.5129,  20.5048,  77.6442,  84.9724,  89.6053,  50.3782,  60.9703,
         59.4358,  97.7242, 119.9655,  61.0244,  48.8803,  77.4418,  80.2644,
        112.1855, 131.9325], device='cuda:0')
NaN loss detected at iteration 59
input_ids: tensor([[  101, 51732, 10762,  ..., 46107, 43904,   102],
        [  101, 11982, 10250,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        ...,
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 11906,  ..., 10372, 10329,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([10.,  6.,  0.,  5., 13.,  4.,  9.,  5.,  4.,  5., 11.,  3., 12.,  0.,
         2.,  9.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 67.2077,  98.7804, 148.3293,  26.0435,  51.6731,  81.6155,  56.1906,
        115.4000,  84.2773,  61.1053,  47.6180, 101.9668,  72.0208,  82.3918,
         87.6992,  26.2954], device='cuda:0')
NaN loss detected at iteration 60
input_ids: tensor([[  101, 15595, 66593,  ...,     0,     0,     0],
        [  101, 46361, 72384,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 10526, 11406,   102],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 7.,  0.,  5.,  6., 13., 11.,  9.,  9.,  1.,  4., 12., 10.,  5.,  4.,
        10.,  8.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 51.3670,  61.3225, 173.9604, 132.7554,  35.3669, 110.2670,  74.3726,
         38.7441,  88.6680, 101.9668, 111.9007,  70.3214,  34.7706,  59.6713,
         78.4014,  69.7060], device='cuda:0')
NaN loss detected at iteration 61
input_ids: tensor([[  101, 12699, 10268,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 13830, 15936,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10167, 46503,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 7.,  5.,  4.,  7.,  5.,  9.,  4.,  0., 11.,  0., 13.,  4.,  6.,  9.,
         7.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 82.3400,  96.0969,  93.5499,  84.2773, 172.2613, 120.7723,  61.0732,
        157.0465,  34.8087,  48.6240,  99.6053, 103.2900,  68.3160, 144.3342,
        111.8594, 109.0582], device='cuda:0')
NaN loss detected at iteration 62
input_ids: tensor([[   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        ...,
        [   101,  11982,  10208,  ...,      0,      0,      0],
        [   101,  11982,  10669,  ...,  10304,  19453,    102],
        [   101,  51732,  16719,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 4.,  1.,  0.,  5.,  6.,  8.,  3.,  6.,  0.,  4.,  1.,  6., 13.,  9.,
         9.,  8.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([182.5036, 101.9698, 116.3202, 109.0582,  62.2565,  57.3288,  91.9116,
        120.4043, 138.2827,  90.9842,  82.0496,  82.7975,  35.3669,  90.3338,
         90.6038, 101.9698], device='cuda:0')
NaN loss detected at iteration 63
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 63672, 10342,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10236, 20560,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 10167, 10118,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 4.,  0.,  8.,  2.,  9.,  8.,  5.,  8.,  0., 10.,  0.,  0.,  0.,  7.,
         0.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([135.9242, 124.0893,  78.1978,  84.2324,  71.6320,  67.9708,  76.3910,
         86.5803,  67.3036, 119.8266,  63.7995, 121.3755, 130.6880,  34.9573,
        142.2642,  87.7494], device='cuda:0')
NaN loss detected at iteration 64
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11699, 65587,  ...,     0,     0,     0],
        ...,
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101,   115,   115,  ...,     0,     0,     0],
        [  101, 10167, 10153,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 3.,  3.,  1.,  7.,  3.,  5.,  4.,  4.,  8.,  3.,  7.,  5.,  2.,  7.,
        11.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 97.8501,  93.8697, 163.9258, 119.9655, 113.6805,  73.4204,  47.1978,
        145.6586, 113.4881,  93.5499,  92.2559,  51.1236,  61.7175,  39.1823,

Training Text Model:   2%|▎                   | 70/4114 [00:22<19:31,  3.45it/s]
NaN loss detected at iteration 65
input_ids: tensor([[  101, 11982,   122,  ...,   165,   182,   102],
        [  101, 52208, 24931,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        ...,
        [  101, 43690, 18460,  ...,     0,     0,     0],
        [  101, 30932, 10681,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([14.,  0.,  0.,  8.,  0.,  5.,  8.,  0.,  0.,  8.,  7.,  4.,  4.,  0.,
         3.,  4.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([120.5129, 117.3248,  51.1162, 107.3419, 100.8273, 111.8594, 100.0372,
        110.2465,  20.5048,  65.8420,  59.8845,  73.4204, 107.6476, 115.9127,
         65.6698,  60.9703], device='cuda:0')
NaN loss detected at iteration 66
input_ids: tensor([[  101, 10167, 30518,  ...,     0,     0,     0],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ..., 79427, 27426,   102],
        [  101, 10672, 74658,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([11.,  0.,  0.,  1.,  1.,  5.,  6.,  6.,  0.,  4.,  5., 11.,  6.,  5.,
         7.,  6.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 43.8032, 109.8450,  88.6680,  63.1706, 111.9271,  84.5759, 111.8594,
         78.4014, 138.2969,  83.4928,  62.9535, 128.0522,  90.0999,  80.4560,
        148.3777, 110.7243], device='cuda:0')
NaN loss detected at iteration 67
input_ids: tensor([[  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10243, 13295,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([5., 7., 3., 1., 8., 0., 0., 3., 5., 4., 2., 5., 0., 5., 0., 4.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 80.4560, 153.8549, 100.8200,  95.8782, 134.9760,  86.5573,  88.5574,
         84.3917,  97.1703,  30.1471, 132.3790,  45.2776,  94.4785, 135.8302,
        162.5845, 156.0057], device='cuda:0')
NaN loss detected at iteration 68
input_ids: tensor([[   101,  10313,    187,  ...,      0,      0,      0],
        [   101,  10167,  15826,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        ...,
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  11982,  66593,  ...,      0,      0,      0],
        [   101,  10796,  65817,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([4., 5., 2., 8., 3., 5., 0., 2., 2., 7., 6., 4., 0., 7., 9., 3.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([103.2900, 112.7749, 116.3202, 125.8537, 101.9668, 149.0346, 133.7706,
         93.8697,  96.5530,  32.1372,  33.2763,  97.3965,  46.3732, 101.9698,
        108.6525, 107.2845], device='cuda:0')
NaN loss detected at iteration 69
input_ids: tensor([[  101, 13830, 11419,  ...,     0,     0,     0],
        [  101, 10796, 48480,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 19728, 10806,  ..., 14332, 49235,   102],
        [  101, 13830, 61694,  ...,     0,     0,     0],
        [  101, 32560, 47873,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2.,  9.,  5.,  5., 12.,  6.,  5.,  0., 13.,  6.,  1.,  0.,  8.,  6.,
         8.,  8.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 50.5625,  72.1763, 103.1871,  50.1364,  97.7242, 101.9232,  76.7720,
        128.6350, 132.5271, 121.6605,  98.7408, 125.9277,  51.4598,  72.0955,
        101.8587,  54.7379], device='cuda:0')
NaN loss detected at iteration 70
input_ids: tensor([[  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 13830, 61694,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 10208,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0., 10.,  3., 12., 13.,  0.,  2.,  3.,  0.,  2.,  9., 11.,  3.,  7.,
         5.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 46.3732,  55.7802,  97.1703, 147.8534,  97.7242, 107.1292,  72.0955,
         83.4928, 149.5067,  69.5174, 111.9007,  34.8087, 116.2469,  47.1978,
         89.8892,  86.2927], device='cuda:0')
NaN loss detected at iteration 71
input_ids: tensor([[  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 13029, 10119,  ...,     0,     0,     0],
        [  101,   146, 29239,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 13830, 92746,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  0.,  0., 10.,  5.,  7.,  6.,  6.,  0.,  6.,  0.,  3.,  0.,  5.,
         3.,  9.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 92.2002, 114.0468,  66.3855,  26.2954, 125.9519, 110.0655, 101.9668,
        111.0106, 133.1675,  84.4694,  99.5081, 113.4881, 153.9068,  40.0226,

Training Text Model:   2%|▎                   | 77/4114 [00:24<19:29,  3.45it/s]
NaN loss detected at iteration 72
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        ...,
        [  101, 10313, 10312,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 13830, 10788,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([11.,  6.,  7.,  6.,  4.,  6.,  2.,  5.,  4.,  5.,  1.,  0.,  1.,  5.,
         2.,  6.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 64.0918,  69.2053,  51.1236,  61.3055, 117.8280, 113.9276,  63.3525,
        111.8482, 105.8216, 113.9008,  55.4237, 113.6154,  97.5631, 162.6413,
        113.8676, 101.9232], device='cuda:0')
NaN loss detected at iteration 73
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 56819, 10790,  ...,     0,     0,     0],
        [  101, 10313, 10312,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  2.,  5.,  2.,  6.,  6.,  9.,  9.,  6.,  4.,  1., 10.,  1.,  1.,
         0.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 83.9148,  65.6698, 162.6413, 104.8592,  47.1133,  61.0732,  39.0712,
         47.5972, 135.0588,  47.1978, 101.4394,  38.8275,  86.4483,  77.8959,
        137.2040,  69.4673], device='cuda:0')
NaN loss detected at iteration 74
input_ids: tensor([[  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0],
        ...,
        [  101, 10672, 74658,  ...,     0,     0,     0],
        [  101, 13830, 10788,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([5., 5., 2., 0., 7., 7., 3., 8., 8., 0., 3., 3., 1., 2., 4., 6.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 61.0732,  63.6273,  83.4928, 200.6644,  86.5803,  68.3160,  55.4237,
         64.0918,  89.2187, 165.7333,  71.9936, 116.0713,  74.3483,  92.1605,
         90.5818,  72.3660], device='cuda:0')
NaN loss detected at iteration 75
input_ids: tensor([[  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 13830, 10788,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  1., 13.,  3.,  0.,  4.,  0., 13., 10.,  7.,  1.,  3.,  7.,  4.,
         3., 11.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([142.8494,  43.4819, 103.7605, 113.8676, 178.6319,  32.5057, 108.6488,
        112.4636,  35.3669, 124.5378,  97.3259, 157.2870,  40.0226, 176.7817,
         71.3136, 109.3474], device='cuda:0')
NaN loss detected at iteration 76
input_ids: tensor([[  101, 10167, 11906,  ...,     0,     0,     0],
        [  101, 10167, 50855,  ...,     0,     0,     0],
        [  101, 10167, 10745,  ...,     0,     0,     0],
        ...,
        [  101, 71164, 48865,  ...,     0,     0,     0],
        [  101, 13830,   187,  ...,     0,     0,     0],
        [  101, 32580, 14657,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([11.,  1.,  6.,  2.,  5.,  0.,  8.,  5.,  1.,  1.,  7.,  6.,  5.,  0.,
         8.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 97.0673,  74.3483, 109.2680,  98.8139,  88.6642, 119.5634, 119.4386,
         72.2873,  86.4483, 117.3278,  69.7060,  69.1885,  61.0732, 102.7199,
         64.1393, 113.2726], device='cuda:0')
NaN loss detected at iteration 77
input_ids: tensor([[  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 13830, 15936,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        ...,
        [  101, 10243, 22873,  ...,     0,     0,     0],
        [  101, 13830, 79427,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([0., 6., 1., 0., 5., 3., 0., 1., 2., 3., 6., 2., 4., 1., 2., 9.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 66.2478,  82.3400, 113.6154, 133.7858,  81.6155,  91.9994, 148.3293,
        116.2032,  81.8949,  44.6396,  88.8918,  65.6698,  98.7767, 133.1675,
        133.5628,  73.2870], device='cuda:0')
NaN loss detected at iteration 78
input_ids: tensor([[  101, 13830, 39710,  ...,     0,     0,     0],
        [  101, 21851, 72894,  ...,     0,     0,     0],
        [  101, 10167, 12231,  ...,     0,     0,     0],
        ...,
        [  101, 11982, 10250,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ..., 20856, 10307,   102],
        [  101, 13338, 49316,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 4.,  0.,  0.,  0.,  4.,  5.,  0.,  1.,  7., 11.,  0.,  7.,  0.,  8.,
         9.,  0.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([124.5378,  78.4597,  86.9333, 124.4810,  61.0732,  88.8918,  87.0876,
         68.6087,  34.7706,  67.6765,  68.3333,  74.9864, 168.4754, 109.5770,

Training Text Model:   2%|▍                   | 84/4114 [00:26<19:26,  3.45it/s]
NaN loss detected at iteration 79
input_ids: tensor([[   101,  11982,  66593,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,    135,    133,    102],
        [   101,  29224,  45243,  ...,      0,      0,      0],
        ...,
        [   101,  13338, 101328,  ...,      0,      0,      0],
        [   101,  13209,  11479,  ...,      0,      0,      0],
        [   101,  15595,    122,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([7., 8., 3., 0., 9., 2., 8., 7., 0., 7., 7., 0., 6., 0., 0., 5.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 96.0969, 113.4881, 129.2656, 113.1563,  72.1763,  82.3855, 109.2685,
         79.9334,  91.8566, 120.5129, 151.7578,  85.9820,  76.8517, 105.1226,
        130.6880, 115.2842], device='cuda:0')
NaN loss detected at iteration 80
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830, 20560,  ...,     0,     0,     0],
        [  101, 51732, 10762,  ..., 37432, 10304,   102],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830, 61694,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ..., 10136, 10953,   102]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 1.,  3.,  8.,  5., 12.,  8.,  5.,  0.,  7., 10.,  8.,  6.,  0.,  4.,
         6., 15.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 80.8954,  81.6155, 120.5129, 135.0588,  47.6180, 120.5129,  62.2565,
        101.7443,  79.9334, 146.7394,  99.9530, 210.5288, 168.4754,  84.2773,
        113.4881, 120.5129], device='cuda:0')
NaN loss detected at iteration 81
input_ids: tensor([[   101,  10167,  10118,  ...,      0,      0,      0],
        [   101,  13830, 101328,  ...,      0,      0,      0],
        [   101,  10167,  10599,  ...,    119,    133,    102],
        ...,
        [   101,    150,  14902,  ...,      0,      0,      0],
        [   101, 102204,  40405,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 2.,  3.,  6.,  4.,  2.,  3.,  4.,  5., 10.,  7.,  6.,  7., 10.,  4.,
         9.,  6.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 86.8030,  46.0141, 141.1521,  51.7564,  76.8135,  93.5499, 174.2724,
         61.0732,  81.4317,  91.0425, 112.9181, 132.5085,  56.1906,  51.7564,
         71.6320,  67.3493], device='cuda:0')
NaN loss detected at iteration 82
input_ids: tensor([[  101, 13029, 10119,  ...,     0,     0,     0],
        [  101, 10313, 77807,  ...,     0,     0,     0],
        [  101, 10167, 50855,  ...,     0,     0,     0],
        ...,
        [  101, 10167, 20220,  ..., 12576, 12964,   102],
        [  101, 20483, 10268,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  3.,  8.,  0.,  8.,  2.,  4.,  0.,  7.,  2.,  2.,  0.,  7., 11.,
         7.,  7.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([109.7281, 179.5784,  58.0071, 128.6350, 119.1181, 146.5804,  87.7185,
        147.8677,  81.6662,  64.1393,  46.3732, 187.4622,  90.0999,  92.0227,
         61.3055,  69.1885], device='cuda:0')
NaN loss detected at iteration 83
input_ids: tensor([[  101, 13068, 11369,  ..., 11583, 10726,   102],
        [  101, 27426, 15527,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10313, 31300,  ...,   119,   102,     0],
        [  101, 12664, 10745,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([11.,  6.,  8.,  3.,  0.,  9.,  6.,  6.,  6.,  0.,  4.,  1.,  1.,  4.,
        16.,  6.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 60.2439,  97.2488,  40.0226,  35.0088, 122.9194, 119.9536,  66.6077,
         97.2488,  95.8782,  89.3835,  85.2361, 198.0511,  47.5635,  78.8352,
         37.1506,  68.5149], device='cuda:0')
NaN loss detected at iteration 84
input_ids: tensor([[   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  11500,    187,  ...,      0,      0,      0],
        ...,
        [   101,  11982,  10669,  ...,      0,      0,      0],
        [   101,  71164,  48865,  ...,      0,      0,      0],
        [   101,  51732,  10762,  ...,  15062,  45258,    102]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([ 2.,  0.,  2., 12.,  8.,  3.,  2.,  1.,  2.,  6.,  5.,  0., 10.,  6.,
         0., 11.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 81.8949,  59.1710, 107.2845, 111.9007,  72.0955,  68.3160, 145.2906,
        103.2843,  83.4928,  68.3160, 134.1044,  47.5635,  72.1763,  71.4302,
        164.5873,  85.8097], device='cuda:0')
NaN loss detected at iteration 85
input_ids: tensor([[  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0],
        [  101, 10236, 39710,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 13830,   187,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 6.,  3., 11.,  5.,  2.,  9.,  0.,  3.,  4.,  7.,  4.,  0.,  6.,  5.,
         5.,  5.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 84.3884,  63.7042,  77.9310,  98.9470,  54.7379, 100.2758,  78.7654,
         61.2797,  76.2296,  88.0273,  67.3493,  71.2844, 119.9655,  84.3884,

Training Text Model:   2%|▍                   | 91/4114 [00:28<19:26,  3.45it/s]
NaN loss detected at iteration 86
input_ids: tensor([[   101,  51732,  16719,  ...,  10848,  10359,    102],
        [   101,  14321,  71843,  ...,    120,    135,    102],
        [   101,  11982,    122,  ...,      0,      0,      0],
        ...,
        [   101,  13830,  79427,  ...,      0,      0,      0],
        [   101,  73784, 104046,  ...,      0,      0,      0],
        [   101,  11982,  66593,  ...,  10147, 104278,    102]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
keyword_count: tensor([12.,  7.,  3.,  6.,  9.,  6.,  0.,  2.,  4.,  7.,  0.,  9.,  3.,  5.,
         0., 16.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 34.8087, 111.8594,  52.9569,  40.2379, 119.9655,  76.2833,  88.6680,
         50.5625,  76.8517,  55.8393, 118.8089,  39.1823, 115.5245, 108.4150,
        113.1563,  94.6859], device='cuda:0')
NaN loss detected at iteration 87
input_ids: tensor([[  101, 51732, 10762,  ...,     0,     0,     0],
        [  101, 19728, 10806,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        ...,
        [  101, 10556,   131,  ...,   144, 17894,   102],
        [  101, 32580, 10260,  ...,     0,     0,     0],
        [  101, 10236, 79427,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 7.,  2.,  1., 13.,  2.,  6.,  5.,  5.,  4.,  6., 12.,  7.,  8.,  9.,
         0.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 40.0226,  72.0955, 101.8930, 134.9760,  81.6155,  72.0955,  97.0279,
         92.0203, 151.8966,  68.3160, 113.5054, 138.7497,  74.9581,  60.1338,
        108.6488,  87.5872], device='cuda:0')
NaN loss detected at iteration 88
input_ids: tensor([[  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 10669,  ...,     0,     0,     0],
        ...,
        [  101, 13830, 10788,  ...,     0,     0,     0],
        [  101, 22687, 10141,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([2., 3., 7., 4., 3., 2., 0., 1., 2., 9., 7., 1., 0., 5., 0., 0.],
       device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 96.9368,  93.1135,  81.6155, 169.5490,  77.8959,  50.5625, 105.6918,
         42.4238,  54.0846,  83.7236, 162.6597,   6.2414,  87.4991,  53.9188,
         46.3732,  66.3063], device='cuda:0')
NaN loss detected at iteration 89
input_ids: tensor([[  101, 52208, 24931,  ...,     0,     0,     0],
        [  101, 10236, 39710,  ..., 21181,   136,   102],
        [  101, 20220, 11643,  ...,     0,     0,     0],
        ...,
        [  101, 11699, 13207,  ...,     0,     0,     0],
        [  101, 13830, 10211,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  5.,  2.,  8., 10.,  2.,  4.,  0.,  6.,  4.,  4.,  8.,  0., 11.,
         2.,  9.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 46.3732,  44.4890,  45.8834,  69.3624,  38.8275,  76.8135,  90.9842,
        128.6350,  18.7737,  51.7564,  91.9739, 101.9698, 116.3786,  35.3669,
        132.3790, 118.6371], device='cuda:0')
NaN loss detected at iteration 90
input_ids: tensor([[  101, 50611, 22913,  ...,     0,     0,     0],
        [  101, 13338, 49316,  ...,     0,     0,     0],
        [  101, 13830, 11744,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 10313,   187,  ...,     0,     0,     0],
        [  101, 11982,   122,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0.,  0.,  8.,  3.,  3., 12.,  5.,  1.,  5.,  1.,  2.,  6.,  6.,  1.,
         7., 10.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([128.6350,  66.3063, 119.9655, 209.6592,  71.4191,  47.6180,  34.7706,
        157.4288,  97.3965, 107.4737,  77.8959, 136.1487,  53.9188,  73.2969,
        111.9007,  69.8303], device='cuda:0')
NaN loss detected at iteration 91
input_ids: tensor([[  101, 51732, 16719,  ...,   165,   182,   102],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        ...,
        [  101, 10796, 48480,  ...,     0,     0,     0],
        [  101,   151,   112,  ...,     0,     0,     0],
        [  101, 11505, 19716,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([11.,  9.,  7.,  3.,  6.,  0.,  4., 11.,  3., 12., 12.,  2.,  0.,  4.,
         1.,  5.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 26.2954, 108.6525, 134.1044,  92.7058, 134.9760, 113.6154,  52.9569,
         35.3669,  36.2959,  83.7236, 132.5271,  85.7035, 105.1368, 116.4669,
        101.1622, 120.4043], device='cuda:0')
NaN loss detected at iteration 92
input_ids: tensor([[  101, 15595, 10669,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 11982, 66593,  ...,     0,     0,     0],
        ...,
        [  101, 11505, 19716,  ...,     0,     0,     0],
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 12786, 11044,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 6.,  9.,  7.,  5.,  4.,  4., 10.,  6.,  2.,  5.,  8.,  5.,  6.,  2.,
         3.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([103.1871,  69.1885,  98.0629, 143.0850,  85.3903,  61.0732, 124.6198,
         67.0275,  77.8959, 169.9366,  11.2538,  61.0732,  84.4694,  64.1393,
Training Text Model:   2%|▍                   | 96/4114 [00:30<21:24,  3.13it/s]
Traceback (most recent call last):
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/combined_model.py", line 235, in <module>
    train_embeddings, test_embeddings = train_text_model(model, train_loader, test_loader, criterion, optimizer, scaler, scheduler, device, EPOCHS, save_path="model/text_model_full.pth")
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/combined_model.py", line 195, in train_text_model
    train_loss, train_mse, train_embeddings = train_epoch_text(model, train_loader, criterion, optimizer, scaler, device)
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/combined_model.py", line 139, in train_epoch_text
    scaler.scale(loss).backward()
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: an illegal memory access was encountered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
NaN loss detected at iteration 93
input_ids: tensor([[   101,  64766, 109067,  ...,      0,      0,      0],
        [   101,  13029,  10119,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        ...,
        [   101,  71164,  48865,  ...,      0,      0,      0],
        [   101,  11982,    122,  ...,      0,      0,      0],
        [   101,  13830,  92746,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  1.,  3., 11.,  9.,  6.,  6.,  7.,  8.,  7.,  5.,  8.,  4.,  0.,
         4.,  2.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 78.4597,  84.8629, 147.6293,  53.4073, 132.2298,  87.6222,  97.2488,
         61.4850,  97.9422,  61.5721,  88.8918,  87.4201,  69.2053, 164.5873,
         88.8247, 105.8216], device='cuda:0')
NaN loss detected at iteration 94
input_ids: tensor([[  101, 13209, 11479,  ...,     0,     0,     0],
        [  101, 13830,   124,  ...,     0,     0,     0],
        [  101,   149,   112,  ...,     0,     0,     0],
        ...,
        [  101, 51732, 16719,  ...,     0,     0,     0],
        [  101, 22034, 10307,  ..., 10112, 39774,   102],
        [  101, 10167, 10118,  ...,     0,     0,     0]], device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 1.,  4.,  0.,  8., 12.,  1., 11.,  2.,  1.,  0.,  7.,  5.,  9., 11.,
         6.,  3.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([111.2677,  80.4560,  20.5048,  79.9334,  11.2538, 163.9258,  56.0026,
         72.3660, 157.4288,  58.3448,  82.9407,  92.3754,  61.4850, 132.5271,
         72.0955, 132.6302], device='cuda:0')
NaN loss detected at iteration 95
input_ids: tensor([[   101,    138,  15864,  ...,      0,      0,      0],
        [   101,    163,  24820,  ...,  55732,  25547,    102],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        ...,
        [   101,  10313, 108522,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0],
        [   101,  10796,  26130,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 0., 12.,  4.,  0.,  6.,  5.,  2.,  1., 12.,  0.,  0.,  9.,  6.,  1.,
         6.,  1.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([123.5018,  61.4850,  40.5938, 151.5048,  63.6273, 108.7571, 108.6525,
        114.9781, 154.8282, 133.7706, 123.5018,  54.7379,  62.3935,  69.0207,
        101.7411, 101.5494], device='cuda:0')
NaN loss detected at iteration 96
input_ids: tensor([[   101,  11505,  19716,  ...,      0,      0,      0],
        [   101,  13338,  49316,  ...,      0,      0,      0],
        [   101,  10167,  10118,  ...,      0,      0,      0],
        ...,
        [   101,  51732,  10762,  ...,      0,      0,      0],
        [   101,  13830, 101328,  ...,      0,      0,      0],
        [   101,  51732,  16719,  ...,      0,      0,      0]],
       device='cuda:0')
attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
keyword_count: tensor([ 6.,  0., 11.,  5.,  1.,  2.,  0.,  2.,  0.,  0.,  4.,  6.,  8., 12.,
         8.,  4.], device='cuda:0')
outputs: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)
targets: tensor([ 72.0208,  88.6680, 160.1040,  51.1265,  86.4483,  69.5174,  90.9860,
         30.1471, 109.9162, 159.8013, 107.6476,  52.9569,  73.1039,  35.3669,
         66.6077,  65.7311], device='cuda:0')