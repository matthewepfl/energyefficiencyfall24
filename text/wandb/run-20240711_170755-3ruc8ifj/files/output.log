[I 2024-07-11 17:08:15,712] A new study created in memory with name: no-name-2a050d8b-09f3-4df7-aef8-0fc8575d17ff
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 7115.29852| val_0_rmse: 219.99548|  0:00:05s
epoch 1  | loss: 1692.84048| val_0_rmse: 92.37167|  0:00:09s
epoch 2  | loss: 1021.40817| val_0_rmse: 32.6997 |  0:00:13s
epoch 3  | loss: 791.71638| val_0_rmse: 31.38492|  0:00:17s
epoch 4  | loss: 614.48454| val_0_rmse: 24.85603|  0:00:21s
epoch 5  | loss: 521.30097| val_0_rmse: 22.23986|  0:00:26s
epoch 6  | loss: 464.45055| val_0_rmse: 20.55075|  0:00:30s
epoch 7  | loss: 425.8455| val_0_rmse: 20.52521|  0:00:34s
epoch 8  | loss: 400.57222| val_0_rmse: 20.2816 |  0:00:38s
epoch 9  | loss: 372.01824| val_0_rmse: 19.40734|  0:00:42s
epoch 10 | loss: 347.52908| val_0_rmse: 18.62127|  0:00:46s
epoch 11 | loss: 326.14477| val_0_rmse: 20.71438|  0:00:50s
epoch 12 | loss: 317.68614| val_0_rmse: 18.43119|  0:00:54s
epoch 13 | loss: 299.86712| val_0_rmse: 17.8468 |  0:00:58s
epoch 14 | loss: 291.96273| val_0_rmse: 17.31479|  0:01:03s
epoch 15 | loss: 278.1665| val_0_rmse: 16.75093|  0:01:07s
epoch 16 | loss: 274.31908| val_0_rmse: 16.61116|  0:01:11s
epoch 17 | loss: 256.36594| val_0_rmse: 16.08014|  0:01:15s
epoch 18 | loss: 245.8593| val_0_rmse: 16.19051|  0:01:19s
epoch 19 | loss: 247.59601| val_0_rmse: 16.15222|  0:01:23s
epoch 20 | loss: 239.29318| val_0_rmse: 15.58911|  0:01:27s
epoch 21 | loss: 229.21955| val_0_rmse: 15.50349|  0:01:31s
epoch 22 | loss: 231.68808| val_0_rmse: 15.8317 |  0:01:35s
epoch 23 | loss: 223.51453| val_0_rmse: 15.35663|  0:01:39s
epoch 24 | loss: 213.50382| val_0_rmse: 15.30825|  0:01:43s
epoch 25 | loss: 212.80862| val_0_rmse: 15.28442|  0:01:47s
epoch 26 | loss: 204.04921| val_0_rmse: 14.9929 |  0:01:52s
epoch 27 | loss: 199.50076| val_0_rmse: 14.91899|  0:01:56s
epoch 28 | loss: 201.42172| val_0_rmse: 14.88999|  0:02:00s
epoch 29 | loss: 189.68508| val_0_rmse: 14.874  |  0:02:04s
epoch 30 | loss: 188.55749| val_0_rmse: 14.17123|  0:02:08s
epoch 31 | loss: 187.66357| val_0_rmse: 14.01017|  0:02:12s
epoch 32 | loss: 184.95965| val_0_rmse: 14.34528|  0:02:16s
epoch 33 | loss: 187.00764| val_0_rmse: 14.0337 |  0:02:20s
epoch 34 | loss: 174.65695| val_0_rmse: 13.84896|  0:02:24s
epoch 35 | loss: 176.58371| val_0_rmse: 14.01211|  0:02:28s
epoch 36 | loss: 174.59424| val_0_rmse: 13.9827 |  0:02:32s
epoch 37 | loss: 169.0606| val_0_rmse: 14.16282|  0:02:36s
epoch 38 | loss: 165.22451| val_0_rmse: 13.90825|  0:02:40s
epoch 39 | loss: 167.34659| val_0_rmse: 14.21424|  0:02:44s
epoch 40 | loss: 163.4425| val_0_rmse: 13.61466|  0:02:49s
epoch 41 | loss: 161.0122| val_0_rmse: 13.60195|  0:02:53s
epoch 42 | loss: 162.27259| val_0_rmse: 13.62421|  0:02:57s
epoch 43 | loss: 160.47359| val_0_rmse: 13.43878|  0:03:01s
epoch 44 | loss: 156.14193| val_0_rmse: 13.50987|  0:03:05s
epoch 45 | loss: 156.72714| val_0_rmse: 13.45884|  0:03:09s
epoch 46 | loss: 153.37572| val_0_rmse: 14.01881|  0:03:13s
epoch 47 | loss: 152.87161| val_0_rmse: 13.14353|  0:03:17s
epoch 48 | loss: 152.60594| val_0_rmse: 13.42166|  0:03:21s
epoch 49 | loss: 146.47419| val_0_rmse: 13.46767|  0:03:25s
epoch 50 | loss: 151.55506| val_0_rmse: 13.62266|  0:03:29s
epoch 51 | loss: 145.94644| val_0_rmse: 13.63262|  0:03:33s
epoch 52 | loss: 149.83051| val_0_rmse: 13.19169|  0:03:37s
epoch 53 | loss: 143.7237| val_0_rmse: 13.17122|  0:03:41s
epoch 54 | loss: 146.89461| val_0_rmse: 13.33369|  0:03:45s
epoch 55 | loss: 146.80325| val_0_rmse: 12.89144|  0:03:49s
epoch 56 | loss: 140.89421| val_0_rmse: 13.38817|  0:03:54s
epoch 57 | loss: 142.01874| val_0_rmse: 13.09387|  0:03:58s
epoch 58 | loss: 143.55031| val_0_rmse: 13.0547 |  0:04:02s
epoch 59 | loss: 144.24219| val_0_rmse: 13.28088|  0:04:06s
epoch 60 | loss: 138.4725| val_0_rmse: 12.706  |  0:04:10s
epoch 61 | loss: 137.07501| val_0_rmse: 12.88589|  0:04:14s
epoch 62 | loss: 135.81646| val_0_rmse: 12.69882|  0:04:18s
epoch 63 | loss: 137.15695| val_0_rmse: 12.78064|  0:04:22s
epoch 64 | loss: 136.75802| val_0_rmse: 13.21543|  0:04:26s
epoch 65 | loss: 127.79115| val_0_rmse: 13.86422|  0:04:30s
epoch 66 | loss: 133.56377| val_0_rmse: 13.31073|  0:04:34s
epoch 67 | loss: 131.54764| val_0_rmse: 12.84705|  0:04:38s
epoch 68 | loss: 131.96832| val_0_rmse: 12.92659|  0:04:42s
epoch 69 | loss: 128.87926| val_0_rmse: 12.73789|  0:04:46s
epoch 70 | loss: 130.06348| val_0_rmse: 12.58778|  0:04:51s
epoch 71 | loss: 131.06528| val_0_rmse: 13.04136|  0:04:55s
epoch 72 | loss: 125.37032| val_0_rmse: 12.63371|  0:04:59s
epoch 73 | loss: 129.49734| val_0_rmse: 13.04165|  0:05:03s
epoch 74 | loss: 131.2935| val_0_rmse: 12.70622|  0:05:07s
epoch 75 | loss: 124.31386| val_0_rmse: 12.70072|  0:05:11s
epoch 76 | loss: 126.03505| val_0_rmse: 12.4252 |  0:05:15s
epoch 77 | loss: 122.21394| val_0_rmse: 12.45398|  0:05:19s
epoch 78 | loss: 130.90437| val_0_rmse: 12.62067|  0:05:23s
epoch 79 | loss: 123.22407| val_0_rmse: 12.49302|  0:05:27s
epoch 80 | loss: 122.8161| val_0_rmse: 12.67143|  0:05:31s
epoch 81 | loss: 125.80308| val_0_rmse: 12.59668|  0:05:35s
epoch 82 | loss: 119.77198| val_0_rmse: 12.33598|  0:05:39s
epoch 83 | loss: 127.07374| val_0_rmse: 12.72345|  0:05:43s
epoch 84 | loss: 123.60763| val_0_rmse: 12.887  |  0:05:47s
epoch 85 | loss: 122.69608| val_0_rmse: 12.43394|  0:05:52s
epoch 86 | loss: 120.52071| val_0_rmse: 12.32169|  0:05:56s
epoch 87 | loss: 122.85488| val_0_rmse: 13.12131|  0:06:00s
epoch 88 | loss: 117.87491| val_0_rmse: 12.34491|  0:06:04s
epoch 89 | loss: 122.28335| val_0_rmse: 12.39094|  0:06:08s
epoch 90 | loss: 117.16518| val_0_rmse: 12.28574|  0:06:12s
epoch 91 | loss: 115.39115| val_0_rmse: 12.58176|  0:06:16s
epoch 92 | loss: 122.39405| val_0_rmse: 12.66039|  0:06:20s
epoch 93 | loss: 121.14196| val_0_rmse: 12.48438|  0:06:24s
epoch 94 | loss: 116.10213| val_0_rmse: 12.86382|  0:06:28s
epoch 95 | loss: 119.28455| val_0_rmse: 12.737  |  0:06:32s
epoch 96 | loss: 117.83801| val_0_rmse: 12.41777|  0:06:36s
epoch 97 | loss: 117.34924| val_0_rmse: 12.65413|  0:06:41s
epoch 98 | loss: 117.31105| val_0_rmse: 12.52667|  0:06:45s
epoch 99 | loss: 116.54209| val_0_rmse: 12.54688|  0:06:49s
epoch 100| loss: 117.21194| val_0_rmse: 12.26192|  0:06:53s
epoch 101| loss: 113.7623| val_0_rmse: 12.67115|  0:06:57s
epoch 102| loss: 116.39205| val_0_rmse: 12.55468|  0:07:01s
epoch 103| loss: 119.85374| val_0_rmse: 11.99468|  0:07:05s
epoch 104| loss: 113.19365| val_0_rmse: 12.27365|  0:07:09s
epoch 105| loss: 111.80013| val_0_rmse: 12.12659|  0:07:13s
epoch 106| loss: 118.41338| val_0_rmse: 12.35715|  0:07:17s
epoch 107| loss: 113.07927| val_0_rmse: 12.30852|  0:07:21s
epoch 108| loss: 113.27954| val_0_rmse: 12.24685|  0:07:25s
epoch 109| loss: 117.16129| val_0_rmse: 12.65986|  0:07:29s
epoch 110| loss: 114.18113| val_0_rmse: 12.59446|  0:07:33s
epoch 111| loss: 111.32817| val_0_rmse: 12.459  |  0:07:37s
epoch 112| loss: 116.98104| val_0_rmse: 12.34583|  0:07:41s
epoch 113| loss: 110.80332| val_0_rmse: 12.35991|  0:07:46s
epoch 114| loss: 113.3527| val_0_rmse: 12.31557|  0:07:50s
epoch 115| loss: 113.56037| val_0_rmse: 12.32394|  0:07:54s
epoch 116| loss: 111.32388| val_0_rmse: 12.16922|  0:07:58s
epoch 117| loss: 116.76363| val_0_rmse: 12.50836|  0:08:02s
epoch 118| loss: 112.52212| val_0_rmse: 12.3306 |  0:08:06s
epoch 119| loss: 107.72697| val_0_rmse: 12.11494|  0:08:10s
epoch 120| loss: 109.34978| val_0_rmse: 12.54933|  0:08:14s
epoch 121| loss: 110.08197| val_0_rmse: 12.45253|  0:08:18s
epoch 122| loss: 109.99297| val_0_rmse: 12.06886|  0:08:22s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 123| loss: 110.06855| val_0_rmse: 12.14001|  0:08:26s
Early stopping occurred at epoch 123 with best_epoch = 103 and best_val_0_rmse = 11.99468
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 17:16:52,549] Trial 0 finished with value: 11.994676293941609 and parameters: {'lr': 0.009226995405096082, 'n_steps': 3, 'gamma': 1.1071457801743374, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.0400663967532432, 'weight_decay': 3.1821403066088324e-05, 'batch_size': 256, 'virtual_batch_size': 32}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8620.26074| val_0_rmse: 94.01638|  0:00:22s
epoch 1  | loss: 8155.83548| val_0_rmse: 73.89017|  0:00:43s
epoch 2  | loss: 1950.86353| val_0_rmse: 34.90223|  0:01:05s
epoch 3  | loss: 1045.1049| val_0_rmse: 35.78034|  0:01:27s
epoch 4  | loss: 1034.54156| val_0_rmse: 33.31642|  0:01:49s
epoch 5  | loss: 1021.58416| val_0_rmse: 32.83218|  0:02:11s
epoch 6  | loss: 999.90447| val_0_rmse: 31.5778 |  0:02:33s
epoch 7  | loss: 979.93576| val_0_rmse: 32.46854|  0:02:55s
epoch 8  | loss: 970.30496| val_0_rmse: 34.18996|  0:03:17s
epoch 9  | loss: 961.2576| val_0_rmse: 31.58217|  0:03:39s
epoch 10 | loss: 946.31463| val_0_rmse: 31.03703|  0:04:01s
epoch 11 | loss: 915.08134| val_0_rmse: 30.85771|  0:04:23s
epoch 12 | loss: 906.29831| val_0_rmse: 30.24627|  0:04:45s
epoch 13 | loss: 881.05472| val_0_rmse: 29.91556|  0:05:07s
epoch 14 | loss: 820.01293| val_0_rmse: 30.08138|  0:05:29s
epoch 15 | loss: 742.36226| val_0_rmse: 27.73803|  0:05:51s
epoch 16 | loss: 665.14441| val_0_rmse: 26.45831|  0:06:13s
epoch 17 | loss: 596.51884| val_0_rmse: 25.63404|  0:06:35s
epoch 18 | loss: 547.78357| val_0_rmse: 24.0575 |  0:06:57s
epoch 19 | loss: 509.03755| val_0_rmse: 23.26103|  0:07:19s
epoch 20 | loss: 469.4116| val_0_rmse: 22.73118|  0:07:41s
epoch 21 | loss: 445.51408| val_0_rmse: 23.40094|  0:08:03s
epoch 22 | loss: 420.17291| val_0_rmse: 22.45934|  0:08:25s
epoch 23 | loss: 408.5236| val_0_rmse: 23.23927|  0:08:46s
epoch 24 | loss: 389.19332| val_0_rmse: 21.82569|  0:09:08s
epoch 25 | loss: 366.91563| val_0_rmse: 21.16062|  0:09:30s
epoch 26 | loss: 353.98825| val_0_rmse: 21.73403|  0:09:52s
epoch 27 | loss: 340.91685| val_0_rmse: 21.08473|  0:10:14s
epoch 28 | loss: 321.36534| val_0_rmse: 20.01837|  0:10:36s
epoch 29 | loss: 300.91218| val_0_rmse: 19.82864|  0:10:58s
epoch 30 | loss: 296.74723| val_0_rmse: 19.72124|  0:11:20s
epoch 31 | loss: 285.24973| val_0_rmse: 19.40917|  0:11:42s
epoch 32 | loss: 275.5285| val_0_rmse: 19.44009|  0:12:04s
epoch 33 | loss: 268.04981| val_0_rmse: 19.16899|  0:12:26s
epoch 34 | loss: 255.96788| val_0_rmse: 19.06035|  0:12:47s
epoch 35 | loss: 259.09472| val_0_rmse: 20.01843|  0:13:09s
epoch 36 | loss: 253.34  | val_0_rmse: 18.39021|  0:13:31s
epoch 37 | loss: 249.61211| val_0_rmse: 18.91513|  0:13:53s
epoch 38 | loss: 240.18335| val_0_rmse: 18.3508 |  0:14:15s
epoch 39 | loss: 235.91451| val_0_rmse: 19.01106|  0:14:37s
epoch 40 | loss: 232.08242| val_0_rmse: 18.13678|  0:14:59s
epoch 41 | loss: 227.53466| val_0_rmse: 19.12015|  0:15:21s
epoch 42 | loss: 217.80531| val_0_rmse: 18.86971|  0:15:43s
epoch 43 | loss: 215.69209| val_0_rmse: 18.33831|  0:16:05s
epoch 44 | loss: 208.73554| val_0_rmse: 18.53002|  0:16:27s
epoch 45 | loss: 212.11154| val_0_rmse: 17.34651|  0:16:48s
epoch 46 | loss: 204.95329| val_0_rmse: 18.84113|  0:17:10s
epoch 47 | loss: 203.52987| val_0_rmse: 18.19045|  0:17:32s
epoch 48 | loss: 199.15747| val_0_rmse: 19.97607|  0:17:54s
epoch 49 | loss: 192.12193| val_0_rmse: 18.06456|  0:18:16s
epoch 50 | loss: 193.91771| val_0_rmse: 17.81061|  0:18:38s
epoch 51 | loss: 190.37322| val_0_rmse: 18.96444|  0:19:00s
epoch 52 | loss: 177.71004| val_0_rmse: 18.98845|  0:19:22s
epoch 53 | loss: 178.97469| val_0_rmse: 18.14612|  0:19:44s
epoch 54 | loss: 179.36826| val_0_rmse: 18.82563|  0:20:06s
epoch 55 | loss: 175.22811| val_0_rmse: 17.56802|  0:20:28s
epoch 56 | loss: 177.17123| val_0_rmse: 17.06043|  0:20:50s
epoch 57 | loss: 172.3571| val_0_rmse: 18.38916|  0:21:12s
epoch 58 | loss: 170.02812| val_0_rmse: 17.56341|  0:21:34s
epoch 59 | loss: 171.84781| val_0_rmse: 16.50393|  0:21:56s
epoch 60 | loss: 169.8101| val_0_rmse: 17.69673|  0:22:17s
epoch 61 | loss: 166.1749| val_0_rmse: 17.11171|  0:22:39s
epoch 62 | loss: 169.05714| val_0_rmse: 17.33945|  0:23:01s
epoch 63 | loss: 161.15534| val_0_rmse: 24.33243|  0:23:23s
epoch 64 | loss: 164.20785| val_0_rmse: 17.44964|  0:23:45s
epoch 65 | loss: 160.47368| val_0_rmse: 17.76532|  0:24:07s
epoch 66 | loss: 161.56831| val_0_rmse: 18.86108|  0:24:29s
epoch 67 | loss: 166.38577| val_0_rmse: 17.16985|  0:24:51s
epoch 68 | loss: 158.66684| val_0_rmse: 18.15651|  0:25:13s
epoch 69 | loss: 159.78161| val_0_rmse: 17.68007|  0:25:35s
epoch 70 | loss: 154.86876| val_0_rmse: 18.38761|  0:25:57s
epoch 71 | loss: 155.67095| val_0_rmse: 18.87355|  0:26:19s
epoch 72 | loss: 151.70929| val_0_rmse: 17.83366|  0:26:40s
epoch 73 | loss: 154.60141| val_0_rmse: 17.69984|  0:27:02s
epoch 74 | loss: 153.50647| val_0_rmse: 18.27213|  0:27:24s
epoch 75 | loss: 148.56891| val_0_rmse: 875784.97849|  0:27:46s
epoch 76 | loss: 149.69385| val_0_rmse: 366.71708|  0:28:08s
epoch 77 | loss: 149.54367| val_0_rmse: 2410.41461|  0:28:30s
epoch 78 | loss: 148.00625| val_0_rmse: 6220.32153|  0:28:52s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 79 | loss: 148.35348| val_0_rmse: 252.48541|  0:29:14s
Early stopping occurred at epoch 79 with best_epoch = 59 and best_val_0_rmse = 16.50393
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 17:47:16,545] Trial 1 finished with value: 16.50393021532624 and parameters: {'lr': 0.0018985868457556164, 'n_steps': 9, 'gamma': 1.4708860032950941, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.08635666319532159, 'weight_decay': 3.774778443912067e-05, 'batch_size': 64, 'virtual_batch_size': 32}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8919.21966| val_0_rmse: 94.67271|  0:00:07s
epoch 1  | loss: 8680.92586| val_0_rmse: 93.69376|  0:00:15s
epoch 2  | loss: 8220.7201| val_0_rmse: 89.13164|  0:00:23s
epoch 3  | loss: 6866.9383| val_0_rmse: 76.77918|  0:00:31s
epoch 4  | loss: 4469.40804| val_0_rmse: 58.69087|  0:00:39s
epoch 5  | loss: 2269.71271| val_0_rmse: 40.63647|  0:00:47s
epoch 6  | loss: 1327.68793| val_0_rmse: 35.36804|  0:00:55s
epoch 7  | loss: 1169.07498| val_0_rmse: 34.8307 |  0:01:03s
epoch 8  | loss: 1135.30813| val_0_rmse: 34.21966|  0:01:11s
epoch 9  | loss: 1092.3993| val_0_rmse: 33.75313|  0:01:19s
epoch 10 | loss: 1066.57731| val_0_rmse: 33.26478|  0:01:27s
epoch 11 | loss: 1045.86164| val_0_rmse: 33.68903|  0:01:34s
epoch 12 | loss: 1027.10264| val_0_rmse: 33.37874|  0:01:42s
epoch 13 | loss: 1022.55287| val_0_rmse: 32.99392|  0:01:50s
epoch 14 | loss: 1018.18153| val_0_rmse: 33.58289|  0:01:58s
epoch 15 | loss: 1006.29843| val_0_rmse: 33.3756 |  0:02:06s
epoch 16 | loss: 1000.01594| val_0_rmse: 36.44703|  0:02:14s
epoch 17 | loss: 993.40677| val_0_rmse: 34.65579|  0:02:22s
epoch 18 | loss: 989.592 | val_0_rmse: 33.16935|  0:02:29s
epoch 19 | loss: 985.27633| val_0_rmse: 33.41007|  0:02:37s
epoch 20 | loss: 979.67746| val_0_rmse: 34.3196 |  0:02:45s
epoch 21 | loss: 979.27443| val_0_rmse: 35.88401|  0:02:53s
epoch 22 | loss: 977.77174| val_0_rmse: 33.51111|  0:03:01s
epoch 23 | loss: 974.43252| val_0_rmse: 34.17979|  0:03:09s
epoch 24 | loss: 969.83597| val_0_rmse: 34.33396|  0:03:17s
epoch 25 | loss: 967.2235| val_0_rmse: 33.92474|  0:03:24s
epoch 26 | loss: 956.00923| val_0_rmse: 34.09843|  0:03:32s
epoch 27 | loss: 953.85427| val_0_rmse: 31.91527|  0:03:40s
epoch 28 | loss: 947.47317| val_0_rmse: 33.01037|  0:03:48s
epoch 29 | loss: 949.97729| val_0_rmse: 31.47859|  0:03:56s
epoch 30 | loss: 939.1868| val_0_rmse: 30.93911|  0:04:04s
epoch 31 | loss: 935.98732| val_0_rmse: 31.09245|  0:04:12s
epoch 32 | loss: 928.56284| val_0_rmse: 30.94492|  0:04:20s
epoch 33 | loss: 922.63287| val_0_rmse: 30.75701|  0:04:27s
epoch 34 | loss: 918.06394| val_0_rmse: 30.5746 |  0:04:35s
epoch 35 | loss: 917.22828| val_0_rmse: 31.43569|  0:04:43s
epoch 36 | loss: 910.29816| val_0_rmse: 32.19395|  0:04:51s
epoch 37 | loss: 908.57832| val_0_rmse: 30.68894|  0:04:59s
epoch 38 | loss: 910.1241| val_0_rmse: 30.60138|  0:05:07s
epoch 39 | loss: 903.96296| val_0_rmse: 30.16268|  0:05:15s
epoch 40 | loss: 902.78592| val_0_rmse: 30.90788|  0:05:23s
epoch 41 | loss: 886.23875| val_0_rmse: 31.59751|  0:05:30s
epoch 42 | loss: 890.17328| val_0_rmse: 30.63164|  0:05:38s
epoch 43 | loss: 880.54684| val_0_rmse: 30.62366|  0:05:46s
epoch 44 | loss: 876.92316| val_0_rmse: 31.93369|  0:05:54s
epoch 45 | loss: 877.50864| val_0_rmse: 30.965  |  0:06:02s
epoch 46 | loss: 885.13842| val_0_rmse: 31.46106|  0:06:10s
epoch 47 | loss: 883.65928| val_0_rmse: 30.83232|  0:06:17s
epoch 48 | loss: 873.82835| val_0_rmse: 29.92365|  0:06:25s
epoch 49 | loss: 869.72529| val_0_rmse: 30.2804 |  0:06:33s
epoch 50 | loss: 870.14325| val_0_rmse: 30.09183|  0:06:41s
epoch 51 | loss: 863.10088| val_0_rmse: 29.92801|  0:06:49s
epoch 52 | loss: 861.65205| val_0_rmse: 30.01579|  0:06:57s
epoch 53 | loss: 863.13701| val_0_rmse: 29.68853|  0:07:04s
epoch 54 | loss: 860.54054| val_0_rmse: 30.80832|  0:07:12s
epoch 55 | loss: 861.27872| val_0_rmse: 29.92385|  0:07:20s
epoch 56 | loss: 851.095 | val_0_rmse: 30.44352|  0:07:28s
epoch 57 | loss: 853.99303| val_0_rmse: 30.17603|  0:07:36s
epoch 58 | loss: 843.29602| val_0_rmse: 30.16703|  0:07:44s
epoch 59 | loss: 836.97489| val_0_rmse: 29.59032|  0:07:52s
epoch 60 | loss: 835.17851| val_0_rmse: 29.44206|  0:08:00s
epoch 61 | loss: 829.32251| val_0_rmse: 29.67743|  0:08:07s
epoch 62 | loss: 821.10128| val_0_rmse: 29.57847|  0:08:15s
epoch 63 | loss: 821.08115| val_0_rmse: 29.4955 |  0:08:23s
epoch 64 | loss: 811.56165| val_0_rmse: 29.5075 |  0:08:31s
epoch 65 | loss: 800.0499| val_0_rmse: 30.96083|  0:08:39s
epoch 66 | loss: 795.6588| val_0_rmse: 29.78924|  0:08:47s
epoch 67 | loss: 781.48998| val_0_rmse: 30.20478|  0:08:55s
epoch 68 | loss: 771.55262| val_0_rmse: 30.10509|  0:09:03s
epoch 69 | loss: 762.31093| val_0_rmse: 65.07841|  0:09:11s
epoch 70 | loss: 752.55397| val_0_rmse: 48.599  |  0:09:18s
epoch 71 | loss: 743.04073| val_0_rmse: 56.42879|  0:09:26s
epoch 72 | loss: 732.80224| val_0_rmse: 47.05146|  0:09:34s
epoch 73 | loss: 709.29252| val_0_rmse: 44.58422|  0:09:42s
epoch 74 | loss: 702.47931| val_0_rmse: 48.97817|  0:09:50s
epoch 75 | loss: 686.95766| val_0_rmse: 45.13265|  0:09:58s
epoch 76 | loss: 665.10463| val_0_rmse: 38.36192|  0:10:06s
epoch 77 | loss: 642.96763| val_0_rmse: 30.2848 |  0:10:13s
epoch 78 | loss: 626.1665| val_0_rmse: 40.73655|  0:10:21s
epoch 79 | loss: 607.90307| val_0_rmse: 26.26722|  0:10:29s
epoch 80 | loss: 591.74244| val_0_rmse: 25.57114|  0:10:37s
epoch 81 | loss: 587.97601| val_0_rmse: 24.60959|  0:10:45s
epoch 82 | loss: 577.02444| val_0_rmse: 25.92655|  0:10:53s
epoch 83 | loss: 570.55299| val_0_rmse: 26.22832|  0:11:01s
epoch 84 | loss: 558.19763| val_0_rmse: 28.02904|  0:11:08s
epoch 85 | loss: 544.5182| val_0_rmse: 28.88756|  0:11:16s
epoch 86 | loss: 535.62904| val_0_rmse: 34.46871|  0:11:24s
epoch 87 | loss: 525.09289| val_0_rmse: 35.46294|  0:11:32s
epoch 88 | loss: 521.7286| val_0_rmse: 25.4793 |  0:11:40s
epoch 89 | loss: 511.49879| val_0_rmse: 22.36795|  0:11:48s
epoch 90 | loss: 504.30889| val_0_rmse: 22.40195|  0:11:56s
epoch 91 | loss: 492.50047| val_0_rmse: 22.03567|  0:12:03s
epoch 92 | loss: 494.65997| val_0_rmse: 22.258  |  0:12:11s
epoch 93 | loss: 479.38826| val_0_rmse: 22.43965|  0:12:19s
epoch 94 | loss: 472.2749| val_0_rmse: 22.22566|  0:12:27s
epoch 95 | loss: 469.99406| val_0_rmse: 22.97615|  0:12:35s
epoch 96 | loss: 466.62863| val_0_rmse: 21.98289|  0:12:43s
epoch 97 | loss: 459.12146| val_0_rmse: 26.82488|  0:12:51s
epoch 98 | loss: 458.8399| val_0_rmse: 27.40323|  0:12:59s
epoch 99 | loss: 454.97567| val_0_rmse: 54.8591 |  0:13:06s
epoch 100| loss: 452.26026| val_0_rmse: 55.56515|  0:13:14s
epoch 101| loss: 445.72716| val_0_rmse: 64.2107 |  0:13:22s
epoch 102| loss: 439.02874| val_0_rmse: 49.30069|  0:13:30s
epoch 103| loss: 431.48562| val_0_rmse: 36.46676|  0:13:38s
epoch 104| loss: 428.2057| val_0_rmse: 32.71199|  0:13:46s
epoch 105| loss: 423.24287| val_0_rmse: 50.72099|  0:13:54s
epoch 106| loss: 425.67961| val_0_rmse: 44.94246|  0:14:01s
epoch 107| loss: 413.34423| val_0_rmse: 22.21872|  0:14:09s
epoch 108| loss: 407.267 | val_0_rmse: 65.68165|  0:14:17s
epoch 109| loss: 407.84328| val_0_rmse: 34.75343|  0:14:25s
epoch 110| loss: 397.29094| val_0_rmse: 61.54532|  0:14:33s
epoch 111| loss: 390.78869| val_0_rmse: 31.04241|  0:14:41s
epoch 112| loss: 390.34038| val_0_rmse: 89.0439 |  0:14:49s
epoch 113| loss: 383.41373| val_0_rmse: 58.94893|  0:14:56s
epoch 114| loss: 383.11776| val_0_rmse: 46.4397 |  0:15:04s
epoch 115| loss: 368.92505| val_0_rmse: 41.73255|  0:15:12s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 116| loss: 369.48042| val_0_rmse: 61.37544|  0:15:20s
Early stopping occurred at epoch 116 with best_epoch = 96 and best_val_0_rmse = 21.98289
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 18:03:04,194] Trial 2 finished with value: 21.982885510150844 and parameters: {'lr': 0.001220535139475017, 'n_steps': 7, 'gamma': 1.4758627284569825, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.06821599843918445, 'weight_decay': 1.7526691725647886e-05, 'batch_size': 128, 'virtual_batch_size': 32}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8802.16112| val_0_rmse: 95.10967|  0:00:16s
epoch 1  | loss: 8600.60561| val_0_rmse: 88.64914|  0:00:32s
epoch 2  | loss: 4097.72865| val_0_rmse: 40.89884|  0:00:48s
epoch 3  | loss: 1057.4414| val_0_rmse: 30.84331|  0:01:04s
epoch 4  | loss: 888.37968| val_0_rmse: 29.68383|  0:01:20s
epoch 5  | loss: 828.37325| val_0_rmse: 28.73373|  0:01:37s
epoch 6  | loss: 788.22882| val_0_rmse: 30.80847|  0:01:53s
epoch 7  | loss: 766.11588| val_0_rmse: 27.48802|  0:02:09s
epoch 8  | loss: 749.24904| val_0_rmse: 27.29708|  0:02:25s
epoch 9  | loss: 732.95283| val_0_rmse: 28.4271 |  0:02:42s
epoch 10 | loss: 715.72947| val_0_rmse: 27.22183|  0:02:58s
epoch 11 | loss: 688.83966| val_0_rmse: 26.40745|  0:03:14s
epoch 12 | loss: 666.19377| val_0_rmse: 26.04238|  0:03:30s
epoch 13 | loss: 652.66465| val_0_rmse: 25.49567|  0:03:46s
epoch 14 | loss: 629.97993| val_0_rmse: 24.86707|  0:04:02s
epoch 15 | loss: 599.05748| val_0_rmse: 24.56817|  0:04:19s
epoch 16 | loss: 561.38065| val_0_rmse: 23.64166|  0:04:35s
epoch 17 | loss: 528.46211| val_0_rmse: 23.06266|  0:04:51s
epoch 18 | loss: 500.27998| val_0_rmse: 23.22094|  0:05:07s
epoch 19 | loss: 466.4386| val_0_rmse: 21.22537|  0:05:23s
epoch 20 | loss: 428.77708| val_0_rmse: 20.84026|  0:05:39s
epoch 21 | loss: 398.39324| val_0_rmse: 20.50733|  0:05:56s
epoch 22 | loss: 374.62052| val_0_rmse: 20.3107 |  0:06:12s
epoch 23 | loss: 362.83154| val_0_rmse: 20.29282|  0:06:28s
epoch 24 | loss: 347.68916| val_0_rmse: 19.78172|  0:06:44s
epoch 25 | loss: 333.35965| val_0_rmse: 19.1414 |  0:07:00s
epoch 26 | loss: 322.22173| val_0_rmse: 19.35351|  0:07:16s
epoch 27 | loss: 305.37232| val_0_rmse: 19.06453|  0:07:33s
epoch 28 | loss: 298.14574| val_0_rmse: 18.62   |  0:07:49s
epoch 29 | loss: 287.87832| val_0_rmse: 18.41494|  0:08:05s
epoch 30 | loss: 279.79244| val_0_rmse: 18.62841|  0:08:21s
epoch 31 | loss: 274.02273| val_0_rmse: 18.4413 |  0:08:37s
epoch 32 | loss: 269.7402| val_0_rmse: 18.00379|  0:08:53s
epoch 33 | loss: 263.08978| val_0_rmse: 18.12922|  0:09:10s
epoch 34 | loss: 260.16663| val_0_rmse: 18.35201|  0:09:26s
epoch 35 | loss: 250.51257| val_0_rmse: 17.8857 |  0:09:42s
epoch 36 | loss: 247.17212| val_0_rmse: 17.8051 |  0:09:58s
epoch 37 | loss: 246.56394| val_0_rmse: 18.52041|  0:10:14s
epoch 38 | loss: 239.21554| val_0_rmse: 18.09168|  0:10:30s
epoch 39 | loss: 226.96109| val_0_rmse: 17.19404|  0:10:46s
epoch 40 | loss: 227.9622| val_0_rmse: 16.99137|  0:11:03s
epoch 41 | loss: 224.57108| val_0_rmse: 17.15872|  0:11:19s
epoch 42 | loss: 218.76821| val_0_rmse: 17.52979|  0:11:35s
epoch 43 | loss: 220.47298| val_0_rmse: 16.97497|  0:11:51s
epoch 44 | loss: 214.07407| val_0_rmse: 17.35443|  0:12:07s
epoch 45 | loss: 209.20087| val_0_rmse: 18.02454|  0:12:23s
epoch 46 | loss: 209.74889| val_0_rmse: 17.04704|  0:12:39s
epoch 47 | loss: 209.19518| val_0_rmse: 16.73372|  0:12:55s
epoch 48 | loss: 206.18519| val_0_rmse: 16.81032|  0:13:11s
epoch 49 | loss: 204.71401| val_0_rmse: 17.47531|  0:13:27s
epoch 50 | loss: 200.23389| val_0_rmse: 16.99149|  0:13:44s
epoch 51 | loss: 197.22615| val_0_rmse: 16.71673|  0:14:00s
epoch 52 | loss: 194.51044| val_0_rmse: 17.00429|  0:14:16s
epoch 53 | loss: 189.99332| val_0_rmse: 16.40602|  0:14:32s
epoch 54 | loss: 190.0723| val_0_rmse: 16.92787|  0:14:48s
epoch 55 | loss: 195.03704| val_0_rmse: 15.97109|  0:15:04s
epoch 56 | loss: 191.44013| val_0_rmse: 17.52718|  0:15:20s
epoch 57 | loss: 180.87978| val_0_rmse: 15.89973|  0:15:36s
epoch 58 | loss: 187.09152| val_0_rmse: 16.94249|  0:15:52s
epoch 59 | loss: 181.53875| val_0_rmse: 16.24727|  0:16:08s
epoch 60 | loss: 185.70825| val_0_rmse: 16.56842|  0:16:24s
epoch 61 | loss: 180.3165| val_0_rmse: 16.86354|  0:16:40s
epoch 62 | loss: 180.622 | val_0_rmse: 16.49204|  0:16:56s
epoch 63 | loss: 183.90247| val_0_rmse: 17.15749|  0:17:12s
epoch 64 | loss: 179.46063| val_0_rmse: 15.88695|  0:17:28s
epoch 65 | loss: 179.86472| val_0_rmse: 16.45916|  0:17:45s
epoch 66 | loss: 176.90205| val_0_rmse: 16.07972|  0:18:01s
epoch 67 | loss: 179.0613| val_0_rmse: 16.5171 |  0:18:17s
epoch 68 | loss: 172.67956| val_0_rmse: 16.32444|  0:18:33s
epoch 69 | loss: 174.0568| val_0_rmse: 16.38936|  0:18:49s
epoch 70 | loss: 179.15212| val_0_rmse: 17.05665|  0:19:05s
epoch 71 | loss: 166.07013| val_0_rmse: 16.33712|  0:19:21s
epoch 72 | loss: 176.09746| val_0_rmse: 16.70822|  0:19:37s
epoch 73 | loss: 171.95164| val_0_rmse: 16.45349|  0:19:53s
epoch 74 | loss: 172.27742| val_0_rmse: 15.75401|  0:20:10s
epoch 75 | loss: 170.67374| val_0_rmse: 15.92785|  0:20:26s
epoch 76 | loss: 167.40205| val_0_rmse: 15.91996|  0:20:42s
epoch 77 | loss: 166.34295| val_0_rmse: 15.94784|  0:20:58s
epoch 78 | loss: 170.87479| val_0_rmse: 15.75201|  0:21:14s
epoch 79 | loss: 171.06556| val_0_rmse: 15.54823|  0:21:30s
epoch 80 | loss: 164.64752| val_0_rmse: 16.77089|  0:21:46s
epoch 81 | loss: 170.38344| val_0_rmse: 16.60913|  0:22:02s
epoch 82 | loss: 164.26259| val_0_rmse: 16.51084|  0:22:19s
epoch 83 | loss: 159.93784| val_0_rmse: 16.0306 |  0:22:35s
epoch 84 | loss: 166.30174| val_0_rmse: 15.85684|  0:22:51s
epoch 85 | loss: 165.16368| val_0_rmse: 16.5281 |  0:23:07s
epoch 86 | loss: 162.22913| val_0_rmse: 16.2239 |  0:23:23s
epoch 87 | loss: 169.50054| val_0_rmse: 16.62358|  0:23:39s
epoch 88 | loss: 161.23152| val_0_rmse: 16.58539|  0:23:55s
epoch 89 | loss: 162.5771| val_0_rmse: 17.0119 |  0:24:11s
epoch 90 | loss: 162.66497| val_0_rmse: 16.43328|  0:24:27s
epoch 91 | loss: 165.88095| val_0_rmse: 15.77169|  0:24:43s
epoch 92 | loss: 162.13607| val_0_rmse: 16.10119|  0:24:59s
epoch 93 | loss: 157.83089| val_0_rmse: 16.42837|  0:25:15s
epoch 94 | loss: 160.99371| val_0_rmse: 16.06759|  0:25:32s
epoch 95 | loss: 157.07505| val_0_rmse: 15.92452|  0:25:48s
epoch 96 | loss: 155.53224| val_0_rmse: 16.61048|  0:26:04s
epoch 97 | loss: 160.14274| val_0_rmse: 16.35547|  0:26:20s
epoch 98 | loss: 155.41366| val_0_rmse: 16.43997|  0:26:36s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 99 | loss: 154.95491| val_0_rmse: 15.97634|  0:26:52s
Early stopping occurred at epoch 99 with best_epoch = 79 and best_val_0_rmse = 15.54823
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 18:31:03,215] Trial 3 finished with value: 15.548226323019165 and parameters: {'lr': 0.0013478863517160738, 'n_steps': 9, 'gamma': 1.1246856871336786, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.06464142041639173, 'weight_decay': 0.0003868209555654833, 'batch_size': 64, 'virtual_batch_size': 64}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 9159.70404| val_0_rmse: 96.30849|  0:00:05s
epoch 1  | loss: 9060.61193| val_0_rmse: 94.38746|  0:00:11s
epoch 2  | loss: 9000.71814| val_0_rmse: 94.9239 |  0:00:16s
epoch 3  | loss: 8957.25869| val_0_rmse: 94.95863|  0:00:22s
epoch 4  | loss: 8912.40132| val_0_rmse: 94.82396|  0:00:27s
epoch 5  | loss: 8848.59423| val_0_rmse: 94.53633|  0:00:33s
epoch 6  | loss: 8688.85204| val_0_rmse: 93.59705|  0:00:39s
epoch 7  | loss: 8162.87505| val_0_rmse: 88.2491 |  0:00:44s
epoch 8  | loss: 6439.92315| val_0_rmse: 72.22721|  0:00:50s
epoch 9  | loss: 3708.36452| val_0_rmse: 52.00485|  0:00:55s
epoch 10 | loss: 2214.96788| val_0_rmse: 43.75724|  0:01:01s
epoch 11 | loss: 1505.2933| val_0_rmse: 37.15342|  0:01:06s
epoch 12 | loss: 1135.08881| val_0_rmse: 34.71434|  0:01:12s
epoch 13 | loss: 1060.61052| val_0_rmse: 33.74581|  0:01:18s
epoch 14 | loss: 1031.7776| val_0_rmse: 33.12854|  0:01:23s
epoch 15 | loss: 1016.14333| val_0_rmse: 33.24773|  0:01:29s
epoch 16 | loss: 1006.72107| val_0_rmse: 32.65426|  0:01:34s
epoch 17 | loss: 1004.69056| val_0_rmse: 33.35371|  0:01:40s
epoch 18 | loss: 1027.6054| val_0_rmse: 33.30958|  0:01:45s
epoch 19 | loss: 994.07896| val_0_rmse: 32.12403|  0:01:51s
epoch 20 | loss: 992.67578| val_0_rmse: 32.19515|  0:01:57s
epoch 21 | loss: 979.34849| val_0_rmse: 32.38248|  0:02:02s
epoch 22 | loss: 966.28636| val_0_rmse: 31.87704|  0:02:08s
epoch 23 | loss: 964.869 | val_0_rmse: 31.57139|  0:02:13s
epoch 24 | loss: 960.94527| val_0_rmse: 31.99579|  0:02:19s
epoch 25 | loss: 954.8217| val_0_rmse: 31.73092|  0:02:24s
epoch 26 | loss: 942.10443| val_0_rmse: 31.36013|  0:02:30s
epoch 27 | loss: 938.79984| val_0_rmse: 31.49902|  0:02:36s
epoch 28 | loss: 931.73838| val_0_rmse: 31.08037|  0:02:41s
epoch 29 | loss: 929.81528| val_0_rmse: 31.52049|  0:02:47s
epoch 30 | loss: 928.2294| val_0_rmse: 31.26443|  0:02:52s
epoch 31 | loss: 922.51411| val_0_rmse: 30.34708|  0:02:58s
epoch 32 | loss: 916.13848| val_0_rmse: 31.00431|  0:03:03s
epoch 33 | loss: 912.85532| val_0_rmse: 30.57574|  0:03:09s
epoch 34 | loss: 907.80337| val_0_rmse: 30.73876|  0:03:15s
epoch 35 | loss: 898.28698| val_0_rmse: 30.96268|  0:03:20s
epoch 36 | loss: 898.95198| val_0_rmse: 30.75165|  0:03:26s
epoch 37 | loss: 896.78547| val_0_rmse: 30.60308|  0:03:31s
epoch 38 | loss: 894.99897| val_0_rmse: 30.87382|  0:03:37s
epoch 39 | loss: 891.95037| val_0_rmse: 30.95676|  0:03:42s
epoch 40 | loss: 884.89799| val_0_rmse: 30.71254|  0:03:48s
epoch 41 | loss: 885.79646| val_0_rmse: 30.65432|  0:03:53s
epoch 42 | loss: 884.3298| val_0_rmse: 30.31393|  0:03:59s
epoch 43 | loss: 888.53493| val_0_rmse: 30.60604|  0:04:04s
epoch 44 | loss: 885.09702| val_0_rmse: 30.46727|  0:04:10s
epoch 45 | loss: 884.39083| val_0_rmse: 30.33076|  0:04:16s
epoch 46 | loss: 884.42076| val_0_rmse: 29.87724|  0:04:21s
epoch 47 | loss: 882.3883| val_0_rmse: 29.90199|  0:04:27s
epoch 48 | loss: 874.92842| val_0_rmse: 29.62217|  0:04:32s
epoch 49 | loss: 873.25614| val_0_rmse: 29.80692|  0:04:38s
epoch 50 | loss: 869.75552| val_0_rmse: 30.07451|  0:04:43s
epoch 51 | loss: 864.38028| val_0_rmse: 29.65226|  0:04:49s
epoch 52 | loss: 857.55464| val_0_rmse: 29.58042|  0:04:54s
epoch 53 | loss: 858.08869| val_0_rmse: 29.42158|  0:05:00s
epoch 54 | loss: 847.40006| val_0_rmse: 29.32562|  0:05:05s
epoch 55 | loss: 842.82066| val_0_rmse: 29.12235|  0:05:11s
epoch 56 | loss: 845.47081| val_0_rmse: 28.97074|  0:05:17s
epoch 57 | loss: 841.85996| val_0_rmse: 29.07237|  0:05:22s
epoch 58 | loss: 832.41194| val_0_rmse: 29.06797|  0:05:28s
epoch 59 | loss: 827.32285| val_0_rmse: 29.53626|  0:05:33s
epoch 60 | loss: 831.24935| val_0_rmse: 29.04463|  0:05:39s
epoch 61 | loss: 833.84566| val_0_rmse: 29.19522|  0:05:44s
epoch 62 | loss: 825.40006| val_0_rmse: 29.15563|  0:05:50s
epoch 63 | loss: 823.58462| val_0_rmse: 29.15868|  0:05:56s
epoch 64 | loss: 819.98406| val_0_rmse: 29.10185|  0:06:01s
epoch 65 | loss: 819.98005| val_0_rmse: 28.76544|  0:06:07s
epoch 66 | loss: 823.48945| val_0_rmse: 29.11563|  0:06:12s
epoch 67 | loss: 821.00331| val_0_rmse: 28.95809|  0:06:18s
epoch 68 | loss: 826.03416| val_0_rmse: 28.68164|  0:06:23s
epoch 69 | loss: 824.31285| val_0_rmse: 28.99289|  0:06:29s
epoch 70 | loss: 824.12451| val_0_rmse: 29.50436|  0:06:34s
epoch 71 | loss: 822.37687| val_0_rmse: 29.22961|  0:06:40s
epoch 72 | loss: 827.60649| val_0_rmse: 28.65456|  0:06:45s
epoch 73 | loss: 818.85347| val_0_rmse: 28.75841|  0:06:51s
epoch 74 | loss: 823.79435| val_0_rmse: 28.71719|  0:06:57s
epoch 75 | loss: 818.22656| val_0_rmse: 28.83298|  0:07:02s
epoch 76 | loss: 819.67507| val_0_rmse: 28.58771|  0:07:08s
epoch 77 | loss: 811.34487| val_0_rmse: 28.90186|  0:07:13s
epoch 78 | loss: 816.80463| val_0_rmse: 28.8688 |  0:07:19s
epoch 79 | loss: 811.46228| val_0_rmse: 29.09051|  0:07:24s
epoch 80 | loss: 814.39364| val_0_rmse: 28.60915|  0:07:30s
epoch 81 | loss: 812.08882| val_0_rmse: 29.00692|  0:07:35s
epoch 82 | loss: 798.63529| val_0_rmse: 28.85246|  0:07:41s
epoch 83 | loss: 807.88783| val_0_rmse: 28.59422|  0:07:46s
epoch 84 | loss: 803.0923| val_0_rmse: 28.75271|  0:07:52s
epoch 85 | loss: 803.28525| val_0_rmse: 28.4682 |  0:07:57s
epoch 86 | loss: 799.60618| val_0_rmse: 28.80023|  0:08:03s
epoch 87 | loss: 799.04704| val_0_rmse: 28.57009|  0:08:09s
epoch 88 | loss: 795.54005| val_0_rmse: 29.12186|  0:08:14s
epoch 89 | loss: 795.33362| val_0_rmse: 28.80827|  0:08:20s
epoch 90 | loss: 793.33879| val_0_rmse: 28.51073|  0:08:25s
epoch 91 | loss: 796.15005| val_0_rmse: 28.47106|  0:08:31s
epoch 92 | loss: 785.85918| val_0_rmse: 28.71223|  0:08:36s
epoch 93 | loss: 792.80012| val_0_rmse: 27.93646|  0:08:42s
epoch 94 | loss: 788.31822| val_0_rmse: 28.6317 |  0:08:47s
epoch 95 | loss: 779.75742| val_0_rmse: 28.17231|  0:08:53s
epoch 96 | loss: 779.64907| val_0_rmse: 28.45806|  0:08:58s
epoch 97 | loss: 775.78415| val_0_rmse: 28.35219|  0:09:04s
epoch 98 | loss: 779.37817| val_0_rmse: 28.74957|  0:09:10s
epoch 99 | loss: 783.62119| val_0_rmse: 28.74953|  0:09:15s
epoch 100| loss: 776.19869| val_0_rmse: 28.69018|  0:09:21s
epoch 101| loss: 768.95432| val_0_rmse: 29.85732|  0:09:26s
epoch 102| loss: 772.89659| val_0_rmse: 29.18776|  0:09:32s
epoch 103| loss: 769.34286| val_0_rmse: 28.41426|  0:09:37s
epoch 104| loss: 766.23626| val_0_rmse: 29.67782|  0:09:43s
epoch 105| loss: 769.38286| val_0_rmse: 29.37516|  0:09:48s
epoch 106| loss: 758.14945| val_0_rmse: 28.49756|  0:09:54s
epoch 107| loss: 759.9833| val_0_rmse: 28.37561|  0:09:59s
epoch 108| loss: 759.38851| val_0_rmse: 28.53799|  0:10:05s
epoch 109| loss: 753.78212| val_0_rmse: 27.99067|  0:10:10s
epoch 110| loss: 750.4843| val_0_rmse: 28.72433|  0:10:16s
epoch 111| loss: 738.21267| val_0_rmse: 29.42766|  0:10:22s
epoch 112| loss: 735.55159| val_0_rmse: 28.07551|  0:10:27s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 113| loss: 737.46498| val_0_rmse: 28.11555|  0:10:33s
Early stopping occurred at epoch 113 with best_epoch = 93 and best_val_0_rmse = 27.93646
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 18:41:56,413] Trial 4 finished with value: 27.936457239441914 and parameters: {'lr': 0.0015475583569365637, 'n_steps': 10, 'gamma': 1.405953647828833, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.027703807264023037, 'weight_decay': 1.1246717638926949e-05, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8921.89603| val_0_rmse: 95.41356|  0:00:09s
epoch 1  | loss: 8845.91557| val_0_rmse: 94.64568|  0:00:18s
epoch 2  | loss: 8780.99547| val_0_rmse: 94.39713|  0:00:27s
epoch 3  | loss: 8619.35324| val_0_rmse: 92.98239|  0:00:36s
epoch 4  | loss: 8007.68963| val_0_rmse: 85.61484|  0:00:45s
epoch 5  | loss: 5690.18095| val_0_rmse: 63.76861|  0:00:54s
epoch 6  | loss: 2711.30391| val_0_rmse: 43.4193 |  0:01:03s
epoch 7  | loss: 1443.64321| val_0_rmse: 35.63262|  0:01:12s
epoch 8  | loss: 1107.89002| val_0_rmse: 32.56134|  0:01:21s
epoch 9  | loss: 1005.47814| val_0_rmse: 32.00314|  0:01:30s
epoch 10 | loss: 942.45304| val_0_rmse: 31.92509|  0:01:39s
epoch 11 | loss: 900.29462| val_0_rmse: 33.14841|  0:01:48s
epoch 12 | loss: 865.66584| val_0_rmse: 30.54593|  0:01:57s
epoch 13 | loss: 833.8631| val_0_rmse: 31.06154|  0:02:07s
epoch 14 | loss: 808.38844| val_0_rmse: 31.08738|  0:02:16s
epoch 15 | loss: 788.28929| val_0_rmse: 33.12302|  0:02:25s
epoch 16 | loss: 767.52113| val_0_rmse: 29.19209|  0:02:34s
epoch 17 | loss: 753.12865| val_0_rmse: 29.94912|  0:02:43s
epoch 18 | loss: 738.05939| val_0_rmse: 31.25281|  0:02:52s
epoch 19 | loss: 734.3801| val_0_rmse: 29.27097|  0:03:01s
epoch 20 | loss: 718.00679| val_0_rmse: 28.13701|  0:03:10s
epoch 21 | loss: 707.03664| val_0_rmse: 27.77467|  0:03:19s
epoch 22 | loss: 699.64794| val_0_rmse: 29.14128|  0:03:28s
epoch 23 | loss: 689.08683| val_0_rmse: 28.68775|  0:03:37s
epoch 24 | loss: 690.59031| val_0_rmse: 29.51058|  0:03:46s
epoch 25 | loss: 680.19631| val_0_rmse: 28.93121|  0:03:55s
epoch 26 | loss: 677.05903| val_0_rmse: 28.03874|  0:04:04s
epoch 27 | loss: 673.92655| val_0_rmse: 29.86474|  0:04:13s
epoch 28 | loss: 666.30153| val_0_rmse: 27.65003|  0:04:22s
epoch 29 | loss: 656.97109| val_0_rmse: 27.46952|  0:04:31s
epoch 30 | loss: 654.37506| val_0_rmse: 28.20867|  0:04:40s
epoch 31 | loss: 651.01769| val_0_rmse: 26.7798 |  0:04:49s
epoch 32 | loss: 640.03704| val_0_rmse: 28.57246|  0:04:58s
epoch 33 | loss: 635.28901| val_0_rmse: 27.12335|  0:05:08s
epoch 34 | loss: 627.94158| val_0_rmse: 27.28984|  0:05:17s
epoch 35 | loss: 621.13745| val_0_rmse: 25.97265|  0:05:26s
epoch 36 | loss: 605.61851| val_0_rmse: 24.95307|  0:05:35s
epoch 37 | loss: 598.71414| val_0_rmse: 24.55871|  0:05:44s
epoch 38 | loss: 582.88761| val_0_rmse: 24.97882|  0:05:53s
epoch 39 | loss: 565.56518| val_0_rmse: 23.81739|  0:06:02s
epoch 40 | loss: 552.17746| val_0_rmse: 24.04105|  0:06:11s
epoch 41 | loss: 530.89514| val_0_rmse: 23.59625|  0:06:20s
epoch 42 | loss: 506.8056| val_0_rmse: 24.56387|  0:06:29s
epoch 43 | loss: 497.28098| val_0_rmse: 22.88909|  0:06:38s
epoch 44 | loss: 489.63148| val_0_rmse: 22.68902|  0:06:47s
epoch 45 | loss: 477.81481| val_0_rmse: 22.32877|  0:06:56s
epoch 46 | loss: 464.29186| val_0_rmse: 21.97334|  0:07:05s
epoch 47 | loss: 455.67974| val_0_rmse: 21.42502|  0:07:14s
epoch 48 | loss: 439.91912| val_0_rmse: 21.22303|  0:07:23s
epoch 49 | loss: 429.63323| val_0_rmse: 21.18967|  0:07:32s
epoch 50 | loss: 423.4885| val_0_rmse: 20.69692|  0:07:41s
epoch 51 | loss: 412.82682| val_0_rmse: 20.37823|  0:07:50s
epoch 52 | loss: 406.99858| val_0_rmse: 21.00251|  0:08:00s
epoch 53 | loss: 399.03192| val_0_rmse: 20.40137|  0:08:09s
epoch 54 | loss: 392.24031| val_0_rmse: 20.56207|  0:08:18s
epoch 55 | loss: 383.61549| val_0_rmse: 19.69055|  0:08:27s
epoch 56 | loss: 377.84962| val_0_rmse: 19.75054|  0:08:36s
epoch 57 | loss: 362.38047| val_0_rmse: 19.3805 |  0:08:45s
epoch 58 | loss: 364.91553| val_0_rmse: 19.60358|  0:08:54s
epoch 59 | loss: 358.81173| val_0_rmse: 19.01554|  0:09:03s
epoch 60 | loss: 347.3307| val_0_rmse: 19.04122|  0:09:12s
epoch 61 | loss: 344.92583| val_0_rmse: 18.90026|  0:09:21s
epoch 62 | loss: 340.42806| val_0_rmse: 18.59815|  0:09:30s
epoch 63 | loss: 331.92551| val_0_rmse: 18.57949|  0:09:39s
epoch 64 | loss: 327.62667| val_0_rmse: 18.78286|  0:09:48s
epoch 65 | loss: 320.73257| val_0_rmse: 18.40043|  0:09:57s
epoch 66 | loss: 313.08583| val_0_rmse: 18.30313|  0:10:06s
epoch 67 | loss: 313.35701| val_0_rmse: 18.09907|  0:10:16s
epoch 68 | loss: 309.55931| val_0_rmse: 17.7938 |  0:10:25s
epoch 69 | loss: 304.43439| val_0_rmse: 17.61368|  0:10:34s
epoch 70 | loss: 301.14888| val_0_rmse: 17.72133|  0:10:43s
epoch 71 | loss: 302.54384| val_0_rmse: 17.64194|  0:10:52s
epoch 72 | loss: 295.11016| val_0_rmse: 17.7929 |  0:11:01s
epoch 73 | loss: 285.91241| val_0_rmse: 17.49363|  0:11:10s
epoch 74 | loss: 281.81098| val_0_rmse: 17.1149 |  0:11:19s
epoch 75 | loss: 282.08019| val_0_rmse: 17.00474|  0:11:28s
epoch 76 | loss: 277.09151| val_0_rmse: 16.93662|  0:11:37s
epoch 77 | loss: 276.7478| val_0_rmse: 17.16719|  0:11:46s
epoch 78 | loss: 275.01859| val_0_rmse: 16.60951|  0:11:55s
epoch 79 | loss: 265.67298| val_0_rmse: 16.61302|  0:12:04s
epoch 80 | loss: 264.23887| val_0_rmse: 16.39427|  0:12:13s
epoch 81 | loss: 265.9622| val_0_rmse: 16.55075|  0:12:22s
epoch 82 | loss: 258.26363| val_0_rmse: 16.20772|  0:12:31s
epoch 83 | loss: 256.81359| val_0_rmse: 15.96709|  0:12:40s
epoch 84 | loss: 256.51649| val_0_rmse: 16.21725|  0:12:49s
epoch 85 | loss: 253.74253| val_0_rmse: 15.79584|  0:12:58s
epoch 86 | loss: 257.52068| val_0_rmse: 15.86128|  0:13:07s
epoch 87 | loss: 250.32731| val_0_rmse: 15.89996|  0:13:16s
epoch 88 | loss: 245.57006| val_0_rmse: 15.90834|  0:13:26s
epoch 89 | loss: 247.29011| val_0_rmse: 15.85372|  0:13:35s
epoch 90 | loss: 245.68575| val_0_rmse: 16.01316|  0:13:44s
epoch 91 | loss: 245.83714| val_0_rmse: 16.26214|  0:13:53s
epoch 92 | loss: 235.31903| val_0_rmse: 15.72167|  0:14:02s
epoch 93 | loss: 234.43678| val_0_rmse: 15.82149|  0:14:11s
epoch 94 | loss: 237.12575| val_0_rmse: 15.78286|  0:14:20s
epoch 95 | loss: 232.88343| val_0_rmse: 15.68273|  0:14:29s
epoch 96 | loss: 233.04696| val_0_rmse: 15.49283|  0:14:38s
epoch 97 | loss: 227.02148| val_0_rmse: 15.79381|  0:14:47s
epoch 98 | loss: 226.02448| val_0_rmse: 15.6575 |  0:14:56s
epoch 99 | loss: 222.86502| val_0_rmse: 15.93564|  0:15:05s
epoch 100| loss: 223.55733| val_0_rmse: 15.14469|  0:15:14s
epoch 101| loss: 225.00478| val_0_rmse: 15.34872|  0:15:23s
epoch 102| loss: 220.43475| val_0_rmse: 15.44633|  0:15:32s
epoch 103| loss: 221.68837| val_0_rmse: 15.75887|  0:15:41s
epoch 104| loss: 218.99355| val_0_rmse: 15.20622|  0:15:50s
epoch 105| loss: 214.95556| val_0_rmse: 15.25462|  0:15:59s
epoch 106| loss: 211.5148| val_0_rmse: 15.66303|  0:16:08s
epoch 107| loss: 224.56452| val_0_rmse: 15.10084|  0:16:17s
epoch 108| loss: 206.79559| val_0_rmse: 15.63813|  0:16:26s
epoch 109| loss: 211.83334| val_0_rmse: 15.09987|  0:16:35s
epoch 110| loss: 210.79192| val_0_rmse: 14.9826 |  0:16:45s
epoch 111| loss: 207.16186| val_0_rmse: 15.20379|  0:16:54s
epoch 112| loss: 201.88699| val_0_rmse: 14.68656|  0:17:03s
epoch 113| loss: 206.50667| val_0_rmse: 15.04091|  0:17:12s
epoch 114| loss: 204.58645| val_0_rmse: 14.98506|  0:17:21s
epoch 115| loss: 207.23383| val_0_rmse: 14.58951|  0:17:30s
epoch 116| loss: 204.36024| val_0_rmse: 16.1116 |  0:17:39s
epoch 117| loss: 203.10982| val_0_rmse: 15.16933|  0:17:48s
epoch 118| loss: 195.95667| val_0_rmse: 14.7345 |  0:17:57s
epoch 119| loss: 193.49022| val_0_rmse: 14.82172|  0:18:06s
epoch 120| loss: 193.04165| val_0_rmse: 14.82893|  0:18:15s
epoch 121| loss: 192.30143| val_0_rmse: 14.69641|  0:18:24s
epoch 122| loss: 194.75585| val_0_rmse: 14.85223|  0:18:33s
epoch 123| loss: 191.88741| val_0_rmse: 14.52522|  0:18:42s
epoch 124| loss: 184.70967| val_0_rmse: 14.80416|  0:18:51s
epoch 125| loss: 184.3764| val_0_rmse: 14.52005|  0:19:00s
epoch 126| loss: 185.88389| val_0_rmse: 14.44048|  0:19:09s
epoch 127| loss: 178.70239| val_0_rmse: 14.32056|  0:19:18s
epoch 128| loss: 182.76864| val_0_rmse: 14.57735|  0:19:27s
epoch 129| loss: 180.33164| val_0_rmse: 14.58835|  0:19:36s
epoch 130| loss: 188.61636| val_0_rmse: 14.57305|  0:19:45s
epoch 131| loss: 178.79926| val_0_rmse: 14.14039|  0:19:54s
epoch 132| loss: 179.21405| val_0_rmse: 13.92113|  0:20:03s
epoch 133| loss: 172.00252| val_0_rmse: 14.51556|  0:20:12s
epoch 134| loss: 180.36563| val_0_rmse: 14.40731|  0:20:22s
epoch 135| loss: 174.23587| val_0_rmse: 14.48281|  0:20:31s
epoch 136| loss: 174.53722| val_0_rmse: 14.74315|  0:20:40s
epoch 137| loss: 170.92358| val_0_rmse: 14.3332 |  0:20:49s
epoch 138| loss: 172.82726| val_0_rmse: 14.66909|  0:20:58s
epoch 139| loss: 164.9251| val_0_rmse: 14.06261|  0:21:07s
epoch 140| loss: 169.73872| val_0_rmse: 14.00991|  0:21:16s
epoch 141| loss: 169.63017| val_0_rmse: 14.14358|  0:21:25s
epoch 142| loss: 168.60133| val_0_rmse: 14.52305|  0:21:34s
epoch 143| loss: 164.72272| val_0_rmse: 14.49509|  0:21:43s
epoch 144| loss: 171.38007| val_0_rmse: 14.75192|  0:21:52s
epoch 145| loss: 164.58756| val_0_rmse: 15.05516|  0:22:01s
epoch 146| loss: 165.33118| val_0_rmse: 14.11904|  0:22:10s
epoch 147| loss: 165.31354| val_0_rmse: 14.9188 |  0:22:19s
epoch 148| loss: 166.25786| val_0_rmse: 14.74408|  0:22:28s
epoch 149| loss: 169.28069| val_0_rmse: 13.97868|  0:22:37s
epoch 150| loss: 160.09734| val_0_rmse: 14.07128|  0:22:46s
epoch 151| loss: 165.35745| val_0_rmse: 14.01037|  0:22:55s
epoch 152| loss: 166.50609| val_0_rmse: 13.76824|  0:23:04s
epoch 153| loss: 160.14607| val_0_rmse: 14.17838|  0:23:13s
epoch 154| loss: 165.13141| val_0_rmse: 14.17348|  0:23:22s
epoch 155| loss: 163.3603| val_0_rmse: 14.14117|  0:23:31s
epoch 156| loss: 158.89947| val_0_rmse: 14.3869 |  0:23:40s
epoch 157| loss: 159.63091| val_0_rmse: 14.40523|  0:23:49s
epoch 158| loss: 158.84307| val_0_rmse: 14.17131|  0:23:59s
epoch 159| loss: 159.77516| val_0_rmse: 14.09425|  0:24:08s
epoch 160| loss: 159.81101| val_0_rmse: 14.40593|  0:24:17s
epoch 161| loss: 156.54744| val_0_rmse: 14.34579|  0:24:26s
epoch 162| loss: 155.50767| val_0_rmse: 13.96699|  0:24:35s
epoch 163| loss: 153.78815| val_0_rmse: 13.69505|  0:24:44s
epoch 164| loss: 155.537 | val_0_rmse: 14.52722|  0:24:53s
epoch 165| loss: 156.56045| val_0_rmse: 13.76442|  0:25:02s
epoch 166| loss: 155.88162| val_0_rmse: 13.90812|  0:25:11s
epoch 167| loss: 156.75239| val_0_rmse: 13.60265|  0:25:20s
epoch 168| loss: 156.76579| val_0_rmse: 14.0634 |  0:25:29s
epoch 169| loss: 153.64744| val_0_rmse: 14.23143|  0:25:38s
epoch 170| loss: 157.56085| val_0_rmse: 14.00591|  0:25:47s
epoch 171| loss: 155.99434| val_0_rmse: 13.91117|  0:25:56s
epoch 172| loss: 152.36515| val_0_rmse: 14.3544 |  0:26:05s
epoch 173| loss: 150.19717| val_0_rmse: 14.1209 |  0:26:14s
epoch 174| loss: 153.14009| val_0_rmse: 14.49684|  0:26:23s
epoch 175| loss: 149.36835| val_0_rmse: 14.31266|  0:26:32s
epoch 176| loss: 151.28222| val_0_rmse: 13.74137|  0:26:41s
epoch 177| loss: 148.85405| val_0_rmse: 13.64746|  0:26:51s
epoch 178| loss: 150.31866| val_0_rmse: 14.23056|  0:27:00s
epoch 179| loss: 150.0311| val_0_rmse: 14.73344|  0:27:09s
epoch 180| loss: 153.32328| val_0_rmse: 13.77888|  0:27:18s
epoch 181| loss: 150.99717| val_0_rmse: 14.47698|  0:27:27s
epoch 182| loss: 146.4009| val_0_rmse: 14.08668|  0:27:36s
epoch 183| loss: 148.47499| val_0_rmse: 14.04512|  0:27:45s
epoch 184| loss: 149.27511| val_0_rmse: 14.10679|  0:27:54s
epoch 185| loss: 154.20918| val_0_rmse: 14.18873|  0:28:03s
epoch 186| loss: 153.04434| val_0_rmse: 13.9753 |  0:28:12s
epoch 187| loss: 148.75012| val_0_rmse: 13.67267|  0:28:21s
Early stopping occurred at epoch 187 with best_epoch = 167 and best_val_0_rmse = 13.60265
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 19:10:45,809] Trial 5 finished with value: 13.60264906335106 and parameters: {'lr': 0.0010178952607898574, 'n_steps': 7, 'gamma': 1.1103860776569185, 'n_independent': 3, 'n_shared': 1, 'momentum': 0.04839100203300604, 'weight_decay': 0.00038541075528843524, 'batch_size': 128, 'virtual_batch_size': 32}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8682.00545| val_0_rmse: 93.77433|  0:00:04s
epoch 1  | loss: 8715.34881| val_0_rmse: 94.2936 |  0:00:08s
epoch 2  | loss: 8744.93446| val_0_rmse: 94.43844|  0:00:13s
epoch 3  | loss: 8756.40437| val_0_rmse: 94.40149|  0:00:17s
epoch 4  | loss: 8748.79356| val_0_rmse: 94.35499|  0:00:22s
epoch 5  | loss: 8722.93344| val_0_rmse: 94.41211|  0:00:26s
epoch 6  | loss: 8684.26644| val_0_rmse: 94.21354|  0:00:30s
epoch 7  | loss: 8628.0249| val_0_rmse: 93.81165|  0:00:35s
epoch 8  | loss: 8560.3797| val_0_rmse: 93.60525|  0:00:39s
epoch 9  | loss: 8446.97784| val_0_rmse: 92.15621|  0:00:44s
epoch 10 | loss: 8260.48703| val_0_rmse: 91.19539|  0:00:48s
epoch 11 | loss: 7860.34389| val_0_rmse: 87.61302|  0:00:52s
epoch 12 | loss: 6932.79756| val_0_rmse: 80.06154|  0:00:57s
epoch 13 | loss: 5467.63415| val_0_rmse: 68.3944 |  0:01:01s
epoch 14 | loss: 3542.90973| val_0_rmse: 52.097  |  0:01:06s
epoch 15 | loss: 2086.68994| val_0_rmse: 40.40637|  0:01:10s
epoch 16 | loss: 1324.05852| val_0_rmse: 34.43588|  0:01:14s
epoch 17 | loss: 1046.64042| val_0_rmse: 31.9168 |  0:01:19s
epoch 18 | loss: 968.38588| val_0_rmse: 31.13806|  0:01:23s
epoch 19 | loss: 925.1542| val_0_rmse: 30.7055 |  0:01:28s
epoch 20 | loss: 887.97457| val_0_rmse: 29.75991|  0:01:32s
epoch 21 | loss: 849.93215| val_0_rmse: 30.27232|  0:01:36s
epoch 22 | loss: 811.59635| val_0_rmse: 29.28014|  0:01:41s
epoch 23 | loss: 790.10944| val_0_rmse: 29.02341|  0:01:45s
epoch 24 | loss: 761.08958| val_0_rmse: 28.12904|  0:01:50s
epoch 25 | loss: 748.03712| val_0_rmse: 27.4544 |  0:01:54s
epoch 26 | loss: 727.77379| val_0_rmse: 27.15953|  0:01:59s
epoch 27 | loss: 708.13567| val_0_rmse: 26.50257|  0:02:03s
epoch 28 | loss: 697.88349| val_0_rmse: 27.27209|  0:02:07s
epoch 29 | loss: 680.49012| val_0_rmse: 26.51946|  0:02:12s
epoch 30 | loss: 680.12782| val_0_rmse: 27.43501|  0:02:16s
epoch 31 | loss: 662.89333| val_0_rmse: 26.12306|  0:02:20s
epoch 32 | loss: 655.99196| val_0_rmse: 30.40373|  0:02:25s
epoch 33 | loss: 649.20395| val_0_rmse: 27.67322|  0:02:29s
epoch 34 | loss: 642.48918| val_0_rmse: 25.64425|  0:02:34s
epoch 35 | loss: 637.82272| val_0_rmse: 27.58184|  0:02:38s
epoch 36 | loss: 635.00614| val_0_rmse: 26.14371|  0:02:42s
epoch 37 | loss: 626.23643| val_0_rmse: 26.2598 |  0:02:47s
epoch 38 | loss: 632.2357| val_0_rmse: 26.34478|  0:02:51s
epoch 39 | loss: 616.20147| val_0_rmse: 29.81759|  0:02:56s
epoch 40 | loss: 615.73932| val_0_rmse: 25.73364|  0:03:00s
epoch 41 | loss: 611.59919| val_0_rmse: 25.80022|  0:03:05s
epoch 42 | loss: 605.65336| val_0_rmse: 26.30174|  0:03:09s
epoch 43 | loss: 605.79296| val_0_rmse: 26.96637|  0:03:13s
epoch 44 | loss: 592.41635| val_0_rmse: 25.86663|  0:03:18s
epoch 45 | loss: 592.2571| val_0_rmse: 25.42915|  0:03:22s
epoch 46 | loss: 594.39249| val_0_rmse: 26.86552|  0:03:27s
epoch 47 | loss: 579.89087| val_0_rmse: 25.43435|  0:03:31s
epoch 48 | loss: 584.9065| val_0_rmse: 27.32703|  0:03:35s
epoch 49 | loss: 574.0099| val_0_rmse: 26.20345|  0:03:40s
epoch 50 | loss: 578.04567| val_0_rmse: 25.54482|  0:03:44s
epoch 51 | loss: 573.00467| val_0_rmse: 25.36182|  0:03:48s
epoch 52 | loss: 569.90798| val_0_rmse: 24.45149|  0:03:53s
epoch 53 | loss: 572.59366| val_0_rmse: 24.35889|  0:03:57s
epoch 54 | loss: 568.04255| val_0_rmse: 24.40663|  0:04:02s
epoch 55 | loss: 564.34582| val_0_rmse: 24.72369|  0:04:06s
epoch 56 | loss: 563.37093| val_0_rmse: 23.7896 |  0:04:10s
epoch 57 | loss: 557.48748| val_0_rmse: 25.00601|  0:04:15s
epoch 58 | loss: 557.87297| val_0_rmse: 24.69664|  0:04:19s
epoch 59 | loss: 549.42821| val_0_rmse: 24.9851 |  0:04:24s
epoch 60 | loss: 547.89808| val_0_rmse: 24.68956|  0:04:28s
epoch 61 | loss: 540.84417| val_0_rmse: 24.16882|  0:04:33s
epoch 62 | loss: 541.77099| val_0_rmse: 23.76687|  0:04:37s
epoch 63 | loss: 542.9318| val_0_rmse: 24.33761|  0:04:41s
epoch 64 | loss: 533.42688| val_0_rmse: 24.4374 |  0:04:46s
epoch 65 | loss: 535.25023| val_0_rmse: 23.91115|  0:04:50s
epoch 66 | loss: 524.05058| val_0_rmse: 23.84293|  0:04:54s
epoch 67 | loss: 523.80777| val_0_rmse: 23.58796|  0:04:59s
epoch 68 | loss: 518.3977| val_0_rmse: 23.48726|  0:05:03s
epoch 69 | loss: 513.76867| val_0_rmse: 24.04667|  0:05:08s
epoch 70 | loss: 506.86978| val_0_rmse: 26.24021|  0:05:12s
epoch 71 | loss: 509.41299| val_0_rmse: 23.69347|  0:05:17s
epoch 72 | loss: 504.00747| val_0_rmse: 22.97099|  0:05:21s
epoch 73 | loss: 498.55871| val_0_rmse: 23.25599|  0:05:25s
epoch 74 | loss: 492.75946| val_0_rmse: 22.96371|  0:05:30s
epoch 75 | loss: 489.32231| val_0_rmse: 24.3877 |  0:05:34s
epoch 76 | loss: 484.33374| val_0_rmse: 24.89783|  0:05:38s
epoch 77 | loss: 484.51066| val_0_rmse: 23.64997|  0:05:43s
epoch 78 | loss: 480.7045| val_0_rmse: 23.9768 |  0:05:47s
epoch 79 | loss: 470.42827| val_0_rmse: 25.15301|  0:05:51s
epoch 80 | loss: 464.93331| val_0_rmse: 25.24716|  0:05:56s
epoch 81 | loss: 462.61184| val_0_rmse: 23.2507 |  0:06:00s
epoch 82 | loss: 461.57086| val_0_rmse: 26.4596 |  0:06:05s
epoch 83 | loss: 450.95052| val_0_rmse: 24.95818|  0:06:09s
epoch 84 | loss: 448.77273| val_0_rmse: 22.61251|  0:06:13s
epoch 85 | loss: 442.25408| val_0_rmse: 26.71699|  0:06:18s
epoch 86 | loss: 435.60468| val_0_rmse: 22.78172|  0:06:22s
epoch 87 | loss: 428.2082| val_0_rmse: 23.23126|  0:06:27s
epoch 88 | loss: 422.57011| val_0_rmse: 23.39582|  0:06:31s
epoch 89 | loss: 422.91458| val_0_rmse: 24.14446|  0:06:35s
epoch 90 | loss: 412.57725| val_0_rmse: 23.39329|  0:06:40s
epoch 91 | loss: 413.32215| val_0_rmse: 26.26134|  0:06:44s
epoch 92 | loss: 397.54102| val_0_rmse: 24.79995|  0:06:48s
epoch 93 | loss: 402.84037| val_0_rmse: 26.05038|  0:06:53s
epoch 94 | loss: 397.31173| val_0_rmse: 29.14425|  0:06:57s
epoch 95 | loss: 397.57447| val_0_rmse: 25.10894|  0:07:02s
epoch 96 | loss: 383.84616| val_0_rmse: 21.39639|  0:07:06s
epoch 97 | loss: 389.38922| val_0_rmse: 22.65672|  0:07:10s
epoch 98 | loss: 390.4238| val_0_rmse: 23.30573|  0:07:15s
epoch 99 | loss: 381.35373| val_0_rmse: 29.5577 |  0:07:19s
epoch 100| loss: 382.75083| val_0_rmse: 23.05027|  0:07:23s
epoch 101| loss: 370.72285| val_0_rmse: 27.69823|  0:07:28s
epoch 102| loss: 369.26311| val_0_rmse: 24.48178|  0:07:32s
epoch 103| loss: 371.81841| val_0_rmse: 26.21537|  0:07:37s
epoch 104| loss: 365.00394| val_0_rmse: 23.73509|  0:07:41s
epoch 105| loss: 358.31498| val_0_rmse: 25.15929|  0:07:45s
epoch 106| loss: 358.08749| val_0_rmse: 21.62637|  0:07:50s
epoch 107| loss: 353.83578| val_0_rmse: 21.64082|  0:07:54s
epoch 108| loss: 355.72759| val_0_rmse: 20.67592|  0:07:58s
epoch 109| loss: 345.64571| val_0_rmse: 21.76427|  0:08:03s
epoch 110| loss: 341.05644| val_0_rmse: 22.11786|  0:08:07s
epoch 111| loss: 340.45842| val_0_rmse: 23.21001|  0:08:12s
epoch 112| loss: 333.34279| val_0_rmse: 20.89858|  0:08:16s
epoch 113| loss: 335.04191| val_0_rmse: 22.60689|  0:08:20s
epoch 114| loss: 323.89599| val_0_rmse: 22.48701|  0:08:25s
epoch 115| loss: 328.14451| val_0_rmse: 25.29521|  0:08:29s
epoch 116| loss: 322.86544| val_0_rmse: 22.93391|  0:08:33s
epoch 117| loss: 325.21161| val_0_rmse: 22.2632 |  0:08:38s
epoch 118| loss: 320.43595| val_0_rmse: 21.06698|  0:08:42s
epoch 119| loss: 319.04135| val_0_rmse: 19.46   |  0:08:47s
epoch 120| loss: 308.99449| val_0_rmse: 19.85331|  0:08:51s
epoch 121| loss: 314.60164| val_0_rmse: 18.96845|  0:08:55s
epoch 122| loss: 308.60885| val_0_rmse: 20.09328|  0:09:00s
epoch 123| loss: 303.41544| val_0_rmse: 19.11867|  0:09:04s
epoch 124| loss: 305.62025| val_0_rmse: 19.64929|  0:09:08s
epoch 125| loss: 303.95123| val_0_rmse: 18.38127|  0:09:13s
epoch 126| loss: 298.72702| val_0_rmse: 18.14545|  0:09:17s
epoch 127| loss: 297.98397| val_0_rmse: 18.04672|  0:09:22s
epoch 128| loss: 294.23339| val_0_rmse: 17.65292|  0:09:26s
epoch 129| loss: 289.0299| val_0_rmse: 17.86469|  0:09:30s
epoch 130| loss: 284.59464| val_0_rmse: 17.29205|  0:09:35s
epoch 131| loss: 280.68115| val_0_rmse: 17.47703|  0:09:39s
epoch 132| loss: 288.12934| val_0_rmse: 17.39256|  0:09:43s
epoch 133| loss: 280.34833| val_0_rmse: 17.66021|  0:09:48s
epoch 134| loss: 274.91248| val_0_rmse: 17.47413|  0:09:52s
epoch 135| loss: 278.61363| val_0_rmse: 19.06456|  0:09:57s
epoch 136| loss: 273.87572| val_0_rmse: 17.91929|  0:10:01s
epoch 137| loss: 267.80459| val_0_rmse: 17.11975|  0:10:05s
epoch 138| loss: 269.7544| val_0_rmse: 17.08805|  0:10:10s
epoch 139| loss: 266.56629| val_0_rmse: 16.58166|  0:10:14s
epoch 140| loss: 266.61239| val_0_rmse: 17.22377|  0:10:18s
epoch 141| loss: 267.33456| val_0_rmse: 17.41428|  0:10:23s
epoch 142| loss: 265.03829| val_0_rmse: 17.12012|  0:10:27s
epoch 143| loss: 264.30343| val_0_rmse: 17.15821|  0:10:32s
epoch 144| loss: 259.49636| val_0_rmse: 17.81644|  0:10:36s
epoch 145| loss: 255.80332| val_0_rmse: 18.07687|  0:10:40s
epoch 146| loss: 251.56403| val_0_rmse: 17.27804|  0:10:45s
epoch 147| loss: 260.98668| val_0_rmse: 17.09447|  0:10:49s
epoch 148| loss: 249.59432| val_0_rmse: 17.42629|  0:10:53s
epoch 149| loss: 250.49968| val_0_rmse: 16.91886|  0:10:58s
epoch 150| loss: 249.56654| val_0_rmse: 16.53311|  0:11:02s
epoch 151| loss: 247.12047| val_0_rmse: 16.58642|  0:11:06s
epoch 152| loss: 241.24318| val_0_rmse: 16.27066|  0:11:11s
epoch 153| loss: 242.20543| val_0_rmse: 16.13697|  0:11:15s
epoch 154| loss: 246.04885| val_0_rmse: 15.78864|  0:11:20s
epoch 155| loss: 234.99436| val_0_rmse: 16.92463|  0:11:24s
epoch 156| loss: 234.82221| val_0_rmse: 16.8704 |  0:11:28s
epoch 157| loss: 236.02703| val_0_rmse: 15.65707|  0:11:33s
epoch 158| loss: 235.53739| val_0_rmse: 15.50144|  0:11:37s
epoch 159| loss: 233.45221| val_0_rmse: 16.35114|  0:11:41s
epoch 160| loss: 230.31881| val_0_rmse: 16.30772|  0:11:46s
epoch 161| loss: 232.13203| val_0_rmse: 15.61251|  0:11:50s
epoch 162| loss: 231.28372| val_0_rmse: 15.60058|  0:11:55s
epoch 163| loss: 224.84043| val_0_rmse: 15.82837|  0:11:59s
epoch 164| loss: 223.15095| val_0_rmse: 16.63917|  0:12:03s
epoch 165| loss: 225.93667| val_0_rmse: 17.63985|  0:12:08s
epoch 166| loss: 220.10706| val_0_rmse: 16.45476|  0:12:12s
epoch 167| loss: 222.13233| val_0_rmse: 16.36594|  0:12:16s
epoch 168| loss: 216.48237| val_0_rmse: 16.36139|  0:12:21s
epoch 169| loss: 218.78246| val_0_rmse: 16.49041|  0:12:25s
epoch 170| loss: 214.0666| val_0_rmse: 15.11088|  0:12:30s
epoch 171| loss: 212.5226| val_0_rmse: 17.13748|  0:12:34s
epoch 172| loss: 214.4994| val_0_rmse: 17.93745|  0:12:38s
epoch 173| loss: 214.12461| val_0_rmse: 18.17709|  0:12:43s
epoch 174| loss: 211.98807| val_0_rmse: 16.54004|  0:12:47s
epoch 175| loss: 213.88546| val_0_rmse: 16.76751|  0:12:51s
epoch 176| loss: 208.29265| val_0_rmse: 17.54261|  0:12:56s
epoch 177| loss: 207.12609| val_0_rmse: 17.88936|  0:13:00s
epoch 178| loss: 204.10581| val_0_rmse: 15.78898|  0:13:05s
epoch 179| loss: 204.27048| val_0_rmse: 17.02925|  0:13:09s
epoch 180| loss: 202.42019| val_0_rmse: 15.49893|  0:13:13s
epoch 181| loss: 201.34383| val_0_rmse: 17.33306|  0:13:18s
epoch 182| loss: 201.49567| val_0_rmse: 17.60741|  0:13:22s
epoch 183| loss: 202.19149| val_0_rmse: 18.06044|  0:13:26s
epoch 184| loss: 196.73879| val_0_rmse: 15.61029|  0:13:31s
epoch 185| loss: 197.75113| val_0_rmse: 15.44681|  0:13:35s
epoch 186| loss: 201.79761| val_0_rmse: 15.37813|  0:13:40s
epoch 187| loss: 194.60704| val_0_rmse: 14.65745|  0:13:44s
epoch 188| loss: 197.06167| val_0_rmse: 15.58782|  0:13:48s
epoch 189| loss: 193.14535| val_0_rmse: 15.89286|  0:13:53s
epoch 190| loss: 192.37372| val_0_rmse: 16.46307|  0:13:57s
epoch 191| loss: 190.40321| val_0_rmse: 16.03324|  0:14:02s
epoch 192| loss: 191.25084| val_0_rmse: 15.0608 |  0:14:06s
epoch 193| loss: 187.87101| val_0_rmse: 14.94504|  0:14:10s
epoch 194| loss: 186.43212| val_0_rmse: 14.49641|  0:14:15s
epoch 195| loss: 187.25095| val_0_rmse: 14.43095|  0:14:19s
epoch 196| loss: 186.8014| val_0_rmse: 14.64467|  0:14:24s
epoch 197| loss: 185.07792| val_0_rmse: 14.67239|  0:14:28s
epoch 198| loss: 186.67331| val_0_rmse: 15.18388|  0:14:32s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 199| loss: 181.26849| val_0_rmse: 14.92931|  0:14:37s
Stop training because you reached max_epochs = 200 with best_epoch = 195 and best_val_0_rmse = 14.43095
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 19:25:32,598] Trial 6 finished with value: 14.430947366449795 and parameters: {'lr': 0.0016626737167505234, 'n_steps': 8, 'gamma': 1.038405844774057, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.09832545117150898, 'weight_decay': 0.00018098878628032078, 'batch_size': 512, 'virtual_batch_size': 32}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8267.69528| val_0_rmse: 94.11404|  0:00:19s
epoch 1  | loss: 7799.88338| val_0_rmse: 57.43854|  0:00:38s
epoch 2  | loss: 1435.16053| val_0_rmse: 34.96062|  0:00:57s
epoch 3  | loss: 1079.92278| val_0_rmse: 35.95751|  0:01:16s
epoch 4  | loss: 1042.94175| val_0_rmse: 34.10536|  0:01:35s
epoch 5  | loss: 998.58307| val_0_rmse: 43.10466|  0:01:54s
epoch 6  | loss: 983.64374| val_0_rmse: 32.35988|  0:02:13s
epoch 7  | loss: 966.47871| val_0_rmse: 31.45395|  0:02:32s
epoch 8  | loss: 943.82759| val_0_rmse: 31.70437|  0:02:51s
epoch 9  | loss: 931.00639| val_0_rmse: 31.71418|  0:03:10s
epoch 10 | loss: 917.21906| val_0_rmse: 45.33683|  0:03:29s
epoch 11 | loss: 909.3194| val_0_rmse: 31.98407|  0:03:49s
epoch 12 | loss: 896.19152| val_0_rmse: 51.95497|  0:04:08s
epoch 13 | loss: 863.29405| val_0_rmse: 29.59245|  0:04:27s
epoch 14 | loss: 819.60143| val_0_rmse: 28.95184|  0:04:46s
epoch 15 | loss: 747.39927| val_0_rmse: 31.15739|  0:05:05s
epoch 16 | loss: 678.18659| val_0_rmse: 25.80411|  0:05:24s
epoch 17 | loss: 624.54738| val_0_rmse: 25.1291 |  0:05:43s
epoch 18 | loss: 570.08425| val_0_rmse: 40.07199|  0:06:02s
epoch 19 | loss: 526.77844| val_0_rmse: 23.93602|  0:06:21s
epoch 20 | loss: 491.6005| val_0_rmse: 22.77454|  0:06:40s
epoch 21 | loss: 463.99026| val_0_rmse: 22.47846|  0:06:59s
epoch 22 | loss: 443.51372| val_0_rmse: 21.40044|  0:07:18s
epoch 23 | loss: 420.96765| val_0_rmse: 24.95446|  0:07:37s
epoch 24 | loss: 398.186 | val_0_rmse: 822.75905|  0:07:56s
epoch 25 | loss: 381.41512| val_0_rmse: 20.91975|  0:08:15s
epoch 26 | loss: 376.84867| val_0_rmse: 21.71092|  0:08:34s
epoch 27 | loss: 359.5386| val_0_rmse: 20.01232|  0:08:53s
epoch 28 | loss: 343.76541| val_0_rmse: 19.58106|  0:09:12s
epoch 29 | loss: 336.58264| val_0_rmse: 19.8344 |  0:09:31s
epoch 30 | loss: 333.16457| val_0_rmse: 20.41352|  0:09:51s
epoch 31 | loss: 324.6356| val_0_rmse: 19.66011|  0:10:10s
epoch 32 | loss: 307.86601| val_0_rmse: 39.70644|  0:10:29s
epoch 33 | loss: 304.74989| val_0_rmse: 21.16623|  0:10:48s
epoch 34 | loss: 293.68437| val_0_rmse: 25.10945|  0:11:07s
epoch 35 | loss: 290.70551| val_0_rmse: 20.45361|  0:11:26s
epoch 36 | loss: 272.62182| val_0_rmse: 20.51748|  0:11:45s
epoch 37 | loss: 270.6661| val_0_rmse: 19.24136|  0:12:04s
epoch 38 | loss: 265.67621| val_0_rmse: 51.915  |  0:12:23s
epoch 39 | loss: 252.6766| val_0_rmse: 19.51954|  0:12:42s
epoch 40 | loss: 250.27495| val_0_rmse: 18.84191|  0:13:01s
epoch 41 | loss: 249.8836| val_0_rmse: 18.61814|  0:13:20s
epoch 42 | loss: 241.64811| val_0_rmse: 20.17253|  0:13:39s
epoch 43 | loss: 234.86516| val_0_rmse: 24.77051|  0:13:59s
epoch 44 | loss: 227.84902| val_0_rmse: 20.48859|  0:14:18s
epoch 45 | loss: 226.52994| val_0_rmse: 18.16919|  0:14:37s
epoch 46 | loss: 219.89299| val_0_rmse: 19.88335|  0:14:56s
epoch 47 | loss: 222.42083| val_0_rmse: 18.55025|  0:15:15s
epoch 48 | loss: 211.96866| val_0_rmse: 18.28864|  0:15:34s
epoch 49 | loss: 210.34799| val_0_rmse: 17.98971|  0:15:53s
epoch 50 | loss: 207.27249| val_0_rmse: 18.402  |  0:16:12s
epoch 51 | loss: 205.40198| val_0_rmse: 90.95364|  0:16:31s
epoch 52 | loss: 206.60596| val_0_rmse: 17.83829|  0:16:50s
epoch 53 | loss: 199.67661| val_0_rmse: 18.65276|  0:17:09s
epoch 54 | loss: 192.81323| val_0_rmse: 20.78138|  0:17:28s
epoch 55 | loss: 197.22426| val_0_rmse: 18.58867|  0:17:47s
epoch 56 | loss: 193.19828| val_0_rmse: 20.26138|  0:18:06s
epoch 57 | loss: 194.12866| val_0_rmse: 17.35002|  0:18:25s
epoch 58 | loss: 186.00522| val_0_rmse: 18.6448 |  0:18:44s
epoch 59 | loss: 185.1495| val_0_rmse: 18.10041|  0:19:03s
epoch 60 | loss: 183.55982| val_0_rmse: 17.9752 |  0:19:22s
epoch 61 | loss: 183.62395| val_0_rmse: 19.91896|  0:19:41s
epoch 62 | loss: 180.09868| val_0_rmse: 35.96318|  0:20:00s
epoch 63 | loss: 179.76365| val_0_rmse: 20.45719|  0:20:19s
epoch 64 | loss: 178.40226| val_0_rmse: 20.33522|  0:20:38s
epoch 65 | loss: 176.11309| val_0_rmse: 20.70502|  0:20:57s
epoch 66 | loss: 171.47331| val_0_rmse: 22.55864|  0:21:16s
epoch 67 | loss: 168.9372| val_0_rmse: 29.10117|  0:21:35s
epoch 68 | loss: 167.5021| val_0_rmse: 22.11369|  0:21:54s
epoch 69 | loss: 171.00842| val_0_rmse: 20.45126|  0:22:13s
epoch 70 | loss: 168.58308| val_0_rmse: 17.3228 |  0:22:32s
epoch 71 | loss: 162.71706| val_0_rmse: 18.58519|  0:22:51s
epoch 72 | loss: 163.4023| val_0_rmse: 19.64307|  0:23:10s
epoch 73 | loss: 160.22692| val_0_rmse: 20.6681 |  0:23:29s
epoch 74 | loss: 164.32733| val_0_rmse: 18.13889|  0:23:49s
epoch 75 | loss: 157.60382| val_0_rmse: 18.25212|  0:24:08s
epoch 76 | loss: 162.04891| val_0_rmse: 26.71826|  0:24:27s
epoch 77 | loss: 154.37472| val_0_rmse: 18.6071 |  0:24:46s
epoch 78 | loss: 156.42004| val_0_rmse: 22.38404|  0:25:05s
epoch 79 | loss: 152.46293| val_0_rmse: 29.39814|  0:25:24s
epoch 80 | loss: 156.68724| val_0_rmse: 23.97767|  0:25:43s
epoch 81 | loss: 152.88385| val_0_rmse: 21.48396|  0:26:02s
epoch 82 | loss: 154.93309| val_0_rmse: 18.94656|  0:26:21s
epoch 83 | loss: 154.19306| val_0_rmse: 27.45502|  0:26:40s
epoch 84 | loss: 148.75421| val_0_rmse: 20.26369|  0:26:59s
epoch 85 | loss: 154.77927| val_0_rmse: 17.16742|  0:27:18s
epoch 86 | loss: 148.36171| val_0_rmse: 26.80714|  0:27:37s
epoch 87 | loss: 144.34545| val_0_rmse: 19.62516|  0:27:56s
epoch 88 | loss: 146.42904| val_0_rmse: 17.93154|  0:28:15s
epoch 89 | loss: 150.25392| val_0_rmse: 17.95682|  0:28:34s
epoch 90 | loss: 138.60188| val_0_rmse: 23.2633 |  0:28:53s
epoch 91 | loss: 145.34799| val_0_rmse: 28.29081|  0:29:12s
epoch 92 | loss: 143.23306| val_0_rmse: 19.03245|  0:29:31s
epoch 93 | loss: 147.8364| val_0_rmse: 21.36689|  0:29:50s
epoch 94 | loss: 142.01534| val_0_rmse: 18.60001|  0:30:09s
epoch 95 | loss: 139.10046| val_0_rmse: 17.18733|  0:30:28s
epoch 96 | loss: 136.81094| val_0_rmse: 20.58196|  0:30:47s
epoch 97 | loss: 139.09285| val_0_rmse: 128.82066|  0:31:06s
epoch 98 | loss: 141.10288| val_0_rmse: 22.02294|  0:31:25s
epoch 99 | loss: 135.80809| val_0_rmse: 879.85102|  0:31:44s
epoch 100| loss: 131.85166| val_0_rmse: 24.27817|  0:32:03s
epoch 101| loss: 136.87856| val_0_rmse: 18.59265|  0:32:22s
epoch 102| loss: 135.30946| val_0_rmse: 18.71117|  0:32:41s
epoch 103| loss: 132.0254| val_0_rmse: 17.4941 |  0:33:00s
epoch 104| loss: 132.09566| val_0_rmse: 23.42147|  0:33:20s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 105| loss: 134.58542| val_0_rmse: 23.46368|  0:33:38s
Early stopping occurred at epoch 105 with best_epoch = 85 and best_val_0_rmse = 17.16742
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 20:00:27,660] Trial 7 finished with value: 17.167416425920376 and parameters: {'lr': 0.0020131799037363127, 'n_steps': 10, 'gamma': 1.3858317658327048, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.07035108695864306, 'weight_decay': 2.9666779117522594e-05, 'batch_size': 64, 'virtual_batch_size': 32}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 7741.49986| val_0_rmse: 76.50905|  0:00:07s
epoch 1  | loss: 2729.7313| val_0_rmse: 37.28588|  0:00:15s
epoch 2  | loss: 968.38803| val_0_rmse: 34.98331|  0:00:23s
epoch 3  | loss: 742.19132| val_0_rmse: 30.28599|  0:00:30s
epoch 4  | loss: 597.05136| val_0_rmse: 24.15034|  0:00:38s
epoch 5  | loss: 523.23578| val_0_rmse: 31.35149|  0:00:46s
epoch 6  | loss: 477.81862| val_0_rmse: 23.38831|  0:00:54s
epoch 7  | loss: 438.05465| val_0_rmse: 31.16479|  0:01:01s
epoch 8  | loss: 410.66738| val_0_rmse: 24.28587|  0:01:09s
epoch 9  | loss: 391.89898| val_0_rmse: 21.34167|  0:01:17s
epoch 10 | loss: 373.20088| val_0_rmse: 21.6876 |  0:01:24s
epoch 11 | loss: 349.70085| val_0_rmse: 20.18448|  0:01:32s
epoch 12 | loss: 343.76242| val_0_rmse: 19.5447 |  0:01:40s
epoch 13 | loss: 325.34299| val_0_rmse: 19.69845|  0:01:48s
epoch 14 | loss: 315.85593| val_0_rmse: 19.0993 |  0:01:55s
epoch 15 | loss: 315.22174| val_0_rmse: 18.65595|  0:02:03s
epoch 16 | loss: 298.75647| val_0_rmse: 18.35614|  0:02:11s
epoch 17 | loss: 291.19785| val_0_rmse: 18.50632|  0:02:19s
epoch 18 | loss: 281.765 | val_0_rmse: 18.16201|  0:02:26s
epoch 19 | loss: 273.50596| val_0_rmse: 18.70785|  0:02:34s
epoch 20 | loss: 276.90529| val_0_rmse: 18.60175|  0:02:42s
epoch 21 | loss: 264.64606| val_0_rmse: 18.03192|  0:02:49s
epoch 22 | loss: 255.07758| val_0_rmse: 18.42014|  0:02:57s
epoch 23 | loss: 259.49537| val_0_rmse: 17.76425|  0:03:05s
epoch 24 | loss: 244.60799| val_0_rmse: 18.21217|  0:03:13s
epoch 25 | loss: 246.77914| val_0_rmse: 17.84501|  0:03:20s
epoch 26 | loss: 238.90887| val_0_rmse: 19.55793|  0:03:28s
epoch 27 | loss: 237.10126| val_0_rmse: 17.76342|  0:03:36s
epoch 28 | loss: 228.61285| val_0_rmse: 17.21424|  0:03:43s
epoch 29 | loss: 227.69887| val_0_rmse: 17.60422|  0:03:51s
epoch 30 | loss: 221.35522| val_0_rmse: 18.13858|  0:03:59s
epoch 31 | loss: 218.0805| val_0_rmse: 18.02381|  0:04:07s
epoch 32 | loss: 213.33116| val_0_rmse: 17.63974|  0:04:14s
epoch 33 | loss: 217.60024| val_0_rmse: 17.60656|  0:04:22s
epoch 34 | loss: 209.1299| val_0_rmse: 17.65227|  0:04:30s
epoch 35 | loss: 206.02949| val_0_rmse: 16.82507|  0:04:37s
epoch 36 | loss: 205.71345| val_0_rmse: 17.01953|  0:04:45s
epoch 37 | loss: 206.46354| val_0_rmse: 17.37369|  0:04:53s
epoch 38 | loss: 197.89525| val_0_rmse: 17.16462|  0:05:01s
epoch 39 | loss: 197.549 | val_0_rmse: 16.90741|  0:05:08s
epoch 40 | loss: 200.28322| val_0_rmse: 17.282  |  0:05:16s
epoch 41 | loss: 194.06556| val_0_rmse: 17.60282|  0:05:24s
epoch 42 | loss: 188.55414| val_0_rmse: 18.23137|  0:05:31s
epoch 43 | loss: 188.84411| val_0_rmse: 18.21289|  0:05:39s
epoch 44 | loss: 184.38274| val_0_rmse: 17.21786|  0:05:47s
epoch 45 | loss: 186.71255| val_0_rmse: 17.97045|  0:05:55s
epoch 46 | loss: 181.50844| val_0_rmse: 17.36033|  0:06:02s
epoch 47 | loss: 180.99858| val_0_rmse: 17.28281|  0:06:10s
epoch 48 | loss: 181.51885| val_0_rmse: 17.80606|  0:06:18s
epoch 49 | loss: 179.92933| val_0_rmse: 17.29973|  0:06:26s
epoch 50 | loss: 178.23161| val_0_rmse: 17.20796|  0:06:33s
epoch 51 | loss: 175.62126| val_0_rmse: 17.18295|  0:06:41s
epoch 52 | loss: 170.58369| val_0_rmse: 17.38644|  0:06:49s
epoch 53 | loss: 173.34689| val_0_rmse: 17.88814|  0:06:56s
epoch 54 | loss: 172.19089| val_0_rmse: 17.05006|  0:07:04s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 55 | loss: 174.13907| val_0_rmse: 17.40879|  0:07:12s
Early stopping occurred at epoch 55 with best_epoch = 35 and best_val_0_rmse = 16.82507
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 20:08:03,961] Trial 8 finished with value: 16.825073882519337 and parameters: {'lr': 0.0024777416612834823, 'n_steps': 3, 'gamma': 1.210840490888391, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.016608313225149503, 'weight_decay': 1.1161738359204958e-05, 'batch_size': 64, 'virtual_batch_size': 32}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8113.50363| val_0_rmse: 76.81072|  0:00:06s
epoch 1  | loss: 2833.96394| val_0_rmse: 36.27326|  0:00:12s
epoch 2  | loss: 1105.42038| val_0_rmse: 32.94056|  0:00:18s
epoch 3  | loss: 1051.50316| val_0_rmse: 32.74288|  0:00:24s
epoch 4  | loss: 1018.03071| val_0_rmse: 32.50915|  0:00:30s
epoch 5  | loss: 993.38253| val_0_rmse: 31.94914|  0:00:36s
epoch 6  | loss: 961.63225| val_0_rmse: 31.77974|  0:00:42s
epoch 7  | loss: 918.29303| val_0_rmse: 31.96646|  0:00:48s
epoch 8  | loss: 892.83485| val_0_rmse: 30.15284|  0:00:55s
epoch 9  | loss: 814.01089| val_0_rmse: 28.8174 |  0:01:01s
epoch 10 | loss: 681.84067| val_0_rmse: 26.00811|  0:01:07s
epoch 11 | loss: 603.18484| val_0_rmse: 23.96907|  0:01:13s
epoch 12 | loss: 545.48694| val_0_rmse: 22.39701|  0:01:19s
epoch 13 | loss: 492.22645| val_0_rmse: 21.78588|  0:01:25s
epoch 14 | loss: 458.36327| val_0_rmse: 21.26693|  0:01:31s
epoch 15 | loss: 435.17792| val_0_rmse: 20.53161|  0:01:37s
epoch 16 | loss: 418.226 | val_0_rmse: 20.42298|  0:01:43s
epoch 17 | loss: 407.17206| val_0_rmse: 19.86074|  0:01:49s
epoch 18 | loss: 389.98252| val_0_rmse: 19.27421|  0:01:55s
epoch 19 | loss: 369.19354| val_0_rmse: 19.18754|  0:02:02s
epoch 20 | loss: 361.07514| val_0_rmse: 18.26884|  0:02:08s
epoch 21 | loss: 339.6908| val_0_rmse: 18.22808|  0:02:14s
epoch 22 | loss: 331.58304| val_0_rmse: 18.21377|  0:02:20s
epoch 23 | loss: 313.42501| val_0_rmse: 17.61858|  0:02:26s
epoch 24 | loss: 301.76003| val_0_rmse: 17.56184|  0:02:32s
epoch 25 | loss: 292.1818| val_0_rmse: 17.2205 |  0:02:38s
epoch 26 | loss: 286.25472| val_0_rmse: 17.39285|  0:02:44s
epoch 27 | loss: 276.66767| val_0_rmse: 16.81246|  0:02:50s
epoch 28 | loss: 266.12473| val_0_rmse: 17.00321|  0:02:56s
epoch 29 | loss: 265.98264| val_0_rmse: 16.52685|  0:03:02s
epoch 30 | loss: 254.60805| val_0_rmse: 16.92374|  0:03:08s
epoch 31 | loss: 245.9695| val_0_rmse: 16.66253|  0:03:15s
epoch 32 | loss: 246.50437| val_0_rmse: 17.09939|  0:03:21s
epoch 33 | loss: 238.71411| val_0_rmse: 16.63626|  0:03:27s
epoch 34 | loss: 230.36909| val_0_rmse: 16.28906|  0:03:33s
epoch 35 | loss: 234.40094| val_0_rmse: 16.1636 |  0:03:39s
epoch 36 | loss: 223.92036| val_0_rmse: 15.86301|  0:03:45s
epoch 37 | loss: 220.27939| val_0_rmse: 15.91676|  0:03:51s
epoch 38 | loss: 215.77226| val_0_rmse: 16.21108|  0:03:57s
epoch 39 | loss: 210.7857| val_0_rmse: 15.56522|  0:04:03s
epoch 40 | loss: 204.69415| val_0_rmse: 15.70894|  0:04:09s
epoch 41 | loss: 207.71659| val_0_rmse: 16.9483 |  0:04:15s
epoch 42 | loss: 199.52029| val_0_rmse: 15.74539|  0:04:21s
epoch 43 | loss: 197.85444| val_0_rmse: 15.62587|  0:04:27s
epoch 44 | loss: 197.84489| val_0_rmse: 15.91344|  0:04:33s
epoch 45 | loss: 200.67144| val_0_rmse: 15.70415|  0:04:39s
epoch 46 | loss: 192.8245| val_0_rmse: 15.63798|  0:04:46s
epoch 47 | loss: 188.17859| val_0_rmse: 15.65387|  0:04:52s
epoch 48 | loss: 191.50608| val_0_rmse: 15.45374|  0:04:58s
epoch 49 | loss: 182.83109| val_0_rmse: 15.9624 |  0:05:04s
epoch 50 | loss: 178.5154| val_0_rmse: 15.35697|  0:05:10s
epoch 51 | loss: 182.94927| val_0_rmse: 15.33359|  0:05:16s
epoch 52 | loss: 172.13364| val_0_rmse: 15.46988|  0:05:22s
epoch 53 | loss: 175.65389| val_0_rmse: 15.26115|  0:05:28s
epoch 54 | loss: 175.7455| val_0_rmse: 15.05314|  0:05:34s
epoch 55 | loss: 177.28167| val_0_rmse: 15.18853|  0:05:40s
epoch 56 | loss: 175.27241| val_0_rmse: 15.16695|  0:05:46s
epoch 57 | loss: 174.83015| val_0_rmse: 14.73868|  0:05:52s
epoch 58 | loss: 172.05862| val_0_rmse: 14.47767|  0:05:58s
epoch 59 | loss: 170.39873| val_0_rmse: 14.81748|  0:06:04s
epoch 60 | loss: 171.13327| val_0_rmse: 14.47072|  0:06:10s
epoch 61 | loss: 168.66831| val_0_rmse: 14.58833|  0:06:17s
epoch 62 | loss: 164.25409| val_0_rmse: 14.98741|  0:06:23s
epoch 63 | loss: 167.36561| val_0_rmse: 15.00628|  0:06:29s
epoch 64 | loss: 166.87103| val_0_rmse: 14.87066|  0:06:35s
epoch 65 | loss: 168.44681| val_0_rmse: 14.56824|  0:06:41s
epoch 66 | loss: 161.27938| val_0_rmse: 15.10001|  0:06:47s
epoch 67 | loss: 162.94668| val_0_rmse: 14.31846|  0:06:53s
epoch 68 | loss: 155.80361| val_0_rmse: 14.86053|  0:06:59s
epoch 69 | loss: 162.34779| val_0_rmse: 14.42499|  0:07:05s
epoch 70 | loss: 156.98837| val_0_rmse: 14.80679|  0:07:11s
epoch 71 | loss: 157.8227| val_0_rmse: 14.94808|  0:07:17s
epoch 72 | loss: 156.31568| val_0_rmse: 14.27177|  0:07:24s
epoch 73 | loss: 155.28189| val_0_rmse: 14.47186|  0:07:30s
epoch 74 | loss: 157.02696| val_0_rmse: 14.79091|  0:07:36s
epoch 75 | loss: 153.54139| val_0_rmse: 14.59206|  0:07:42s
epoch 76 | loss: 153.24013| val_0_rmse: 14.16528|  0:07:48s
epoch 77 | loss: 155.35461| val_0_rmse: 14.13077|  0:07:54s
epoch 78 | loss: 151.80292| val_0_rmse: 14.81258|  0:08:00s
epoch 79 | loss: 153.47386| val_0_rmse: 14.70301|  0:08:06s
epoch 80 | loss: 147.01245| val_0_rmse: 14.48095|  0:08:12s
epoch 81 | loss: 152.1239| val_0_rmse: 14.4989 |  0:08:18s
epoch 82 | loss: 148.61487| val_0_rmse: 14.5412 |  0:08:24s
epoch 83 | loss: 150.98624| val_0_rmse: 14.24801|  0:08:30s
epoch 84 | loss: 148.75266| val_0_rmse: 14.47789|  0:08:36s
epoch 85 | loss: 148.28218| val_0_rmse: 14.69701|  0:08:43s
epoch 86 | loss: 147.25063| val_0_rmse: 14.67826|  0:08:49s
epoch 87 | loss: 139.88346| val_0_rmse: 13.98737|  0:08:55s
epoch 88 | loss: 146.64985| val_0_rmse: 14.93596|  0:09:01s
epoch 89 | loss: 143.44998| val_0_rmse: 14.52522|  0:09:07s
epoch 90 | loss: 140.79032| val_0_rmse: 14.45688|  0:09:13s
epoch 91 | loss: 141.63886| val_0_rmse: 14.71181|  0:09:19s
epoch 92 | loss: 143.0441| val_0_rmse: 14.39633|  0:09:25s
epoch 93 | loss: 143.88168| val_0_rmse: 14.54388|  0:09:31s
epoch 94 | loss: 145.31275| val_0_rmse: 14.04232|  0:09:37s
epoch 95 | loss: 140.78524| val_0_rmse: 14.20588|  0:09:43s
epoch 96 | loss: 138.09829| val_0_rmse: 14.58906|  0:09:49s
epoch 97 | loss: 139.23894| val_0_rmse: 15.06917|  0:09:55s
epoch 98 | loss: 135.51805| val_0_rmse: 14.48456|  0:10:02s
epoch 99 | loss: 141.20188| val_0_rmse: 15.49764|  0:10:08s
epoch 100| loss: 134.83164| val_0_rmse: 14.42223|  0:10:14s
epoch 101| loss: 136.64412| val_0_rmse: 14.54791|  0:10:20s
epoch 102| loss: 137.08536| val_0_rmse: 14.88929|  0:10:26s
epoch 103| loss: 137.842 | val_0_rmse: 14.39324|  0:10:32s
epoch 104| loss: 142.33255| val_0_rmse: 14.64312|  0:10:38s
epoch 105| loss: 136.59815| val_0_rmse: 14.93635|  0:10:44s
epoch 106| loss: 138.15021| val_0_rmse: 14.57025|  0:10:50s
epoch 107| loss: 137.62104| val_0_rmse: 14.44099|  0:10:56s
Early stopping occurred at epoch 107 with best_epoch = 87 and best_val_0_rmse = 13.98737
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 20:19:24,040] Trial 9 finished with value: 13.98737029203267 and parameters: {'lr': 0.0040694441740085725, 'n_steps': 6, 'gamma': 1.4897591269082748, 'n_independent': 1, 'n_shared': 1, 'momentum': 0.05218091761538869, 'weight_decay': 4.467755218604389e-05, 'batch_size': 128, 'virtual_batch_size': 32}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 6935.29768| val_0_rmse: 62.6835 |  0:00:03s
epoch 1  | loss: 1491.43435| val_0_rmse: 36.73315|  0:00:07s
epoch 2  | loss: 929.62104| val_0_rmse: 37.27932|  0:00:11s
epoch 3  | loss: 724.53074| val_0_rmse: 42.65945|  0:00:15s
epoch 4  | loss: 566.05226| val_0_rmse: 32.08896|  0:00:18s
epoch 5  | loss: 473.32648| val_0_rmse: 22.29   |  0:00:22s
epoch 6  | loss: 420.18601| val_0_rmse: 20.37659|  0:00:26s
epoch 7  | loss: 384.91652| val_0_rmse: 18.6049 |  0:00:30s
epoch 8  | loss: 353.39954| val_0_rmse: 18.78518|  0:00:34s
epoch 9  | loss: 328.08606| val_0_rmse: 18.13971|  0:00:37s
epoch 10 | loss: 311.72474| val_0_rmse: 17.52121|  0:00:41s
epoch 11 | loss: 293.56842| val_0_rmse: 17.01555|  0:00:45s
epoch 12 | loss: 269.04819| val_0_rmse: 16.58255|  0:00:49s
epoch 13 | loss: 266.08044| val_0_rmse: 16.6434 |  0:00:53s
epoch 14 | loss: 246.26836| val_0_rmse: 16.48945|  0:00:56s
epoch 15 | loss: 248.72667| val_0_rmse: 16.31588|  0:01:00s
epoch 16 | loss: 235.28052| val_0_rmse: 15.73227|  0:01:04s
epoch 17 | loss: 229.67043| val_0_rmse: 16.25678|  0:01:08s
epoch 18 | loss: 223.31909| val_0_rmse: 15.48696|  0:01:12s
epoch 19 | loss: 222.30535| val_0_rmse: 15.48035|  0:01:15s
epoch 20 | loss: 210.66438| val_0_rmse: 15.75638|  0:01:19s
epoch 21 | loss: 202.4553| val_0_rmse: 14.85309|  0:01:23s
epoch 22 | loss: 196.56322| val_0_rmse: 15.24767|  0:01:27s
epoch 23 | loss: 193.15178| val_0_rmse: 14.6781 |  0:01:30s
epoch 24 | loss: 194.81626| val_0_rmse: 14.76065|  0:01:34s
epoch 25 | loss: 191.83627| val_0_rmse: 15.19742|  0:01:38s
epoch 26 | loss: 190.51163| val_0_rmse: 14.63197|  0:01:42s
epoch 27 | loss: 184.26279| val_0_rmse: 14.74499|  0:01:46s
epoch 28 | loss: 176.84063| val_0_rmse: 14.21314|  0:01:49s
epoch 29 | loss: 170.11048| val_0_rmse: 14.53166|  0:01:53s
epoch 30 | loss: 173.87313| val_0_rmse: 14.50727|  0:01:57s
epoch 31 | loss: 171.42496| val_0_rmse: 14.06724|  0:02:01s
epoch 32 | loss: 166.99361| val_0_rmse: 14.42144|  0:02:04s
epoch 33 | loss: 171.31773| val_0_rmse: 14.20981|  0:02:08s
epoch 34 | loss: 161.77604| val_0_rmse: 14.45792|  0:02:12s
epoch 35 | loss: 158.83892| val_0_rmse: 14.37545|  0:02:16s
epoch 36 | loss: 165.19903| val_0_rmse: 14.04746|  0:02:20s
epoch 37 | loss: 161.6619| val_0_rmse: 13.8025 |  0:02:23s
epoch 38 | loss: 159.73517| val_0_rmse: 13.95451|  0:02:27s
epoch 39 | loss: 153.76697| val_0_rmse: 13.60616|  0:02:31s
epoch 40 | loss: 151.86816| val_0_rmse: 14.31511|  0:02:35s
epoch 41 | loss: 147.92192| val_0_rmse: 13.97117|  0:02:38s
epoch 42 | loss: 146.507 | val_0_rmse: 14.16469|  0:02:42s
epoch 43 | loss: 149.66542| val_0_rmse: 13.80225|  0:02:46s
epoch 44 | loss: 149.54758| val_0_rmse: 13.68093|  0:02:50s
epoch 45 | loss: 141.60335| val_0_rmse: 13.64534|  0:02:54s
epoch 46 | loss: 142.93265| val_0_rmse: 14.01057|  0:02:57s
epoch 47 | loss: 144.4406| val_0_rmse: 13.76716|  0:03:01s
epoch 48 | loss: 137.96548| val_0_rmse: 13.33983|  0:03:05s
epoch 49 | loss: 139.16331| val_0_rmse: 13.35209|  0:03:09s
epoch 50 | loss: 140.37455| val_0_rmse: 14.08611|  0:03:12s
epoch 51 | loss: 135.90554| val_0_rmse: 13.1334 |  0:03:16s
epoch 52 | loss: 135.59166| val_0_rmse: 13.25798|  0:03:20s
epoch 53 | loss: 132.36879| val_0_rmse: 13.01892|  0:03:24s
epoch 54 | loss: 129.38965| val_0_rmse: 13.01617|  0:03:28s
epoch 55 | loss: 133.26523| val_0_rmse: 13.19892|  0:03:31s
epoch 56 | loss: 132.59  | val_0_rmse: 12.7925 |  0:03:35s
epoch 57 | loss: 131.62938| val_0_rmse: 13.32734|  0:03:39s
epoch 58 | loss: 126.41428| val_0_rmse: 13.09275|  0:03:43s
epoch 59 | loss: 130.04753| val_0_rmse: 13.34354|  0:03:47s
epoch 60 | loss: 127.50122| val_0_rmse: 13.57021|  0:03:50s
epoch 61 | loss: 126.48592| val_0_rmse: 13.77451|  0:03:54s
epoch 62 | loss: 127.88423| val_0_rmse: 13.37603|  0:03:58s
epoch 63 | loss: 132.36222| val_0_rmse: 13.07578|  0:04:02s
epoch 64 | loss: 129.19952| val_0_rmse: 13.13011|  0:04:05s
epoch 65 | loss: 125.19067| val_0_rmse: 13.42635|  0:04:09s
epoch 66 | loss: 126.07957| val_0_rmse: 13.60661|  0:04:13s
epoch 67 | loss: 125.64662| val_0_rmse: 13.46731|  0:04:17s
epoch 68 | loss: 122.71178| val_0_rmse: 13.41273|  0:04:21s
epoch 69 | loss: 125.80701| val_0_rmse: 13.27879|  0:04:24s
epoch 70 | loss: 124.61343| val_0_rmse: 13.17496|  0:04:28s
epoch 71 | loss: 121.27597| val_0_rmse: 13.30903|  0:04:32s
epoch 72 | loss: 124.5297| val_0_rmse: 13.35112|  0:04:36s
epoch 73 | loss: 118.05575| val_0_rmse: 13.68658|  0:04:39s
epoch 74 | loss: 119.06872| val_0_rmse: 13.13264|  0:04:43s
epoch 75 | loss: 118.17411| val_0_rmse: 13.25039|  0:04:47s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 76 | loss: 117.72727| val_0_rmse: 13.1649 |  0:04:50s
Early stopping occurred at epoch 76 with best_epoch = 56 and best_val_0_rmse = 12.7925
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 20:24:22,159] Trial 10 finished with value: 12.792500974446684 and parameters: {'lr': 0.008691745677854862, 'n_steps': 3, 'gamma': 1.2661224665960076, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.033472865148415874, 'weight_decay': 9.726776559840184e-05, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 0 with value: 11.994676293941609.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 6753.52822| val_0_rmse: 62.85267|  0:00:03s
epoch 1  | loss: 1235.33877| val_0_rmse: 32.52157|  0:00:07s
epoch 2  | loss: 805.83593| val_0_rmse: 34.98531|  0:00:11s
epoch 3  | loss: 614.08244| val_0_rmse: 33.77162|  0:00:15s
epoch 4  | loss: 503.68382| val_0_rmse: 26.54828|  0:00:19s
epoch 5  | loss: 436.75618| val_0_rmse: 20.78776|  0:00:22s
epoch 6  | loss: 394.17814| val_0_rmse: 18.99957|  0:00:26s
epoch 7  | loss: 360.72068| val_0_rmse: 18.20731|  0:00:30s
epoch 8  | loss: 331.34471| val_0_rmse: 17.85294|  0:00:34s
epoch 9  | loss: 308.75255| val_0_rmse: 16.69216|  0:00:37s
epoch 10 | loss: 289.20192| val_0_rmse: 16.75612|  0:00:41s
epoch 11 | loss: 273.94234| val_0_rmse: 16.26236|  0:00:45s
epoch 12 | loss: 252.53861| val_0_rmse: 15.96685|  0:00:49s
epoch 13 | loss: 243.06819| val_0_rmse: 15.90368|  0:00:53s
epoch 14 | loss: 231.2987| val_0_rmse: 15.80365|  0:00:56s
epoch 15 | loss: 228.2729| val_0_rmse: 15.71818|  0:01:00s
epoch 16 | loss: 221.38892| val_0_rmse: 15.50487|  0:01:04s
epoch 17 | loss: 215.03729| val_0_rmse: 15.08348|  0:01:08s
epoch 18 | loss: 209.62846| val_0_rmse: 14.94291|  0:01:11s
epoch 19 | loss: 206.07881| val_0_rmse: 14.58761|  0:01:15s
epoch 20 | loss: 192.4228| val_0_rmse: 14.65543|  0:01:19s
epoch 21 | loss: 191.85547| val_0_rmse: 14.48159|  0:01:23s
epoch 22 | loss: 187.92471| val_0_rmse: 14.39181|  0:01:27s
epoch 23 | loss: 185.29011| val_0_rmse: 14.54198|  0:01:30s
epoch 24 | loss: 183.86123| val_0_rmse: 14.61764|  0:01:34s
epoch 25 | loss: 178.77765| val_0_rmse: 14.47617|  0:01:38s
epoch 26 | loss: 183.72913| val_0_rmse: 14.3892 |  0:01:42s
epoch 27 | loss: 171.80695| val_0_rmse: 14.09811|  0:01:46s
epoch 28 | loss: 169.73379| val_0_rmse: 14.02165|  0:01:49s
epoch 29 | loss: 167.28037| val_0_rmse: 14.0928 |  0:01:53s
epoch 30 | loss: 168.52027| val_0_rmse: 14.30839|  0:01:57s
epoch 31 | loss: 164.24969| val_0_rmse: 13.72325|  0:02:01s
epoch 32 | loss: 159.97  | val_0_rmse: 14.08469|  0:02:04s
epoch 33 | loss: 162.35549| val_0_rmse: 13.51828|  0:02:08s
epoch 34 | loss: 153.49103| val_0_rmse: 13.72479|  0:02:12s
epoch 35 | loss: 153.6821| val_0_rmse: 14.37163|  0:02:16s
epoch 36 | loss: 149.32517| val_0_rmse: 13.60999|  0:02:20s
epoch 37 | loss: 157.45975| val_0_rmse: 13.56584|  0:02:23s
epoch 38 | loss: 157.10511| val_0_rmse: 13.92909|  0:02:27s
epoch 39 | loss: 147.43569| val_0_rmse: 14.02538|  0:02:31s
epoch 40 | loss: 148.68569| val_0_rmse: 14.22814|  0:02:35s
epoch 41 | loss: 145.16217| val_0_rmse: 14.28282|  0:02:38s
epoch 42 | loss: 141.38349| val_0_rmse: 13.56347|  0:02:42s
epoch 43 | loss: 142.25142| val_0_rmse: 13.42221|  0:02:46s
epoch 44 | loss: 150.16071| val_0_rmse: 13.42519|  0:02:50s
epoch 45 | loss: 141.27001| val_0_rmse: 13.55518|  0:02:53s
epoch 46 | loss: 133.28085| val_0_rmse: 13.01154|  0:02:57s
epoch 47 | loss: 138.95213| val_0_rmse: 13.10846|  0:03:01s
epoch 48 | loss: 137.3116| val_0_rmse: 12.98917|  0:03:05s
epoch 49 | loss: 134.49099| val_0_rmse: 12.98084|  0:03:09s
epoch 50 | loss: 137.20578| val_0_rmse: 13.38976|  0:03:12s
epoch 51 | loss: 135.89323| val_0_rmse: 13.98614|  0:03:16s
epoch 52 | loss: 138.51227| val_0_rmse: 13.11371|  0:03:20s
epoch 53 | loss: 135.74946| val_0_rmse: 13.22563|  0:03:24s
epoch 54 | loss: 133.14071| val_0_rmse: 13.51337|  0:03:27s
epoch 55 | loss: 131.9525| val_0_rmse: 13.98169|  0:03:31s
epoch 56 | loss: 131.01494| val_0_rmse: 13.38459|  0:03:35s
epoch 57 | loss: 131.71907| val_0_rmse: 12.98301|  0:03:39s
epoch 58 | loss: 126.4599| val_0_rmse: 13.18804|  0:03:42s
epoch 59 | loss: 131.64321| val_0_rmse: 13.60496|  0:03:46s
epoch 60 | loss: 129.88301| val_0_rmse: 13.94976|  0:03:50s
epoch 61 | loss: 126.45662| val_0_rmse: 13.34223|  0:03:54s
epoch 62 | loss: 123.55456| val_0_rmse: 13.07755|  0:03:58s
epoch 63 | loss: 128.93563| val_0_rmse: 13.02612|  0:04:01s
epoch 64 | loss: 129.36807| val_0_rmse: 13.14281|  0:04:05s
epoch 65 | loss: 120.24486| val_0_rmse: 13.29917|  0:04:09s
epoch 66 | loss: 125.44409| val_0_rmse: 12.81918|  0:04:13s
epoch 67 | loss: 121.44535| val_0_rmse: 12.5702 |  0:04:16s
epoch 68 | loss: 125.06572| val_0_rmse: 12.76886|  0:04:20s
epoch 69 | loss: 122.5115| val_0_rmse: 12.88295|  0:04:24s
epoch 70 | loss: 124.49376| val_0_rmse: 12.49453|  0:04:28s
epoch 71 | loss: 118.30956| val_0_rmse: 12.71049|  0:04:32s
epoch 72 | loss: 123.54067| val_0_rmse: 12.57453|  0:04:35s
epoch 73 | loss: 116.49538| val_0_rmse: 12.47108|  0:04:39s
epoch 74 | loss: 120.95455| val_0_rmse: 13.0116 |  0:04:43s
epoch 75 | loss: 115.13644| val_0_rmse: 12.91476|  0:04:47s
epoch 76 | loss: 116.34746| val_0_rmse: 13.4451 |  0:04:50s
epoch 77 | loss: 119.56667| val_0_rmse: 12.85954|  0:04:54s
epoch 78 | loss: 116.89953| val_0_rmse: 12.76266|  0:04:58s
epoch 79 | loss: 114.99198| val_0_rmse: 12.57582|  0:05:02s
epoch 80 | loss: 117.03719| val_0_rmse: 12.93326|  0:05:06s
epoch 81 | loss: 116.78255| val_0_rmse: 12.3815 |  0:05:09s
epoch 82 | loss: 112.66822| val_0_rmse: 12.73514|  0:05:13s
epoch 83 | loss: 114.74923| val_0_rmse: 12.77154|  0:05:17s
epoch 84 | loss: 117.74549| val_0_rmse: 12.33414|  0:05:21s
epoch 85 | loss: 118.20124| val_0_rmse: 12.59334|  0:05:25s
epoch 86 | loss: 113.2647| val_0_rmse: 12.73932|  0:05:28s
epoch 87 | loss: 111.73111| val_0_rmse: 12.1348 |  0:05:32s
epoch 88 | loss: 113.23367| val_0_rmse: 12.26974|  0:05:36s
epoch 89 | loss: 111.48533| val_0_rmse: 12.55182|  0:05:40s
epoch 90 | loss: 116.37203| val_0_rmse: 12.70636|  0:05:43s
epoch 91 | loss: 110.0775| val_0_rmse: 12.70165|  0:05:47s
epoch 92 | loss: 113.33128| val_0_rmse: 12.84876|  0:05:51s
epoch 93 | loss: 112.18826| val_0_rmse: 12.84056|  0:05:55s
epoch 94 | loss: 115.26368| val_0_rmse: 12.75861|  0:05:59s
epoch 95 | loss: 122.58027| val_0_rmse: 12.44298|  0:06:02s
epoch 96 | loss: 114.96882| val_0_rmse: 12.5534 |  0:06:06s
epoch 97 | loss: 112.2003| val_0_rmse: 12.64196|  0:06:10s
epoch 98 | loss: 106.03534| val_0_rmse: 12.71975|  0:06:14s
epoch 99 | loss: 112.54743| val_0_rmse: 12.82583|  0:06:17s
epoch 100| loss: 112.23573| val_0_rmse: 12.53281|  0:06:21s
epoch 101| loss: 109.85923| val_0_rmse: 12.86539|  0:06:25s
epoch 102| loss: 107.7374| val_0_rmse: 12.29447|  0:06:29s
epoch 103| loss: 110.1574| val_0_rmse: 13.06793|  0:06:32s
epoch 104| loss: 110.67353| val_0_rmse: 12.04681|  0:06:36s
epoch 105| loss: 108.97882| val_0_rmse: 12.72832|  0:06:40s
epoch 106| loss: 112.49575| val_0_rmse: 12.87429|  0:06:44s
epoch 107| loss: 108.49203| val_0_rmse: 13.25791|  0:06:48s
epoch 108| loss: 112.41771| val_0_rmse: 12.69519|  0:06:51s
epoch 109| loss: 107.80977| val_0_rmse: 12.7681 |  0:06:55s
epoch 110| loss: 112.29586| val_0_rmse: 12.64546|  0:06:59s
epoch 111| loss: 111.9933| val_0_rmse: 13.15311|  0:07:03s
epoch 112| loss: 113.05043| val_0_rmse: 12.33075|  0:07:06s
epoch 113| loss: 107.8864| val_0_rmse: 12.90642|  0:07:10s
epoch 114| loss: 105.59281| val_0_rmse: 12.40948|  0:07:14s
epoch 115| loss: 107.92402| val_0_rmse: 12.55583|  0:07:18s
epoch 116| loss: 106.38411| val_0_rmse: 12.22313|  0:07:21s
epoch 117| loss: 108.12317| val_0_rmse: 12.0197 |  0:07:25s
epoch 118| loss: 107.19949| val_0_rmse: 12.1356 |  0:07:29s
epoch 119| loss: 110.90954| val_0_rmse: 12.54074|  0:07:33s
epoch 120| loss: 109.04269| val_0_rmse: 12.38617|  0:07:36s
epoch 121| loss: 102.67031| val_0_rmse: 12.33766|  0:07:40s
epoch 122| loss: 104.64986| val_0_rmse: 12.45137|  0:07:44s
epoch 123| loss: 108.97605| val_0_rmse: 12.37121|  0:07:48s
epoch 124| loss: 102.67142| val_0_rmse: 12.33277|  0:07:51s
epoch 125| loss: 105.16935| val_0_rmse: 12.05989|  0:07:55s
epoch 126| loss: 105.45881| val_0_rmse: 12.04084|  0:07:59s
epoch 127| loss: 102.16005| val_0_rmse: 12.34325|  0:08:03s
epoch 128| loss: 107.85443| val_0_rmse: 12.08168|  0:08:06s
epoch 129| loss: 108.4739| val_0_rmse: 12.69367|  0:08:10s
epoch 130| loss: 104.58447| val_0_rmse: 11.69217|  0:08:14s
epoch 131| loss: 106.94903| val_0_rmse: 12.48995|  0:08:18s
epoch 132| loss: 105.69544| val_0_rmse: 12.03853|  0:08:21s
epoch 133| loss: 100.97112| val_0_rmse: 12.33097|  0:08:25s
epoch 134| loss: 111.23355| val_0_rmse: 12.47542|  0:08:29s
epoch 135| loss: 103.19637| val_0_rmse: 12.57213|  0:08:32s
epoch 136| loss: 106.91996| val_0_rmse: 12.61677|  0:08:36s
epoch 137| loss: 107.44681| val_0_rmse: 12.20784|  0:08:40s
epoch 138| loss: 100.6869| val_0_rmse: 12.16964|  0:08:44s
epoch 139| loss: 103.98201| val_0_rmse: 12.09101|  0:08:47s
epoch 140| loss: 104.56506| val_0_rmse: 12.20882|  0:08:51s
epoch 141| loss: 104.18008| val_0_rmse: 12.55823|  0:08:55s
epoch 142| loss: 107.29854| val_0_rmse: 12.72969|  0:08:59s
epoch 143| loss: 104.14336| val_0_rmse: 12.47646|  0:09:02s
epoch 144| loss: 103.31523| val_0_rmse: 12.45789|  0:09:06s
epoch 145| loss: 105.70952| val_0_rmse: 12.0731 |  0:09:10s
epoch 146| loss: 103.04252| val_0_rmse: 12.22682|  0:09:14s
epoch 147| loss: 102.56557| val_0_rmse: 11.69341|  0:09:17s
epoch 148| loss: 102.21696| val_0_rmse: 11.90779|  0:09:21s
epoch 149| loss: 104.39834| val_0_rmse: 11.93272|  0:09:25s
epoch 150| loss: 102.78564| val_0_rmse: 11.99431|  0:09:29s
Early stopping occurred at epoch 150 with best_epoch = 130 and best_val_0_rmse = 11.69217
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 20:33:58,621] Trial 11 finished with value: 11.692167376435432 and parameters: {'lr': 0.009218184743080046, 'n_steps': 3, 'gamma': 1.270134605406791, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.032055268148120514, 'weight_decay': 0.00011309560513855904, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 11 with value: 11.692167376435432.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8289.90686| val_0_rmse: 80.93688|  0:00:04s
epoch 1  | loss: 2525.71371| val_0_rmse: 32.77245|  0:00:09s
epoch 2  | loss: 996.217 | val_0_rmse: 32.79894|  0:00:14s
epoch 3  | loss: 809.70241| val_0_rmse: 32.48372|  0:00:19s
epoch 4  | loss: 637.40729| val_0_rmse: 26.60749|  0:00:24s
epoch 5  | loss: 550.21563| val_0_rmse: 23.46879|  0:00:29s
epoch 6  | loss: 488.88608| val_0_rmse: 21.69212|  0:00:34s
epoch 7  | loss: 442.66039| val_0_rmse: 20.17354|  0:00:39s
epoch 8  | loss: 402.52178| val_0_rmse: 19.37938|  0:00:44s
epoch 9  | loss: 367.34007| val_0_rmse: 18.93021|  0:00:49s
epoch 10 | loss: 336.8422| val_0_rmse: 18.28272|  0:00:54s
epoch 11 | loss: 320.85435| val_0_rmse: 17.78917|  0:00:59s
epoch 12 | loss: 296.51484| val_0_rmse: 17.27636|  0:01:04s
epoch 13 | loss: 281.79695| val_0_rmse: 16.75177|  0:01:09s
epoch 14 | loss: 272.26831| val_0_rmse: 16.97089|  0:01:14s
epoch 15 | loss: 261.05477| val_0_rmse: 16.36796|  0:01:19s
epoch 16 | loss: 242.6169| val_0_rmse: 16.4039 |  0:01:24s
epoch 17 | loss: 240.73632| val_0_rmse: 15.64668|  0:01:29s
epoch 18 | loss: 224.72299| val_0_rmse: 15.48187|  0:01:34s
epoch 19 | loss: 219.50069| val_0_rmse: 15.38757|  0:01:39s
epoch 20 | loss: 212.93961| val_0_rmse: 15.52261|  0:01:44s
epoch 21 | loss: 210.26993| val_0_rmse: 15.00697|  0:01:49s
epoch 22 | loss: 201.60729| val_0_rmse: 14.75036|  0:01:54s
epoch 23 | loss: 197.90569| val_0_rmse: 15.14114|  0:01:59s
epoch 24 | loss: 195.84747| val_0_rmse: 14.88968|  0:02:04s
epoch 25 | loss: 196.01609| val_0_rmse: 14.80498|  0:02:09s
epoch 26 | loss: 189.54597| val_0_rmse: 14.53666|  0:02:14s
epoch 27 | loss: 187.58786| val_0_rmse: 14.72489|  0:02:19s
epoch 28 | loss: 179.11829| val_0_rmse: 14.58802|  0:02:24s
epoch 29 | loss: 181.9137| val_0_rmse: 14.21985|  0:02:29s
epoch 30 | loss: 175.94912| val_0_rmse: 13.98724|  0:02:34s
epoch 31 | loss: 171.38073| val_0_rmse: 13.93837|  0:02:39s
epoch 32 | loss: 172.31292| val_0_rmse: 14.79126|  0:02:44s
epoch 33 | loss: 173.8363| val_0_rmse: 14.00546|  0:02:49s
epoch 34 | loss: 166.98693| val_0_rmse: 13.74495|  0:02:54s
epoch 35 | loss: 164.33241| val_0_rmse: 13.69699|  0:02:59s
epoch 36 | loss: 166.514 | val_0_rmse: 14.46334|  0:03:04s
epoch 37 | loss: 162.41928| val_0_rmse: 14.04633|  0:03:09s
epoch 38 | loss: 165.6292| val_0_rmse: 13.72809|  0:03:14s
epoch 39 | loss: 156.68345| val_0_rmse: 13.46765|  0:03:19s
epoch 40 | loss: 161.87226| val_0_rmse: 13.2246 |  0:03:24s
epoch 41 | loss: 158.23645| val_0_rmse: 13.3    |  0:03:29s
epoch 42 | loss: 162.47317| val_0_rmse: 13.28476|  0:03:34s
epoch 43 | loss: 160.13313| val_0_rmse: 14.04343|  0:03:39s
epoch 44 | loss: 151.17717| val_0_rmse: 14.34304|  0:03:44s
epoch 45 | loss: 158.34439| val_0_rmse: 13.59513|  0:03:49s
epoch 46 | loss: 152.65768| val_0_rmse: 13.37932|  0:03:54s
epoch 47 | loss: 155.00348| val_0_rmse: 13.61039|  0:03:59s
epoch 48 | loss: 148.66267| val_0_rmse: 13.18858|  0:04:04s
epoch 49 | loss: 151.59707| val_0_rmse: 13.6847 |  0:04:09s
epoch 50 | loss: 148.69876| val_0_rmse: 13.37668|  0:04:14s
epoch 51 | loss: 145.62601| val_0_rmse: 12.99507|  0:04:19s
epoch 52 | loss: 147.00824| val_0_rmse: 13.25284|  0:04:24s
epoch 53 | loss: 143.51993| val_0_rmse: 13.86691|  0:04:29s
epoch 54 | loss: 142.32444| val_0_rmse: 13.14186|  0:04:34s
epoch 55 | loss: 138.09457| val_0_rmse: 13.72233|  0:04:39s
epoch 56 | loss: 140.04624| val_0_rmse: 13.14897|  0:04:44s
epoch 57 | loss: 142.78524| val_0_rmse: 13.71384|  0:04:49s
epoch 58 | loss: 137.44909| val_0_rmse: 13.2766 |  0:04:54s
epoch 59 | loss: 133.60088| val_0_rmse: 13.4059 |  0:04:59s
epoch 60 | loss: 134.67215| val_0_rmse: 12.90358|  0:05:04s
epoch 61 | loss: 133.50755| val_0_rmse: 12.97653|  0:05:09s
epoch 62 | loss: 136.36165| val_0_rmse: 13.05827|  0:05:14s
epoch 63 | loss: 137.51094| val_0_rmse: 13.24013|  0:05:19s
epoch 64 | loss: 134.1234| val_0_rmse: 13.17903|  0:05:24s
epoch 65 | loss: 137.90584| val_0_rmse: 12.66174|  0:05:29s
epoch 66 | loss: 140.42312| val_0_rmse: 13.20524|  0:05:34s
epoch 67 | loss: 131.35465| val_0_rmse: 13.48572|  0:05:39s
epoch 68 | loss: 133.26336| val_0_rmse: 13.15198|  0:05:44s
epoch 69 | loss: 133.8201| val_0_rmse: 13.08073|  0:05:49s
epoch 70 | loss: 139.65422| val_0_rmse: 12.70735|  0:05:54s
epoch 71 | loss: 131.14075| val_0_rmse: 13.21445|  0:05:59s
epoch 72 | loss: 134.37846| val_0_rmse: 12.58376|  0:06:04s
epoch 73 | loss: 130.29405| val_0_rmse: 13.32829|  0:06:08s
epoch 74 | loss: 127.3844| val_0_rmse: 13.18907|  0:06:13s
epoch 75 | loss: 132.09661| val_0_rmse: 12.78604|  0:06:19s
epoch 76 | loss: 125.43534| val_0_rmse: 12.9472 |  0:06:23s
epoch 77 | loss: 122.17424| val_0_rmse: 13.18036|  0:06:28s
epoch 78 | loss: 126.07174| val_0_rmse: 13.02061|  0:06:33s
epoch 79 | loss: 127.27999| val_0_rmse: 13.11537|  0:06:38s
epoch 80 | loss: 125.6042| val_0_rmse: 12.78615|  0:06:43s
epoch 81 | loss: 130.90944| val_0_rmse: 13.16329|  0:06:48s
epoch 82 | loss: 125.17938| val_0_rmse: 12.99797|  0:06:53s
epoch 83 | loss: 128.88538| val_0_rmse: 13.02566|  0:06:58s
epoch 84 | loss: 123.14501| val_0_rmse: 12.65432|  0:07:03s
epoch 85 | loss: 125.86864| val_0_rmse: 13.06823|  0:07:08s
epoch 86 | loss: 128.77321| val_0_rmse: 13.06162|  0:07:13s
epoch 87 | loss: 121.7818| val_0_rmse: 13.07558|  0:07:18s
epoch 88 | loss: 125.891 | val_0_rmse: 12.68321|  0:07:23s
epoch 89 | loss: 124.12042| val_0_rmse: 12.72113|  0:07:28s
epoch 90 | loss: 124.54983| val_0_rmse: 12.81759|  0:07:33s
epoch 91 | loss: 126.01964| val_0_rmse: 12.58635|  0:07:38s
epoch 92 | loss: 119.11284| val_0_rmse: 13.22702|  0:07:43s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
Early stopping occurred at epoch 92 with best_epoch = 72 and best_val_0_rmse = 12.58376
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 20:41:53,661] Trial 12 finished with value: 12.583755319868406 and parameters: {'lr': 0.009486227691674445, 'n_steps': 5, 'gamma': 1.2687190714085588, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.03860620462029535, 'weight_decay': 0.00011512306949131475, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 11 with value: 11.692167376435432.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8329.89763| val_0_rmse: 75.53944|  0:00:04s
epoch 1  | loss: 4517.6346| val_0_rmse: 49.49568|  0:00:08s
epoch 2  | loss: 1117.2591| val_0_rmse: 33.81038|  0:00:13s
epoch 3  | loss: 911.19845| val_0_rmse: 35.6382 |  0:00:17s
epoch 4  | loss: 802.93143| val_0_rmse: 35.41318|  0:00:22s
epoch 5  | loss: 637.99259| val_0_rmse: 28.97631|  0:00:26s
epoch 6  | loss: 518.69426| val_0_rmse: 23.16405|  0:00:30s
epoch 7  | loss: 447.72878| val_0_rmse: 20.43201|  0:00:35s
epoch 8  | loss: 401.9501| val_0_rmse: 19.55067|  0:00:39s
epoch 9  | loss: 369.99811| val_0_rmse: 19.06094|  0:00:44s
epoch 10 | loss: 342.82568| val_0_rmse: 18.25163|  0:00:48s
epoch 11 | loss: 320.52182| val_0_rmse: 18.12759|  0:00:53s
epoch 12 | loss: 297.15462| val_0_rmse: 17.01236|  0:00:57s
epoch 13 | loss: 277.34513| val_0_rmse: 16.65419|  0:01:01s
epoch 14 | loss: 262.40717| val_0_rmse: 16.90139|  0:01:06s
epoch 15 | loss: 260.71372| val_0_rmse: 16.31567|  0:01:10s
epoch 16 | loss: 249.96755| val_0_rmse: 16.40953|  0:01:14s
epoch 17 | loss: 242.05946| val_0_rmse: 16.59581|  0:01:19s
epoch 18 | loss: 232.3847| val_0_rmse: 15.916  |  0:01:23s
epoch 19 | loss: 229.96863| val_0_rmse: 15.47414|  0:01:28s
epoch 20 | loss: 221.52526| val_0_rmse: 16.91466|  0:01:32s
epoch 21 | loss: 224.73786| val_0_rmse: 15.70138|  0:01:38s
epoch 22 | loss: 211.91615| val_0_rmse: 15.4112 |  0:01:42s
epoch 23 | loss: 205.98876| val_0_rmse: 16.18863|  0:01:47s
epoch 24 | loss: 208.82122| val_0_rmse: 15.68183|  0:01:51s
epoch 25 | loss: 206.20177| val_0_rmse: 15.70759|  0:01:56s
epoch 26 | loss: 199.08741| val_0_rmse: 15.51102|  0:02:00s
epoch 27 | loss: 203.21007| val_0_rmse: 14.97201|  0:02:04s
epoch 28 | loss: 205.40372| val_0_rmse: 15.10598|  0:02:09s
epoch 29 | loss: 198.12345| val_0_rmse: 15.01479|  0:02:13s
epoch 30 | loss: 201.7637| val_0_rmse: 14.64052|  0:02:18s
epoch 31 | loss: 198.91243| val_0_rmse: 15.97676|  0:02:22s
epoch 32 | loss: 190.47746| val_0_rmse: 14.93353|  0:02:26s
epoch 33 | loss: 191.36224| val_0_rmse: 14.62267|  0:02:31s
epoch 34 | loss: 198.61543| val_0_rmse: 15.42235|  0:02:35s
epoch 35 | loss: 189.90143| val_0_rmse: 14.85777|  0:02:39s
epoch 36 | loss: 194.72394| val_0_rmse: 15.18723|  0:02:44s
epoch 37 | loss: 188.25238| val_0_rmse: 14.8929 |  0:02:48s
epoch 38 | loss: 191.36851| val_0_rmse: 14.45897|  0:02:53s
epoch 39 | loss: 189.09013| val_0_rmse: 13.94435|  0:02:57s
epoch 40 | loss: 180.01537| val_0_rmse: 13.94494|  0:03:01s
epoch 41 | loss: 184.0684| val_0_rmse: 15.1393 |  0:03:06s
epoch 42 | loss: 180.81791| val_0_rmse: 14.54876|  0:03:10s
epoch 43 | loss: 185.56097| val_0_rmse: 14.65095|  0:03:15s
epoch 44 | loss: 178.28638| val_0_rmse: 14.74109|  0:03:19s
epoch 45 | loss: 183.1397| val_0_rmse: 15.29624|  0:03:23s
epoch 46 | loss: 181.06334| val_0_rmse: 14.66371|  0:03:28s
epoch 47 | loss: 177.90448| val_0_rmse: 15.39959|  0:03:32s
epoch 48 | loss: 174.87708| val_0_rmse: 15.73932|  0:03:36s
epoch 49 | loss: 186.50281| val_0_rmse: 14.99008|  0:03:41s
epoch 50 | loss: 176.85552| val_0_rmse: 14.39098|  0:03:45s
epoch 51 | loss: 178.62061| val_0_rmse: 17.2151 |  0:03:50s
epoch 52 | loss: 176.45202| val_0_rmse: 14.10351|  0:03:54s
epoch 53 | loss: 173.01475| val_0_rmse: 14.70676|  0:03:58s
epoch 54 | loss: 173.43148| val_0_rmse: 18.43326|  0:04:03s
epoch 55 | loss: 173.43542| val_0_rmse: 13.98365|  0:04:07s
epoch 56 | loss: 172.15184| val_0_rmse: 14.11769|  0:04:11s
epoch 57 | loss: 172.80269| val_0_rmse: 16.07056|  0:04:16s
epoch 58 | loss: 168.10287| val_0_rmse: 14.66308|  0:04:20s
epoch 59 | loss: 167.29743| val_0_rmse: 14.36651|  0:04:25s
Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_rmse = 13.94435
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 20:46:28,194] Trial 13 finished with value: 13.944350000424103 and parameters: {'lr': 0.0053982914857378245, 'n_steps': 4, 'gamma': 1.1986555111505497, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.013742198066330404, 'weight_decay': 0.000854275256367588, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 11 with value: 11.692167376435432.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8043.31339| val_0_rmse: 95.37956|  0:00:04s
epoch 1  | loss: 3348.7995| val_0_rmse: 35.87664|  0:00:08s
epoch 2  | loss: 1072.56059| val_0_rmse: 32.74281|  0:00:12s
epoch 3  | loss: 999.05221| val_0_rmse: 31.85861|  0:00:16s
epoch 4  | loss: 953.94593| val_0_rmse: 30.89516|  0:00:20s
epoch 5  | loss: 857.63404| val_0_rmse: 28.62974|  0:00:24s
epoch 6  | loss: 725.82884| val_0_rmse: 26.7021 |  0:00:28s
epoch 7  | loss: 609.73502| val_0_rmse: 24.39558|  0:00:32s
epoch 8  | loss: 544.90877| val_0_rmse: 22.20819|  0:00:36s
epoch 9  | loss: 489.59319| val_0_rmse: 21.03462|  0:00:40s
epoch 10 | loss: 446.38188| val_0_rmse: 20.20063|  0:00:44s
epoch 11 | loss: 406.12446| val_0_rmse: 19.34626|  0:00:48s
epoch 12 | loss: 376.96772| val_0_rmse: 18.91267|  0:00:52s
epoch 13 | loss: 347.68553| val_0_rmse: 18.56737|  0:00:56s
epoch 14 | loss: 334.76835| val_0_rmse: 18.04161|  0:01:00s
epoch 15 | loss: 313.14774| val_0_rmse: 17.52861|  0:01:04s
epoch 16 | loss: 302.49174| val_0_rmse: 17.24492|  0:01:08s
epoch 17 | loss: 283.24413| val_0_rmse: 16.96812|  0:01:12s
epoch 18 | loss: 276.40872| val_0_rmse: 16.70695|  0:01:16s
epoch 19 | loss: 261.41895| val_0_rmse: 16.2252 |  0:01:20s
epoch 20 | loss: 255.75515| val_0_rmse: 16.1448 |  0:01:24s
epoch 21 | loss: 241.16203| val_0_rmse: 15.90335|  0:01:28s
epoch 22 | loss: 241.56991| val_0_rmse: 15.73834|  0:01:32s
epoch 23 | loss: 226.93341| val_0_rmse: 15.19074|  0:01:36s
epoch 24 | loss: 224.38881| val_0_rmse: 15.21978|  0:01:40s
epoch 25 | loss: 216.12371| val_0_rmse: 14.83301|  0:01:44s
epoch 26 | loss: 209.41088| val_0_rmse: 14.95705|  0:01:49s
epoch 27 | loss: 204.82544| val_0_rmse: 14.87649|  0:01:53s
epoch 28 | loss: 206.37235| val_0_rmse: 14.6699 |  0:01:57s
epoch 29 | loss: 197.23848| val_0_rmse: 14.26242|  0:02:01s
epoch 30 | loss: 189.47822| val_0_rmse: 14.1789 |  0:02:05s
epoch 31 | loss: 187.33367| val_0_rmse: 14.60426|  0:02:09s
epoch 32 | loss: 186.68103| val_0_rmse: 14.61908|  0:02:13s
epoch 33 | loss: 176.41882| val_0_rmse: 14.15115|  0:02:17s
epoch 34 | loss: 183.40741| val_0_rmse: 14.04391|  0:02:21s
epoch 35 | loss: 177.51579| val_0_rmse: 13.72276|  0:02:25s
epoch 36 | loss: 170.35374| val_0_rmse: 13.75049|  0:02:29s
epoch 37 | loss: 166.83759| val_0_rmse: 13.82717|  0:02:33s
epoch 38 | loss: 164.01  | val_0_rmse: 13.76083|  0:02:37s
epoch 39 | loss: 163.58912| val_0_rmse: 14.00435|  0:02:41s
epoch 40 | loss: 161.14041| val_0_rmse: 13.5346 |  0:02:45s
epoch 41 | loss: 152.84412| val_0_rmse: 14.09096|  0:02:49s
epoch 42 | loss: 157.83939| val_0_rmse: 13.52665|  0:02:53s
epoch 43 | loss: 152.99013| val_0_rmse: 13.33324|  0:02:57s
epoch 44 | loss: 151.3838| val_0_rmse: 13.28447|  0:03:01s
epoch 45 | loss: 149.50841| val_0_rmse: 13.29785|  0:03:05s
epoch 46 | loss: 151.80922| val_0_rmse: 13.19616|  0:03:09s
epoch 47 | loss: 148.2851| val_0_rmse: 13.33062|  0:03:13s
epoch 48 | loss: 145.10954| val_0_rmse: 13.23177|  0:03:17s
epoch 49 | loss: 147.80349| val_0_rmse: 13.4911 |  0:03:21s
epoch 50 | loss: 138.70099| val_0_rmse: 13.86674|  0:03:25s
epoch 51 | loss: 136.74867| val_0_rmse: 13.26784|  0:03:29s
epoch 52 | loss: 138.35443| val_0_rmse: 13.19143|  0:03:33s
epoch 53 | loss: 141.50397| val_0_rmse: 13.11248|  0:03:37s
epoch 54 | loss: 141.62369| val_0_rmse: 13.06927|  0:03:41s
epoch 55 | loss: 142.16339| val_0_rmse: 13.73411|  0:03:45s
epoch 56 | loss: 135.80168| val_0_rmse: 13.45617|  0:03:49s
epoch 57 | loss: 131.08785| val_0_rmse: 12.96425|  0:03:53s
epoch 58 | loss: 128.86593| val_0_rmse: 13.16388|  0:03:57s
epoch 59 | loss: 132.97466| val_0_rmse: 13.03201|  0:04:01s
epoch 60 | loss: 132.06943| val_0_rmse: 12.84362|  0:04:05s
epoch 61 | loss: 130.26664| val_0_rmse: 12.80563|  0:04:09s
epoch 62 | loss: 124.57031| val_0_rmse: 12.72119|  0:04:12s
epoch 63 | loss: 131.52973| val_0_rmse: 13.03049|  0:04:16s
epoch 64 | loss: 127.91028| val_0_rmse: 12.99636|  0:04:20s
epoch 65 | loss: 127.50124| val_0_rmse: 13.08132|  0:04:24s
epoch 66 | loss: 132.31655| val_0_rmse: 12.75253|  0:04:28s
epoch 67 | loss: 123.03866| val_0_rmse: 12.99383|  0:04:32s
epoch 68 | loss: 126.07676| val_0_rmse: 12.92977|  0:04:36s
epoch 69 | loss: 124.69522| val_0_rmse: 12.94752|  0:04:40s
epoch 70 | loss: 119.59564| val_0_rmse: 12.63297|  0:04:44s
epoch 71 | loss: 120.10528| val_0_rmse: 12.84991|  0:04:48s
epoch 72 | loss: 121.42113| val_0_rmse: 12.90246|  0:04:52s
epoch 73 | loss: 117.86648| val_0_rmse: 12.7203 |  0:04:56s
epoch 74 | loss: 122.01018| val_0_rmse: 12.47748|  0:05:00s
epoch 75 | loss: 116.42974| val_0_rmse: 13.00462|  0:05:04s
epoch 76 | loss: 119.20529| val_0_rmse: 12.6806 |  0:05:08s
epoch 77 | loss: 117.98637| val_0_rmse: 12.37297|  0:05:12s
epoch 78 | loss: 119.50319| val_0_rmse: 12.41991|  0:05:16s
epoch 79 | loss: 120.83989| val_0_rmse: 12.69169|  0:05:20s
epoch 80 | loss: 113.18449| val_0_rmse: 12.32433|  0:05:24s
epoch 81 | loss: 111.7882| val_0_rmse: 12.59587|  0:05:28s
epoch 82 | loss: 116.12239| val_0_rmse: 12.73959|  0:05:32s
epoch 83 | loss: 110.20936| val_0_rmse: 12.5746 |  0:05:36s
epoch 84 | loss: 110.59679| val_0_rmse: 12.45289|  0:05:40s
epoch 85 | loss: 109.03964| val_0_rmse: 12.82104|  0:05:44s
epoch 86 | loss: 113.45563| val_0_rmse: 12.1112 |  0:05:48s
epoch 87 | loss: 115.73087| val_0_rmse: 12.60045|  0:05:52s
epoch 88 | loss: 115.00224| val_0_rmse: 12.40271|  0:05:56s
epoch 89 | loss: 109.80933| val_0_rmse: 12.44656|  0:06:00s
epoch 90 | loss: 106.98406| val_0_rmse: 12.3016 |  0:06:04s
epoch 91 | loss: 110.14225| val_0_rmse: 12.48185|  0:06:08s
epoch 92 | loss: 111.76201| val_0_rmse: 13.19263|  0:06:12s
epoch 93 | loss: 108.86621| val_0_rmse: 12.56069|  0:06:16s
epoch 94 | loss: 105.80995| val_0_rmse: 12.31946|  0:06:20s
epoch 95 | loss: 107.43799| val_0_rmse: 12.80355|  0:06:24s
epoch 96 | loss: 105.00359| val_0_rmse: 12.72859|  0:06:28s
epoch 97 | loss: 103.21312| val_0_rmse: 12.52774|  0:06:32s
epoch 98 | loss: 106.31404| val_0_rmse: 12.51846|  0:06:36s
epoch 99 | loss: 106.54629| val_0_rmse: 12.38253|  0:06:40s
epoch 100| loss: 108.98671| val_0_rmse: 12.86993|  0:06:44s
epoch 101| loss: 107.28577| val_0_rmse: 12.25892|  0:06:48s
epoch 102| loss: 105.91148| val_0_rmse: 12.16356|  0:06:52s
epoch 103| loss: 103.80415| val_0_rmse: 12.25299|  0:06:56s
epoch 104| loss: 98.73324| val_0_rmse: 12.30304|  0:07:00s
epoch 105| loss: 100.59716| val_0_rmse: 12.47654|  0:07:04s
epoch 106| loss: 103.57304| val_0_rmse: 12.37008|  0:07:08s
Early stopping occurred at epoch 106 with best_epoch = 86 and best_val_0_rmse = 12.1112
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 20:53:45,797] Trial 14 finished with value: 12.111197629072437 and parameters: {'lr': 0.006629974766795828, 'n_steps': 4, 'gamma': 1.0120776923902715, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.04115298923556734, 'weight_decay': 7.082622383637549e-05, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 11 with value: 11.692167376435432.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8549.04205| val_0_rmse: 91.13212|  0:00:03s
epoch 1  | loss: 7994.84538| val_0_rmse: 85.94435|  0:00:06s
epoch 2  | loss: 5028.96793| val_0_rmse: 138.20358|  0:00:09s
epoch 3  | loss: 1568.71752| val_0_rmse: 33.81118|  0:00:12s
epoch 4  | loss: 1048.2053| val_0_rmse: 32.70931|  0:00:15s
epoch 5  | loss: 1019.71413| val_0_rmse: 32.51015|  0:00:19s
epoch 6  | loss: 999.43219| val_0_rmse: 33.69049|  0:00:22s
epoch 7  | loss: 995.75351| val_0_rmse: 32.7303 |  0:00:25s
epoch 8  | loss: 968.56119| val_0_rmse: 31.83657|  0:00:28s
epoch 9  | loss: 926.98102| val_0_rmse: 31.57298|  0:00:31s
epoch 10 | loss: 829.38992| val_0_rmse: 28.60052|  0:00:35s
epoch 11 | loss: 726.634 | val_0_rmse: 26.37188|  0:00:38s
epoch 12 | loss: 641.72853| val_0_rmse: 24.08993|  0:00:41s
epoch 13 | loss: 547.67611| val_0_rmse: 23.09534|  0:00:44s
epoch 14 | loss: 478.60457| val_0_rmse: 20.7966 |  0:00:47s
epoch 15 | loss: 429.46484| val_0_rmse: 19.81737|  0:00:50s
epoch 16 | loss: 390.24757| val_0_rmse: 19.17858|  0:00:54s
epoch 17 | loss: 362.67868| val_0_rmse: 18.60516|  0:00:57s
epoch 18 | loss: 347.97148| val_0_rmse: 18.31078|  0:01:00s
epoch 19 | loss: 327.34096| val_0_rmse: 19.5312 |  0:01:03s
epoch 20 | loss: 311.49303| val_0_rmse: 18.14968|  0:01:06s
epoch 21 | loss: 297.68103| val_0_rmse: 16.85987|  0:01:10s
epoch 22 | loss: 279.05243| val_0_rmse: 16.72747|  0:01:13s
epoch 23 | loss: 268.16444| val_0_rmse: 16.23669|  0:01:16s
epoch 24 | loss: 264.27839| val_0_rmse: 16.52157|  0:01:19s
epoch 25 | loss: 244.72665| val_0_rmse: 15.57872|  0:01:22s
epoch 26 | loss: 238.20566| val_0_rmse: 15.43655|  0:01:25s
epoch 27 | loss: 232.60886| val_0_rmse: 15.22441|  0:01:29s
epoch 28 | loss: 228.66722| val_0_rmse: 15.0403 |  0:01:32s
epoch 29 | loss: 224.44003| val_0_rmse: 14.9494 |  0:01:35s
epoch 30 | loss: 209.76499| val_0_rmse: 14.83391|  0:01:38s
epoch 31 | loss: 209.23324| val_0_rmse: 14.48516|  0:01:41s
epoch 32 | loss: 202.78402| val_0_rmse: 14.80889|  0:01:45s
epoch 33 | loss: 206.11837| val_0_rmse: 15.02577|  0:01:48s
epoch 34 | loss: 193.73591| val_0_rmse: 14.32308|  0:01:51s
epoch 35 | loss: 191.35617| val_0_rmse: 14.45391|  0:01:54s
epoch 36 | loss: 184.53315| val_0_rmse: 14.30438|  0:01:57s
epoch 37 | loss: 184.14417| val_0_rmse: 14.36996|  0:02:01s
epoch 38 | loss: 181.99853| val_0_rmse: 14.48002|  0:02:04s
epoch 39 | loss: 179.88558| val_0_rmse: 14.24406|  0:02:07s
epoch 40 | loss: 172.6751| val_0_rmse: 14.06337|  0:02:10s
epoch 41 | loss: 172.73232| val_0_rmse: 13.72603|  0:02:13s
epoch 42 | loss: 168.93402| val_0_rmse: 13.61732|  0:02:16s
epoch 43 | loss: 165.82284| val_0_rmse: 13.94235|  0:02:20s
epoch 44 | loss: 166.49726| val_0_rmse: 14.1139 |  0:02:23s
epoch 45 | loss: 167.20408| val_0_rmse: 13.52746|  0:02:26s
epoch 46 | loss: 157.85348| val_0_rmse: 13.59118|  0:02:29s
epoch 47 | loss: 156.66025| val_0_rmse: 13.62078|  0:02:32s
epoch 48 | loss: 156.95039| val_0_rmse: 13.42473|  0:02:35s
epoch 49 | loss: 150.01715| val_0_rmse: 13.509  |  0:02:39s
epoch 50 | loss: 151.67061| val_0_rmse: 13.23175|  0:02:42s
epoch 51 | loss: 149.55264| val_0_rmse: 13.31225|  0:02:45s
epoch 52 | loss: 148.2811| val_0_rmse: 13.34846|  0:02:48s
epoch 53 | loss: 146.0847| val_0_rmse: 13.21612|  0:02:51s
epoch 54 | loss: 153.60363| val_0_rmse: 13.11553|  0:02:54s
epoch 55 | loss: 146.48144| val_0_rmse: 13.38291|  0:02:58s
epoch 56 | loss: 149.61789| val_0_rmse: 13.45048|  0:03:01s
epoch 57 | loss: 147.33313| val_0_rmse: 13.20314|  0:03:04s
epoch 58 | loss: 143.66198| val_0_rmse: 13.41831|  0:03:07s
epoch 59 | loss: 141.98099| val_0_rmse: 13.36367|  0:03:10s
epoch 60 | loss: 143.44033| val_0_rmse: 13.51532|  0:03:14s
epoch 61 | loss: 143.28174| val_0_rmse: 13.04634|  0:03:17s
epoch 62 | loss: 131.57828| val_0_rmse: 13.32325|  0:03:20s
epoch 63 | loss: 140.89459| val_0_rmse: 13.07674|  0:03:23s
epoch 64 | loss: 135.24217| val_0_rmse: 12.9863 |  0:03:26s
epoch 65 | loss: 133.9409| val_0_rmse: 12.86436|  0:03:30s
epoch 66 | loss: 133.75427| val_0_rmse: 13.02012|  0:03:33s
epoch 67 | loss: 141.09492| val_0_rmse: 13.04739|  0:03:36s
epoch 68 | loss: 129.68744| val_0_rmse: 13.01607|  0:03:39s
epoch 69 | loss: 128.12259| val_0_rmse: 12.64236|  0:03:42s
epoch 70 | loss: 132.76553| val_0_rmse: 13.04011|  0:03:45s
epoch 71 | loss: 125.03615| val_0_rmse: 12.58889|  0:03:49s
epoch 72 | loss: 131.29527| val_0_rmse: 12.86271|  0:03:52s
epoch 73 | loss: 124.22416| val_0_rmse: 12.63798|  0:03:55s
epoch 74 | loss: 125.41255| val_0_rmse: 12.81013|  0:03:58s
epoch 75 | loss: 124.11086| val_0_rmse: 12.96744|  0:04:01s
epoch 76 | loss: 122.63434| val_0_rmse: 13.08111|  0:04:04s
epoch 77 | loss: 123.21327| val_0_rmse: 12.98204|  0:04:08s
epoch 78 | loss: 122.40471| val_0_rmse: 12.61156|  0:04:11s
epoch 79 | loss: 116.82307| val_0_rmse: 12.75662|  0:04:14s
epoch 80 | loss: 116.67386| val_0_rmse: 12.90282|  0:04:17s
epoch 81 | loss: 116.97232| val_0_rmse: 13.14414|  0:04:20s
epoch 82 | loss: 120.43463| val_0_rmse: 12.54604|  0:04:23s
epoch 83 | loss: 117.95219| val_0_rmse: 12.76225|  0:04:26s
epoch 84 | loss: 115.27102| val_0_rmse: 12.48167|  0:04:30s
epoch 85 | loss: 112.04805| val_0_rmse: 12.68676|  0:04:33s
epoch 86 | loss: 113.57042| val_0_rmse: 12.3996 |  0:04:36s
epoch 87 | loss: 116.82267| val_0_rmse: 12.24909|  0:04:39s
epoch 88 | loss: 114.48505| val_0_rmse: 12.83723|  0:04:42s
epoch 89 | loss: 112.37716| val_0_rmse: 12.36748|  0:04:45s
epoch 90 | loss: 113.96816| val_0_rmse: 12.58409|  0:04:49s
epoch 91 | loss: 113.77054| val_0_rmse: 12.3582 |  0:04:52s
epoch 92 | loss: 113.50084| val_0_rmse: 12.60912|  0:04:55s
epoch 93 | loss: 113.32391| val_0_rmse: 12.5318 |  0:04:58s
epoch 94 | loss: 109.43508| val_0_rmse: 12.66717|  0:05:01s
epoch 95 | loss: 110.08717| val_0_rmse: 12.48882|  0:05:04s
epoch 96 | loss: 108.5603| val_0_rmse: 12.76982|  0:05:07s
epoch 97 | loss: 111.69476| val_0_rmse: 12.48424|  0:05:11s
epoch 98 | loss: 108.72563| val_0_rmse: 12.87229|  0:05:14s
epoch 99 | loss: 109.4424| val_0_rmse: 12.74267|  0:05:17s
epoch 100| loss: 115.25272| val_0_rmse: 12.60734|  0:05:20s
epoch 101| loss: 106.42532| val_0_rmse: 12.7337 |  0:05:23s
epoch 102| loss: 107.10929| val_0_rmse: 12.55384|  0:05:26s
epoch 103| loss: 106.95017| val_0_rmse: 12.8557 |  0:05:29s
epoch 104| loss: 109.52082| val_0_rmse: 12.54535|  0:05:33s
epoch 105| loss: 110.0011| val_0_rmse: 12.6514 |  0:05:36s
epoch 106| loss: 105.81515| val_0_rmse: 12.1859 |  0:05:39s
epoch 107| loss: 104.54023| val_0_rmse: 12.1641 |  0:05:42s
epoch 108| loss: 106.70093| val_0_rmse: 12.69748|  0:05:45s
epoch 109| loss: 104.71215| val_0_rmse: 12.405  |  0:05:48s
epoch 110| loss: 105.22822| val_0_rmse: 12.70312|  0:05:51s
epoch 111| loss: 103.83339| val_0_rmse: 12.07565|  0:05:55s
epoch 112| loss: 103.2784| val_0_rmse: 12.66021|  0:05:58s
epoch 113| loss: 107.5434| val_0_rmse: 12.21619|  0:06:01s
epoch 114| loss: 101.54488| val_0_rmse: 12.40107|  0:06:04s
epoch 115| loss: 105.91719| val_0_rmse: 12.42373|  0:06:07s
epoch 116| loss: 101.50213| val_0_rmse: 12.5449 |  0:06:10s
epoch 117| loss: 108.9488| val_0_rmse: 12.48092|  0:06:13s
epoch 118| loss: 102.31134| val_0_rmse: 12.3855 |  0:06:17s
epoch 119| loss: 106.31639| val_0_rmse: 12.91849|  0:06:20s
epoch 120| loss: 103.32805| val_0_rmse: 12.51428|  0:06:23s
epoch 121| loss: 106.79813| val_0_rmse: 12.62339|  0:06:26s
epoch 122| loss: 99.82653| val_0_rmse: 12.54714|  0:06:29s
epoch 123| loss: 102.5184| val_0_rmse: 12.45204|  0:06:32s
epoch 124| loss: 102.23516| val_0_rmse: 12.67997|  0:06:35s
epoch 125| loss: 102.16926| val_0_rmse: 12.43787|  0:06:39s
epoch 126| loss: 100.05364| val_0_rmse: 12.39545|  0:06:42s
epoch 127| loss: 102.10625| val_0_rmse: 12.10933|  0:06:45s
epoch 128| loss: 101.5032| val_0_rmse: 12.8473 |  0:06:48s
epoch 129| loss: 96.39985| val_0_rmse: 12.47347|  0:06:51s
epoch 130| loss: 102.89888| val_0_rmse: 12.65151|  0:06:54s
epoch 131| loss: 100.01636| val_0_rmse: 11.95734|  0:06:57s
epoch 132| loss: 100.23275| val_0_rmse: 12.43026|  0:07:01s
epoch 133| loss: 96.57082| val_0_rmse: 12.50773|  0:07:04s
epoch 134| loss: 99.50059| val_0_rmse: 12.33864|  0:07:07s
epoch 135| loss: 97.21613| val_0_rmse: 12.23738|  0:07:10s
epoch 136| loss: 98.45222| val_0_rmse: 12.24531|  0:07:13s
epoch 137| loss: 97.33687| val_0_rmse: 12.44382|  0:07:16s
epoch 138| loss: 101.14568| val_0_rmse: 12.17609|  0:07:19s
epoch 139| loss: 98.91359| val_0_rmse: 12.10347|  0:07:23s
epoch 140| loss: 99.95692| val_0_rmse: 12.40394|  0:07:26s
epoch 141| loss: 98.1299 | val_0_rmse: 12.14269|  0:07:29s
epoch 142| loss: 100.14614| val_0_rmse: 12.51431|  0:07:32s
epoch 143| loss: 94.54174| val_0_rmse: 12.66673|  0:07:35s
epoch 144| loss: 97.3676 | val_0_rmse: 12.84736|  0:07:38s
epoch 145| loss: 97.53327| val_0_rmse: 12.25215|  0:07:41s
epoch 146| loss: 94.94801| val_0_rmse: 12.53112|  0:07:45s
epoch 147| loss: 95.93647| val_0_rmse: 12.2561 |  0:07:48s
epoch 148| loss: 95.40052| val_0_rmse: 12.08686|  0:07:51s
epoch 149| loss: 94.20015| val_0_rmse: 12.25242|  0:07:54s
epoch 150| loss: 101.40831| val_0_rmse: 12.05328|  0:07:57s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 151| loss: 96.3964 | val_0_rmse: 12.65901|  0:08:00s
Early stopping occurred at epoch 151 with best_epoch = 131 and best_val_0_rmse = 11.95734
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 21:01:52,881] Trial 15 finished with value: 11.957342913770445 and parameters: {'lr': 0.006304520545747167, 'n_steps': 5, 'gamma': 1.3511250085683413, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.023277469151463503, 'weight_decay': 0.00019356740300190996, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 11 with value: 11.692167376435432.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8572.23698| val_0_rmse: 89.48495|  0:00:03s
epoch 1  | loss: 8473.81396| val_0_rmse: 95.70892|  0:00:06s
epoch 2  | loss: 7668.62813| val_0_rmse: 81.24188|  0:00:09s
epoch 3  | loss: 5225.05986| val_0_rmse: 73.12028|  0:00:12s
epoch 4  | loss: 2405.7291| val_0_rmse: 40.95169|  0:00:15s
epoch 5  | loss: 1140.32346| val_0_rmse: 32.2788 |  0:00:19s
epoch 6  | loss: 1007.63509| val_0_rmse: 32.22446|  0:00:22s
epoch 7  | loss: 991.65287| val_0_rmse: 31.78505|  0:00:25s
epoch 8  | loss: 940.97061| val_0_rmse: 32.6276 |  0:00:28s
epoch 9  | loss: 883.94321| val_0_rmse: 31.89197|  0:00:31s
epoch 10 | loss: 825.57876| val_0_rmse: 31.12753|  0:00:34s
epoch 11 | loss: 781.54022| val_0_rmse: 28.951  |  0:00:38s
epoch 12 | loss: 724.53764| val_0_rmse: 26.78186|  0:00:41s
epoch 13 | loss: 672.83906| val_0_rmse: 25.64933|  0:00:44s
epoch 14 | loss: 625.31582| val_0_rmse: 25.46571|  0:00:47s
epoch 15 | loss: 564.75475| val_0_rmse: 23.14897|  0:00:50s
epoch 16 | loss: 519.07483| val_0_rmse: 22.25809|  0:00:54s
epoch 17 | loss: 490.96757| val_0_rmse: 22.62414|  0:00:57s
epoch 18 | loss: 464.15287| val_0_rmse: 22.52997|  0:01:00s
epoch 19 | loss: 442.40695| val_0_rmse: 20.26093|  0:01:03s
epoch 20 | loss: 424.57306| val_0_rmse: 20.58511|  0:01:06s
epoch 21 | loss: 404.87319| val_0_rmse: 20.21251|  0:01:10s
epoch 22 | loss: 383.38896| val_0_rmse: 18.91359|  0:01:13s
epoch 23 | loss: 366.7067| val_0_rmse: 18.77071|  0:01:16s
epoch 24 | loss: 355.0478| val_0_rmse: 18.03479|  0:01:19s
epoch 25 | loss: 337.66144| val_0_rmse: 17.56686|  0:01:22s
epoch 26 | loss: 327.52651| val_0_rmse: 17.26483|  0:01:26s
epoch 27 | loss: 309.3978| val_0_rmse: 16.84827|  0:01:29s
epoch 28 | loss: 305.78604| val_0_rmse: 16.67768|  0:01:32s
epoch 29 | loss: 295.7986| val_0_rmse: 16.30958|  0:01:35s
epoch 30 | loss: 287.34896| val_0_rmse: 16.1965 |  0:01:38s
epoch 31 | loss: 278.10781| val_0_rmse: 16.26555|  0:01:42s
epoch 32 | loss: 278.4087| val_0_rmse: 16.16411|  0:01:45s
epoch 33 | loss: 271.11238| val_0_rmse: 16.11739|  0:01:48s
epoch 34 | loss: 258.42586| val_0_rmse: 15.94959|  0:01:51s
epoch 35 | loss: 260.06105| val_0_rmse: 15.66546|  0:01:54s
epoch 36 | loss: 250.93534| val_0_rmse: 15.81982|  0:01:58s
epoch 37 | loss: 242.05629| val_0_rmse: 15.63717|  0:02:01s
epoch 38 | loss: 235.72679| val_0_rmse: 15.58685|  0:02:04s
epoch 39 | loss: 231.03155| val_0_rmse: 15.3234 |  0:02:07s
epoch 40 | loss: 227.42449| val_0_rmse: 14.71251|  0:02:10s
epoch 41 | loss: 222.89028| val_0_rmse: 14.7409 |  0:02:14s
epoch 42 | loss: 220.82219| val_0_rmse: 14.60332|  0:02:17s
epoch 43 | loss: 220.47265| val_0_rmse: 15.07816|  0:02:20s
epoch 44 | loss: 214.43436| val_0_rmse: 14.73033|  0:02:23s
epoch 45 | loss: 214.6157| val_0_rmse: 14.49135|  0:02:26s
epoch 46 | loss: 210.42692| val_0_rmse: 14.61851|  0:02:30s
epoch 47 | loss: 206.02819| val_0_rmse: 14.74673|  0:02:33s
epoch 48 | loss: 205.8333| val_0_rmse: 14.48425|  0:02:36s
epoch 49 | loss: 202.18256| val_0_rmse: 14.51384|  0:02:39s
epoch 50 | loss: 201.52966| val_0_rmse: 14.14489|  0:02:42s
epoch 51 | loss: 193.39941| val_0_rmse: 14.06114|  0:02:46s
epoch 52 | loss: 198.02617| val_0_rmse: 14.03428|  0:02:49s
epoch 53 | loss: 189.10197| val_0_rmse: 13.81471|  0:02:52s
epoch 54 | loss: 197.56153| val_0_rmse: 14.2118 |  0:02:55s
epoch 55 | loss: 187.47071| val_0_rmse: 13.88784|  0:02:58s
epoch 56 | loss: 185.42445| val_0_rmse: 13.79129|  0:03:01s
epoch 57 | loss: 188.61982| val_0_rmse: 13.87105|  0:03:05s
epoch 58 | loss: 190.11824| val_0_rmse: 13.88268|  0:03:08s
epoch 59 | loss: 183.05262| val_0_rmse: 13.8527 |  0:03:11s
epoch 60 | loss: 183.073 | val_0_rmse: 13.86505|  0:03:14s
epoch 61 | loss: 183.15725| val_0_rmse: 13.49624|  0:03:17s
epoch 62 | loss: 174.5991| val_0_rmse: 13.77094|  0:03:20s
epoch 63 | loss: 176.22963| val_0_rmse: 13.43197|  0:03:24s
epoch 64 | loss: 169.41081| val_0_rmse: 13.366  |  0:03:27s
epoch 65 | loss: 174.14547| val_0_rmse: 13.50705|  0:03:30s
epoch 66 | loss: 165.70563| val_0_rmse: 13.58218|  0:03:33s
epoch 67 | loss: 173.68137| val_0_rmse: 13.28649|  0:03:36s
epoch 68 | loss: 163.86974| val_0_rmse: 13.0386 |  0:03:40s
epoch 69 | loss: 164.86633| val_0_rmse: 13.20376|  0:03:43s
epoch 70 | loss: 165.81507| val_0_rmse: 13.33323|  0:03:46s
epoch 71 | loss: 161.78033| val_0_rmse: 13.21765|  0:03:49s
epoch 72 | loss: 162.62191| val_0_rmse: 13.46269|  0:03:52s
epoch 73 | loss: 156.86479| val_0_rmse: 13.02501|  0:03:56s
epoch 74 | loss: 160.14335| val_0_rmse: 12.82208|  0:03:59s
epoch 75 | loss: 156.69906| val_0_rmse: 13.14841|  0:04:02s
epoch 76 | loss: 153.6056| val_0_rmse: 12.87739|  0:04:05s
epoch 77 | loss: 156.99171| val_0_rmse: 13.08148|  0:04:08s
epoch 78 | loss: 156.10555| val_0_rmse: 12.9422 |  0:04:11s
epoch 79 | loss: 151.3882| val_0_rmse: 13.17762|  0:04:15s
epoch 80 | loss: 155.12077| val_0_rmse: 13.02634|  0:04:18s
epoch 81 | loss: 151.06842| val_0_rmse: 13.03493|  0:04:21s
epoch 82 | loss: 152.96108| val_0_rmse: 12.98513|  0:04:24s
epoch 83 | loss: 148.90158| val_0_rmse: 12.79035|  0:04:27s
epoch 84 | loss: 151.78873| val_0_rmse: 13.27121|  0:04:31s
epoch 85 | loss: 144.91298| val_0_rmse: 12.67197|  0:04:34s
epoch 86 | loss: 147.30523| val_0_rmse: 12.53251|  0:04:37s
epoch 87 | loss: 146.51488| val_0_rmse: 12.64531|  0:04:40s
epoch 88 | loss: 146.44742| val_0_rmse: 12.62658|  0:04:43s
epoch 89 | loss: 144.22738| val_0_rmse: 12.90703|  0:04:46s
epoch 90 | loss: 147.75908| val_0_rmse: 12.4037 |  0:04:49s
epoch 91 | loss: 142.10359| val_0_rmse: 12.69332|  0:04:53s
epoch 92 | loss: 140.98168| val_0_rmse: 12.80144|  0:04:56s
epoch 93 | loss: 141.74748| val_0_rmse: 12.56181|  0:04:59s
epoch 94 | loss: 140.54445| val_0_rmse: 12.39537|  0:05:02s
epoch 95 | loss: 134.94563| val_0_rmse: 12.53224|  0:05:05s
epoch 96 | loss: 138.21497| val_0_rmse: 12.64591|  0:05:08s
epoch 97 | loss: 138.31126| val_0_rmse: 12.79421|  0:05:11s
epoch 98 | loss: 131.80191| val_0_rmse: 12.56833|  0:05:15s
epoch 99 | loss: 136.93752| val_0_rmse: 12.32392|  0:05:18s
epoch 100| loss: 137.95612| val_0_rmse: 12.37016|  0:05:21s
epoch 101| loss: 135.71675| val_0_rmse: 12.23255|  0:05:24s
epoch 102| loss: 131.18531| val_0_rmse: 12.60076|  0:05:27s
epoch 103| loss: 138.43265| val_0_rmse: 12.67168|  0:05:30s
epoch 104| loss: 136.40858| val_0_rmse: 13.01088|  0:05:34s
epoch 105| loss: 136.8115| val_0_rmse: 12.4503 |  0:05:37s
epoch 106| loss: 129.77895| val_0_rmse: 12.55104|  0:05:40s
epoch 107| loss: 130.13433| val_0_rmse: 12.44249|  0:05:43s
epoch 108| loss: 128.06992| val_0_rmse: 11.98965|  0:05:46s
epoch 109| loss: 132.74019| val_0_rmse: 12.16459|  0:05:49s
epoch 110| loss: 129.66104| val_0_rmse: 12.40273|  0:05:52s
epoch 111| loss: 127.55812| val_0_rmse: 12.38511|  0:05:56s
epoch 112| loss: 131.8071| val_0_rmse: 12.46146|  0:05:59s
epoch 113| loss: 126.14221| val_0_rmse: 12.50601|  0:06:02s
epoch 114| loss: 125.9334| val_0_rmse: 12.45306|  0:06:05s
epoch 115| loss: 128.87905| val_0_rmse: 12.48256|  0:06:08s
epoch 116| loss: 130.40678| val_0_rmse: 12.58663|  0:06:11s
epoch 117| loss: 128.2658| val_0_rmse: 12.7745 |  0:06:15s
epoch 118| loss: 123.61765| val_0_rmse: 12.50521|  0:06:18s
epoch 119| loss: 127.9312| val_0_rmse: 12.78411|  0:06:21s
epoch 120| loss: 122.60634| val_0_rmse: 12.50345|  0:06:24s
epoch 121| loss: 126.52077| val_0_rmse: 12.43503|  0:06:27s
epoch 122| loss: 121.98589| val_0_rmse: 12.31842|  0:06:30s
epoch 123| loss: 124.54276| val_0_rmse: 12.15828|  0:06:34s
epoch 124| loss: 125.94599| val_0_rmse: 12.23135|  0:06:37s
epoch 125| loss: 122.9643| val_0_rmse: 12.09983|  0:06:40s
epoch 126| loss: 120.56528| val_0_rmse: 12.26143|  0:06:43s
epoch 127| loss: 125.83472| val_0_rmse: 12.03646|  0:06:46s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 128| loss: 122.61896| val_0_rmse: 12.24779|  0:06:49s
Early stopping occurred at epoch 128 with best_epoch = 108 and best_val_0_rmse = 11.98965
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 21:08:49,057] Trial 16 finished with value: 11.989647124380669 and parameters: {'lr': 0.0044441321560612064, 'n_steps': 5, 'gamma': 1.3385381666751726, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.026433560388549167, 'weight_decay': 0.0002209628322385724, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 11 with value: 11.692167376435432.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8584.42475| val_0_rmse: 93.65528|  0:00:03s
epoch 1  | loss: 8024.5006| val_0_rmse: 158.51474|  0:00:06s
epoch 2  | loss: 5317.87319| val_0_rmse: 57.21935|  0:00:09s
epoch 3  | loss: 1479.08108| val_0_rmse: 43.68318|  0:00:12s
epoch 4  | loss: 1026.97628| val_0_rmse: 35.3755 |  0:00:15s
epoch 5  | loss: 987.15386| val_0_rmse: 36.07145|  0:00:19s
epoch 6  | loss: 965.79982| val_0_rmse: 38.22804|  0:00:22s
epoch 7  | loss: 937.90427| val_0_rmse: 33.29403|  0:00:25s
epoch 8  | loss: 900.2203| val_0_rmse: 30.72818|  0:00:28s
epoch 9  | loss: 839.74827| val_0_rmse: 29.12698|  0:00:31s
epoch 10 | loss: 699.57263| val_0_rmse: 26.36282|  0:00:35s
epoch 11 | loss: 601.76581| val_0_rmse: 24.19728|  0:00:38s
epoch 12 | loss: 513.13038| val_0_rmse: 22.69634|  0:00:41s
epoch 13 | loss: 473.42602| val_0_rmse: 21.77658|  0:00:44s
epoch 14 | loss: 435.30773| val_0_rmse: 20.85443|  0:00:47s
epoch 15 | loss: 405.8996| val_0_rmse: 19.94783|  0:00:51s
epoch 16 | loss: 379.02922| val_0_rmse: 19.20751|  0:00:54s
epoch 17 | loss: 353.10352| val_0_rmse: 18.7134 |  0:00:57s
epoch 18 | loss: 336.5845| val_0_rmse: 18.05223|  0:01:00s
epoch 19 | loss: 311.33641| val_0_rmse: 17.66075|  0:01:03s
epoch 20 | loss: 307.20663| val_0_rmse: 17.71581|  0:01:07s
epoch 21 | loss: 294.14255| val_0_rmse: 17.12542|  0:01:10s
epoch 22 | loss: 278.10206| val_0_rmse: 16.66935|  0:01:13s
epoch 23 | loss: 270.15615| val_0_rmse: 16.70134|  0:01:16s
epoch 24 | loss: 262.63254| val_0_rmse: 16.13883|  0:01:19s
epoch 25 | loss: 250.86776| val_0_rmse: 16.20023|  0:01:22s
epoch 26 | loss: 243.92566| val_0_rmse: 15.64111|  0:01:26s
epoch 27 | loss: 234.55773| val_0_rmse: 15.27034|  0:01:29s
epoch 28 | loss: 230.7401| val_0_rmse: 15.26266|  0:01:32s
epoch 29 | loss: 225.90901| val_0_rmse: 14.84401|  0:01:35s
epoch 30 | loss: 212.16176| val_0_rmse: 15.13039|  0:01:38s
epoch 31 | loss: 215.72079| val_0_rmse: 14.77481|  0:01:42s
epoch 32 | loss: 212.21229| val_0_rmse: 15.12837|  0:01:45s
epoch 33 | loss: 211.54538| val_0_rmse: 14.65392|  0:01:48s
epoch 34 | loss: 205.65827| val_0_rmse: 14.70716|  0:01:51s
epoch 35 | loss: 200.88128| val_0_rmse: 14.65313|  0:01:54s
epoch 36 | loss: 194.96817| val_0_rmse: 14.61126|  0:01:58s
epoch 37 | loss: 185.19446| val_0_rmse: 14.26042|  0:02:01s
epoch 38 | loss: 188.26042| val_0_rmse: 14.48899|  0:02:04s
epoch 39 | loss: 180.36874| val_0_rmse: 14.19253|  0:02:07s
epoch 40 | loss: 179.88093| val_0_rmse: 14.07217|  0:02:10s
epoch 41 | loss: 179.00714| val_0_rmse: 13.82205|  0:02:14s
epoch 42 | loss: 177.32847| val_0_rmse: 13.82635|  0:02:17s
epoch 43 | loss: 176.62462| val_0_rmse: 13.47361|  0:02:20s
epoch 44 | loss: 174.06941| val_0_rmse: 13.6666 |  0:02:23s
epoch 45 | loss: 176.00507| val_0_rmse: 13.54142|  0:02:26s
epoch 46 | loss: 168.95809| val_0_rmse: 13.75329|  0:02:29s
epoch 47 | loss: 166.51846| val_0_rmse: 13.53241|  0:02:33s
epoch 48 | loss: 167.00318| val_0_rmse: 13.25983|  0:02:36s
epoch 49 | loss: 161.63882| val_0_rmse: 13.38508|  0:02:39s
epoch 50 | loss: 160.5837| val_0_rmse: 13.6839 |  0:02:42s
epoch 51 | loss: 156.67471| val_0_rmse: 13.03905|  0:02:45s
epoch 52 | loss: 155.60476| val_0_rmse: 13.05696|  0:02:49s
epoch 53 | loss: 156.5801| val_0_rmse: 13.30376|  0:02:52s
epoch 54 | loss: 158.94705| val_0_rmse: 13.28157|  0:02:55s
epoch 55 | loss: 157.26184| val_0_rmse: 13.05646|  0:02:58s
epoch 56 | loss: 156.32109| val_0_rmse: 13.11071|  0:03:01s
epoch 57 | loss: 156.44307| val_0_rmse: 13.02818|  0:03:04s
epoch 58 | loss: 150.6079| val_0_rmse: 13.05585|  0:03:08s
epoch 59 | loss: 143.41462| val_0_rmse: 12.82086|  0:03:11s
epoch 60 | loss: 148.22796| val_0_rmse: 12.66577|  0:03:14s
epoch 61 | loss: 147.85965| val_0_rmse: 13.11499|  0:03:17s
epoch 62 | loss: 142.09171| val_0_rmse: 13.00083|  0:03:20s
epoch 63 | loss: 143.18972| val_0_rmse: 12.84143|  0:03:24s
epoch 64 | loss: 137.22361| val_0_rmse: 12.71317|  0:03:27s
epoch 65 | loss: 135.82357| val_0_rmse: 12.93241|  0:03:30s
epoch 66 | loss: 132.98253| val_0_rmse: 12.47798|  0:03:33s
epoch 67 | loss: 137.43003| val_0_rmse: 12.63643|  0:03:37s
epoch 68 | loss: 129.44768| val_0_rmse: 12.74784|  0:03:40s
epoch 69 | loss: 131.88087| val_0_rmse: 12.93048|  0:03:43s
epoch 70 | loss: 135.63395| val_0_rmse: 12.86787|  0:03:46s
epoch 71 | loss: 129.84965| val_0_rmse: 12.69508|  0:03:49s
epoch 72 | loss: 132.36596| val_0_rmse: 12.59763|  0:03:53s
epoch 73 | loss: 131.85682| val_0_rmse: 12.78264|  0:03:56s
epoch 74 | loss: 127.13253| val_0_rmse: 12.48246|  0:03:59s
epoch 75 | loss: 128.44306| val_0_rmse: 12.57832|  0:04:02s
epoch 76 | loss: 126.9437| val_0_rmse: 12.60101|  0:04:05s
epoch 77 | loss: 124.68004| val_0_rmse: 12.6971 |  0:04:08s
epoch 78 | loss: 122.26522| val_0_rmse: 12.45373|  0:04:12s
epoch 79 | loss: 125.64435| val_0_rmse: 12.7433 |  0:04:15s
epoch 80 | loss: 124.7023| val_0_rmse: 12.10606|  0:04:18s
epoch 81 | loss: 126.02168| val_0_rmse: 12.80752|  0:04:21s
epoch 82 | loss: 120.7407| val_0_rmse: 12.47253|  0:04:24s
epoch 83 | loss: 121.53873| val_0_rmse: 12.24716|  0:04:28s
epoch 84 | loss: 122.43129| val_0_rmse: 12.45514|  0:04:31s
epoch 85 | loss: 123.74745| val_0_rmse: 12.62903|  0:04:34s
epoch 86 | loss: 120.12731| val_0_rmse: 12.44985|  0:04:37s
epoch 87 | loss: 126.15237| val_0_rmse: 12.67511|  0:04:40s
epoch 88 | loss: 121.61433| val_0_rmse: 12.43063|  0:04:44s
epoch 89 | loss: 123.17128| val_0_rmse: 12.61977|  0:04:47s
epoch 90 | loss: 120.19186| val_0_rmse: 12.54851|  0:04:50s
epoch 91 | loss: 114.73821| val_0_rmse: 12.59308|  0:04:53s
epoch 92 | loss: 123.93274| val_0_rmse: 12.35289|  0:04:56s
epoch 93 | loss: 122.731 | val_0_rmse: 11.98437|  0:05:00s
epoch 94 | loss: 121.02313| val_0_rmse: 12.48069|  0:05:03s
epoch 95 | loss: 117.49138| val_0_rmse: 12.56669|  0:05:06s
epoch 96 | loss: 117.27749| val_0_rmse: 12.07812|  0:05:09s
epoch 97 | loss: 114.95687| val_0_rmse: 12.47325|  0:05:12s
epoch 98 | loss: 122.73037| val_0_rmse: 12.70226|  0:05:15s
epoch 99 | loss: 117.92459| val_0_rmse: 12.38235|  0:05:19s
epoch 100| loss: 114.53702| val_0_rmse: 12.08985|  0:05:22s
epoch 101| loss: 116.60378| val_0_rmse: 12.23617|  0:05:25s
epoch 102| loss: 118.06847| val_0_rmse: 12.22786|  0:05:28s
epoch 103| loss: 118.27227| val_0_rmse: 12.6016 |  0:05:31s
epoch 104| loss: 120.17556| val_0_rmse: 12.5092 |  0:05:34s
epoch 105| loss: 117.00924| val_0_rmse: 12.64731|  0:05:38s
epoch 106| loss: 121.01199| val_0_rmse: 12.41002|  0:05:41s
epoch 107| loss: 118.33664| val_0_rmse: 12.3348 |  0:05:44s
epoch 108| loss: 114.2365| val_0_rmse: 12.25366|  0:05:47s
epoch 109| loss: 113.65421| val_0_rmse: 12.52323|  0:05:50s
epoch 110| loss: 114.43175| val_0_rmse: 12.50328|  0:05:53s
epoch 111| loss: 113.1008| val_0_rmse: 12.4739 |  0:05:57s
epoch 112| loss: 114.08674| val_0_rmse: 12.52405|  0:06:00s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 113| loss: 113.8496| val_0_rmse: 12.2117 |  0:06:03s
Early stopping occurred at epoch 113 with best_epoch = 93 and best_val_0_rmse = 11.98437
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 21:14:58,794] Trial 17 finished with value: 11.984373585639792 and parameters: {'lr': 0.006531610611336191, 'n_steps': 5, 'gamma': 1.3252529006269647, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.021617306881161216, 'weight_decay': 0.0002925660128060426, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 11 with value: 11.692167376435432.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8795.45991| val_0_rmse: 93.3778 |  0:00:02s
epoch 1  | loss: 7958.99861| val_0_rmse: 62.48109|  0:00:05s
epoch 2  | loss: 5468.04386| val_0_rmse: 63.0187 |  0:00:07s
epoch 3  | loss: 1944.38273| val_0_rmse: 69.7945 |  0:00:10s
epoch 4  | loss: 1009.31281| val_0_rmse: 34.01244|  0:00:12s
epoch 5  | loss: 930.08981| val_0_rmse: 35.41354|  0:00:15s
epoch 6  | loss: 782.0698| val_0_rmse: 35.38809|  0:00:17s
epoch 7  | loss: 633.96177| val_0_rmse: 34.63317|  0:00:20s
epoch 8  | loss: 537.15413| val_0_rmse: 29.117  |  0:00:22s
epoch 9  | loss: 455.59801| val_0_rmse: 24.40171|  0:00:25s
epoch 10 | loss: 417.35269| val_0_rmse: 22.73789|  0:00:27s
epoch 11 | loss: 384.06032| val_0_rmse: 20.10283|  0:00:30s
epoch 12 | loss: 354.48524| val_0_rmse: 18.88059|  0:00:32s
epoch 13 | loss: 331.69097| val_0_rmse: 17.84593|  0:00:35s
epoch 14 | loss: 311.56101| val_0_rmse: 17.34277|  0:00:37s
epoch 15 | loss: 307.87312| val_0_rmse: 17.17783|  0:00:40s
epoch 16 | loss: 283.2256| val_0_rmse: 16.60062|  0:00:42s
epoch 17 | loss: 281.41303| val_0_rmse: 16.32368|  0:00:45s
epoch 18 | loss: 270.71525| val_0_rmse: 15.81731|  0:00:48s
epoch 19 | loss: 258.57164| val_0_rmse: 15.71211|  0:00:50s
epoch 20 | loss: 252.40167| val_0_rmse: 15.48754|  0:00:53s
epoch 21 | loss: 245.25833| val_0_rmse: 15.08802|  0:00:55s
epoch 22 | loss: 244.9977| val_0_rmse: 15.31882|  0:00:58s
epoch 23 | loss: 229.91811| val_0_rmse: 15.0713 |  0:01:00s
epoch 24 | loss: 230.44407| val_0_rmse: 15.03931|  0:01:03s
epoch 25 | loss: 220.85826| val_0_rmse: 15.18715|  0:01:05s
epoch 26 | loss: 220.47992| val_0_rmse: 14.70077|  0:01:08s
epoch 27 | loss: 213.99879| val_0_rmse: 14.68976|  0:01:10s
epoch 28 | loss: 206.27118| val_0_rmse: 14.39731|  0:01:13s
epoch 29 | loss: 207.92067| val_0_rmse: 14.34276|  0:01:15s
epoch 30 | loss: 196.691 | val_0_rmse: 14.42447|  0:01:18s
epoch 31 | loss: 198.19183| val_0_rmse: 14.66283|  0:01:20s
epoch 32 | loss: 196.78625| val_0_rmse: 14.16196|  0:01:23s
epoch 33 | loss: 197.19589| val_0_rmse: 14.38909|  0:01:26s
epoch 34 | loss: 188.90413| val_0_rmse: 14.15152|  0:01:28s
epoch 35 | loss: 190.31381| val_0_rmse: 13.96022|  0:01:31s
epoch 36 | loss: 185.74489| val_0_rmse: 13.88011|  0:01:33s
epoch 37 | loss: 184.37262| val_0_rmse: 13.6952 |  0:01:36s
epoch 38 | loss: 180.81975| val_0_rmse: 14.26948|  0:01:38s
epoch 39 | loss: 186.69489| val_0_rmse: 13.99839|  0:01:41s
epoch 40 | loss: 183.66252| val_0_rmse: 14.06825|  0:01:43s
epoch 41 | loss: 179.41735| val_0_rmse: 13.93524|  0:01:46s
epoch 42 | loss: 179.99429| val_0_rmse: 13.41367|  0:01:48s
epoch 43 | loss: 177.31626| val_0_rmse: 14.19677|  0:01:51s
epoch 44 | loss: 172.97151| val_0_rmse: 13.3715 |  0:01:53s
epoch 45 | loss: 175.63184| val_0_rmse: 14.0744 |  0:01:56s
epoch 46 | loss: 169.78519| val_0_rmse: 13.50435|  0:01:58s
epoch 47 | loss: 173.3178| val_0_rmse: 13.43794|  0:02:01s
epoch 48 | loss: 171.418 | val_0_rmse: 13.53766|  0:02:03s
epoch 49 | loss: 173.6603| val_0_rmse: 13.81473|  0:02:06s
epoch 50 | loss: 171.56638| val_0_rmse: 13.61564|  0:02:08s
epoch 51 | loss: 171.28504| val_0_rmse: 13.43768|  0:02:11s
epoch 52 | loss: 175.08066| val_0_rmse: 13.7354 |  0:02:13s
epoch 53 | loss: 172.989 | val_0_rmse: 13.2982 |  0:02:16s
epoch 54 | loss: 166.80708| val_0_rmse: 14.07457|  0:02:18s
epoch 55 | loss: 169.89382| val_0_rmse: 13.20684|  0:02:21s
epoch 56 | loss: 162.14655| val_0_rmse: 13.29731|  0:02:23s
epoch 57 | loss: 166.22764| val_0_rmse: 13.70361|  0:02:26s
epoch 58 | loss: 158.46303| val_0_rmse: 13.17179|  0:02:28s
epoch 59 | loss: 160.53036| val_0_rmse: 12.81809|  0:02:31s
epoch 60 | loss: 157.70339| val_0_rmse: 13.28775|  0:02:33s
epoch 61 | loss: 158.60267| val_0_rmse: 13.93001|  0:02:36s
epoch 62 | loss: 169.77081| val_0_rmse: 13.26201|  0:02:38s
epoch 63 | loss: 161.26027| val_0_rmse: 13.54014|  0:02:41s
epoch 64 | loss: 160.21175| val_0_rmse: 12.98334|  0:02:44s
epoch 65 | loss: 161.7913| val_0_rmse: 13.3663 |  0:02:46s
epoch 66 | loss: 160.05571| val_0_rmse: 12.82652|  0:02:49s
epoch 67 | loss: 151.03705| val_0_rmse: 13.59004|  0:02:51s
epoch 68 | loss: 157.89063| val_0_rmse: 13.15759|  0:02:54s
epoch 69 | loss: 157.38576| val_0_rmse: 12.50435|  0:02:56s
epoch 70 | loss: 155.03348| val_0_rmse: 13.36527|  0:02:59s
epoch 71 | loss: 156.15645| val_0_rmse: 13.04761|  0:03:01s
epoch 72 | loss: 161.04486| val_0_rmse: 12.92216|  0:03:04s
epoch 73 | loss: 153.79491| val_0_rmse: 13.13392|  0:03:06s
epoch 74 | loss: 157.65852| val_0_rmse: 13.04707|  0:03:09s
epoch 75 | loss: 153.14949| val_0_rmse: 13.70778|  0:03:11s
epoch 76 | loss: 152.97527| val_0_rmse: 12.8715 |  0:03:14s
epoch 77 | loss: 148.73991| val_0_rmse: 12.67586|  0:03:16s
epoch 78 | loss: 146.3086| val_0_rmse: 12.83127|  0:03:19s
epoch 79 | loss: 151.7211| val_0_rmse: 12.79562|  0:03:21s
epoch 80 | loss: 146.79749| val_0_rmse: 12.61102|  0:03:24s
epoch 81 | loss: 146.9656| val_0_rmse: 12.7657 |  0:03:26s
epoch 82 | loss: 152.55537| val_0_rmse: 12.61542|  0:03:29s
epoch 83 | loss: 147.75347| val_0_rmse: 12.53261|  0:03:31s
epoch 84 | loss: 150.65595| val_0_rmse: 13.09136|  0:03:34s
epoch 85 | loss: 150.76808| val_0_rmse: 12.72281|  0:03:36s
epoch 86 | loss: 152.43625| val_0_rmse: 12.90043|  0:03:39s
epoch 87 | loss: 146.7902| val_0_rmse: 12.71138|  0:03:41s
epoch 88 | loss: 147.55612| val_0_rmse: 12.75525|  0:03:44s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 89 | loss: 146.22512| val_0_rmse: 12.69711|  0:03:47s
Early stopping occurred at epoch 89 with best_epoch = 69 and best_val_0_rmse = 12.50435
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 21:18:50,976] Trial 18 finished with value: 12.504354714260815 and parameters: {'lr': 0.007044609311410059, 'n_steps': 4, 'gamma': 1.3144856578565793, 'n_independent': 1, 'n_shared': 3, 'momentum': 0.02951306626522332, 'weight_decay': 0.0007329182417757419, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 11 with value: 11.692167376435432.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8618.9168| val_0_rmse: 94.59824|  0:00:03s
epoch 1  | loss: 8655.0404| val_0_rmse: 94.1752 |  0:00:07s
epoch 2  | loss: 8640.35667| val_0_rmse: 94.05268|  0:00:10s
epoch 3  | loss: 8475.16022| val_0_rmse: 92.2238 |  0:00:14s
epoch 4  | loss: 7726.02594| val_0_rmse: 83.00046|  0:00:17s
epoch 5  | loss: 5708.9382| val_0_rmse: 62.38442|  0:00:21s
epoch 6  | loss: 2954.91685| val_0_rmse: 55.57893|  0:00:24s
epoch 7  | loss: 1405.40755| val_0_rmse: 36.56485|  0:00:28s
epoch 8  | loss: 1149.41167| val_0_rmse: 33.90384|  0:00:31s
epoch 9  | loss: 1070.65846| val_0_rmse: 33.09679|  0:00:35s
epoch 10 | loss: 1029.43929| val_0_rmse: 32.19976|  0:00:39s
epoch 11 | loss: 1000.79316| val_0_rmse: 32.12849|  0:00:42s
epoch 12 | loss: 971.28188| val_0_rmse: 31.06872|  0:00:46s
epoch 13 | loss: 945.11492| val_0_rmse: 30.61493|  0:00:49s
epoch 14 | loss: 928.9661| val_0_rmse: 30.34892|  0:00:53s
epoch 15 | loss: 918.23777| val_0_rmse: 29.81154|  0:00:56s
epoch 16 | loss: 902.66587| val_0_rmse: 30.08073|  0:01:00s
epoch 17 | loss: 899.77942| val_0_rmse: 29.9291 |  0:01:04s
epoch 18 | loss: 891.39482| val_0_rmse: 30.32139|  0:01:07s
epoch 19 | loss: 884.6241| val_0_rmse: 29.64875|  0:01:11s
epoch 20 | loss: 848.64225| val_0_rmse: 29.93706|  0:01:14s
epoch 21 | loss: 834.87226| val_0_rmse: 29.26258|  0:01:18s
epoch 22 | loss: 824.55725| val_0_rmse: 29.47941|  0:01:21s
epoch 23 | loss: 806.41779| val_0_rmse: 28.73486|  0:01:25s
epoch 24 | loss: 791.69511| val_0_rmse: 28.80467|  0:01:28s
epoch 25 | loss: 756.90466| val_0_rmse: 29.02861|  0:01:32s
epoch 26 | loss: 719.27614| val_0_rmse: 26.75495|  0:01:35s
epoch 27 | loss: 660.72982| val_0_rmse: 24.81241|  0:01:39s
epoch 28 | loss: 612.23335| val_0_rmse: 24.11039|  0:01:42s
epoch 29 | loss: 578.77008| val_0_rmse: 23.7754 |  0:01:46s
epoch 30 | loss: 547.96473| val_0_rmse: 23.3105 |  0:01:50s
epoch 31 | loss: 524.40915| val_0_rmse: 22.77693|  0:01:53s
epoch 32 | loss: 509.06179| val_0_rmse: 21.86311|  0:01:57s
epoch 33 | loss: 483.25545| val_0_rmse: 21.58778|  0:02:00s
epoch 34 | loss: 466.66015| val_0_rmse: 20.90676|  0:02:04s
epoch 35 | loss: 450.69848| val_0_rmse: 20.83711|  0:02:07s
epoch 36 | loss: 445.76943| val_0_rmse: 20.32379|  0:02:11s
epoch 37 | loss: 428.56941| val_0_rmse: 20.46523|  0:02:14s
epoch 38 | loss: 418.52029| val_0_rmse: 20.02058|  0:02:18s
epoch 39 | loss: 409.35626| val_0_rmse: 19.55642|  0:02:22s
epoch 40 | loss: 396.71787| val_0_rmse: 19.55099|  0:02:25s
epoch 41 | loss: 386.23298| val_0_rmse: 19.19057|  0:02:29s
epoch 42 | loss: 375.61333| val_0_rmse: 18.74044|  0:02:32s
epoch 43 | loss: 362.75937| val_0_rmse: 18.48735|  0:02:36s
epoch 44 | loss: 353.1204| val_0_rmse: 18.33006|  0:02:40s
epoch 45 | loss: 344.04628| val_0_rmse: 17.96263|  0:02:43s
epoch 46 | loss: 339.41375| val_0_rmse: 17.91364|  0:02:47s
epoch 47 | loss: 327.45531| val_0_rmse: 17.64339|  0:02:50s
epoch 48 | loss: 316.66423| val_0_rmse: 17.35371|  0:02:54s
epoch 49 | loss: 306.99226| val_0_rmse: 17.1497 |  0:02:57s
epoch 50 | loss: 303.60912| val_0_rmse: 16.89981|  0:03:01s
epoch 51 | loss: 291.29296| val_0_rmse: 16.88961|  0:03:04s
epoch 52 | loss: 290.89966| val_0_rmse: 16.55319|  0:03:08s
epoch 53 | loss: 280.21159| val_0_rmse: 16.50484|  0:03:12s
epoch 54 | loss: 274.56896| val_0_rmse: 16.09099|  0:03:15s
epoch 55 | loss: 264.68847| val_0_rmse: 16.47764|  0:03:19s
epoch 56 | loss: 258.02074| val_0_rmse: 15.98708|  0:03:22s
epoch 57 | loss: 258.72746| val_0_rmse: 15.85038|  0:03:26s
epoch 58 | loss: 248.86379| val_0_rmse: 16.06013|  0:03:29s
epoch 59 | loss: 242.49003| val_0_rmse: 15.65558|  0:03:33s
epoch 60 | loss: 237.26571| val_0_rmse: 15.25844|  0:03:37s
epoch 61 | loss: 242.13862| val_0_rmse: 15.29614|  0:03:40s
epoch 62 | loss: 230.49281| val_0_rmse: 15.30077|  0:03:44s
epoch 63 | loss: 226.25993| val_0_rmse: 15.18916|  0:03:47s
epoch 64 | loss: 223.33733| val_0_rmse: 14.8664 |  0:03:51s
epoch 65 | loss: 221.03033| val_0_rmse: 15.27031|  0:03:54s
epoch 66 | loss: 213.97146| val_0_rmse: 15.22964|  0:03:58s
epoch 67 | loss: 220.4042| val_0_rmse: 14.79569|  0:04:01s
epoch 68 | loss: 213.92217| val_0_rmse: 14.53562|  0:04:05s
epoch 69 | loss: 208.23761| val_0_rmse: 14.44104|  0:04:09s
epoch 70 | loss: 202.3592| val_0_rmse: 14.72704|  0:04:12s
epoch 71 | loss: 207.0685| val_0_rmse: 14.35189|  0:04:16s
epoch 72 | loss: 205.85202| val_0_rmse: 14.21053|  0:04:19s
epoch 73 | loss: 199.44877| val_0_rmse: 14.17239|  0:04:23s
epoch 74 | loss: 195.28244| val_0_rmse: 14.29459|  0:04:26s
epoch 75 | loss: 196.25495| val_0_rmse: 13.81426|  0:04:30s
epoch 76 | loss: 192.33265| val_0_rmse: 14.12573|  0:04:33s
epoch 77 | loss: 194.92943| val_0_rmse: 13.77548|  0:04:37s
epoch 78 | loss: 188.85735| val_0_rmse: 13.62868|  0:04:41s
epoch 79 | loss: 178.58094| val_0_rmse: 13.84002|  0:04:44s
epoch 80 | loss: 182.30633| val_0_rmse: 13.62225|  0:04:48s
epoch 81 | loss: 180.20704| val_0_rmse: 13.71055|  0:04:51s
epoch 82 | loss: 180.78495| val_0_rmse: 13.71972|  0:04:55s
epoch 83 | loss: 178.42611| val_0_rmse: 13.46738|  0:04:58s
epoch 84 | loss: 175.02331| val_0_rmse: 13.69113|  0:05:02s
epoch 85 | loss: 176.61414| val_0_rmse: 13.54752|  0:05:05s
epoch 86 | loss: 173.08566| val_0_rmse: 13.27052|  0:05:09s
epoch 87 | loss: 168.39283| val_0_rmse: 13.12081|  0:05:12s
epoch 88 | loss: 167.80441| val_0_rmse: 13.36661|  0:05:16s
epoch 89 | loss: 169.14559| val_0_rmse: 13.11139|  0:05:19s
epoch 90 | loss: 165.58199| val_0_rmse: 13.11531|  0:05:23s
epoch 91 | loss: 165.86601| val_0_rmse: 14.04445|  0:05:26s
epoch 92 | loss: 165.23648| val_0_rmse: 13.14578|  0:05:30s
epoch 93 | loss: 163.94907| val_0_rmse: 13.13684|  0:05:33s
epoch 94 | loss: 160.15969| val_0_rmse: 13.07581|  0:05:37s
epoch 95 | loss: 161.40013| val_0_rmse: 13.50231|  0:05:40s
epoch 96 | loss: 158.4576| val_0_rmse: 12.62982|  0:05:44s
epoch 97 | loss: 162.32994| val_0_rmse: 13.03481|  0:05:47s
epoch 98 | loss: 151.08783| val_0_rmse: 13.33075|  0:05:51s
epoch 99 | loss: 157.19045| val_0_rmse: 12.84289|  0:05:54s
epoch 100| loss: 155.95626| val_0_rmse: 12.89258|  0:05:58s
epoch 101| loss: 158.07888| val_0_rmse: 12.99724|  0:06:01s
epoch 102| loss: 152.23941| val_0_rmse: 12.68902|  0:06:05s
epoch 103| loss: 153.19747| val_0_rmse: 13.0613 |  0:06:08s
epoch 104| loss: 149.91062| val_0_rmse: 13.07685|  0:06:12s
epoch 105| loss: 149.63291| val_0_rmse: 12.95846|  0:06:15s
epoch 106| loss: 151.87557| val_0_rmse: 12.58583|  0:06:19s
epoch 107| loss: 147.31636| val_0_rmse: 12.63028|  0:06:22s
epoch 108| loss: 147.52828| val_0_rmse: 12.46695|  0:06:26s
epoch 109| loss: 143.21758| val_0_rmse: 12.66345|  0:06:30s
epoch 110| loss: 145.37099| val_0_rmse: 12.89716|  0:06:33s
epoch 111| loss: 144.66502| val_0_rmse: 12.64108|  0:06:37s
epoch 112| loss: 138.53569| val_0_rmse: 12.61575|  0:06:40s
epoch 113| loss: 138.75664| val_0_rmse: 12.57178|  0:06:44s
epoch 114| loss: 142.16842| val_0_rmse: 12.50966|  0:06:47s
epoch 115| loss: 140.46183| val_0_rmse: 12.23187|  0:06:51s
epoch 116| loss: 137.8815| val_0_rmse: 12.23479|  0:06:55s
epoch 117| loss: 133.67115| val_0_rmse: 12.31049|  0:06:58s
epoch 118| loss: 137.56918| val_0_rmse: 12.1859 |  0:07:02s
epoch 119| loss: 136.56551| val_0_rmse: 12.34356|  0:07:05s
epoch 120| loss: 133.26479| val_0_rmse: 12.24287|  0:07:09s
epoch 121| loss: 133.5441| val_0_rmse: 12.3308 |  0:07:12s
epoch 122| loss: 133.75189| val_0_rmse: 12.25913|  0:07:16s
epoch 123| loss: 132.44977| val_0_rmse: 11.9827 |  0:07:19s
epoch 124| loss: 132.36849| val_0_rmse: 11.85782|  0:07:23s
epoch 125| loss: 132.1487| val_0_rmse: 11.87321|  0:07:26s
epoch 126| loss: 132.5171| val_0_rmse: 12.10058|  0:07:30s
epoch 127| loss: 134.06974| val_0_rmse: 12.00155|  0:07:33s
epoch 128| loss: 133.04055| val_0_rmse: 11.84696|  0:07:37s
epoch 129| loss: 132.8634| val_0_rmse: 12.00153|  0:07:40s
epoch 130| loss: 128.63251| val_0_rmse: 11.90891|  0:07:44s
epoch 131| loss: 127.03509| val_0_rmse: 11.97737|  0:07:47s
epoch 132| loss: 124.22681| val_0_rmse: 11.92825|  0:07:51s
epoch 133| loss: 128.19533| val_0_rmse: 11.99729|  0:07:54s
epoch 134| loss: 127.12333| val_0_rmse: 11.84837|  0:07:58s
epoch 135| loss: 123.97109| val_0_rmse: 12.0508 |  0:08:01s
epoch 136| loss: 124.351 | val_0_rmse: 12.33517|  0:08:05s
epoch 137| loss: 120.33784| val_0_rmse: 12.03305|  0:08:08s
epoch 138| loss: 121.48416| val_0_rmse: 11.85397|  0:08:12s
epoch 139| loss: 122.68744| val_0_rmse: 12.17766|  0:08:15s
epoch 140| loss: 120.67258| val_0_rmse: 11.78166|  0:08:19s
epoch 141| loss: 118.51082| val_0_rmse: 11.98211|  0:08:22s
epoch 142| loss: 116.22076| val_0_rmse: 11.93398|  0:08:26s
epoch 143| loss: 118.89032| val_0_rmse: 11.94378|  0:08:29s
epoch 144| loss: 122.62248| val_0_rmse: 11.94284|  0:08:33s
epoch 145| loss: 116.2029| val_0_rmse: 11.74687|  0:08:36s
epoch 146| loss: 117.86226| val_0_rmse: 11.66691|  0:08:40s
epoch 147| loss: 114.46957| val_0_rmse: 11.63378|  0:08:43s
epoch 148| loss: 114.82492| val_0_rmse: 11.83213|  0:08:47s
epoch 149| loss: 116.8807| val_0_rmse: 11.59589|  0:08:51s
epoch 150| loss: 116.81288| val_0_rmse: 11.96269|  0:08:54s
epoch 151| loss: 116.55419| val_0_rmse: 12.04647|  0:08:58s
epoch 152| loss: 116.91617| val_0_rmse: 11.72422|  0:09:01s
epoch 153| loss: 115.08116| val_0_rmse: 12.00234|  0:09:05s
epoch 154| loss: 110.97759| val_0_rmse: 11.66123|  0:09:08s
epoch 155| loss: 112.46219| val_0_rmse: 11.64075|  0:09:12s
epoch 156| loss: 117.00217| val_0_rmse: 11.69113|  0:09:15s
epoch 157| loss: 115.6936| val_0_rmse: 11.9177 |  0:09:19s
epoch 158| loss: 108.81371| val_0_rmse: 11.80511|  0:09:22s
epoch 159| loss: 109.2146| val_0_rmse: 11.90531|  0:09:26s
epoch 160| loss: 111.54162| val_0_rmse: 11.83976|  0:09:29s
epoch 161| loss: 108.49148| val_0_rmse: 11.86392|  0:09:33s
epoch 162| loss: 112.42787| val_0_rmse: 11.61091|  0:09:36s
epoch 163| loss: 110.86319| val_0_rmse: 11.7596 |  0:09:40s
epoch 164| loss: 108.10683| val_0_rmse: 11.56868|  0:09:43s
epoch 165| loss: 111.93224| val_0_rmse: 11.73215|  0:09:47s
epoch 166| loss: 110.2509| val_0_rmse: 11.92048|  0:09:50s
epoch 167| loss: 110.27588| val_0_rmse: 11.83394|  0:09:54s
epoch 168| loss: 108.5361| val_0_rmse: 11.93012|  0:09:57s
epoch 169| loss: 110.25219| val_0_rmse: 11.72037|  0:10:01s
epoch 170| loss: 106.4309| val_0_rmse: 11.6264 |  0:10:04s
epoch 171| loss: 105.44923| val_0_rmse: 11.80347|  0:10:08s
epoch 172| loss: 105.8341| val_0_rmse: 11.73967|  0:10:11s
epoch 173| loss: 106.67424| val_0_rmse: 11.62444|  0:10:15s
epoch 174| loss: 107.43642| val_0_rmse: 11.62958|  0:10:18s
epoch 175| loss: 105.53889| val_0_rmse: 11.62381|  0:10:22s
epoch 176| loss: 104.71107| val_0_rmse: 11.84523|  0:10:26s
epoch 177| loss: 106.18786| val_0_rmse: 11.95533|  0:10:29s
epoch 178| loss: 105.63659| val_0_rmse: 11.94507|  0:10:33s
epoch 179| loss: 102.78493| val_0_rmse: 11.63866|  0:10:36s
epoch 180| loss: 104.13965| val_0_rmse: 11.7132 |  0:10:40s
epoch 181| loss: 104.79922| val_0_rmse: 11.67114|  0:10:43s
epoch 182| loss: 103.14825| val_0_rmse: 11.6975 |  0:10:47s
epoch 183| loss: 101.46633| val_0_rmse: 11.57243|  0:10:50s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 184| loss: 99.59362| val_0_rmse: 11.67124|  0:10:54s
Early stopping occurred at epoch 184 with best_epoch = 164 and best_val_0_rmse = 11.56868
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 21:29:53,009] Trial 19 finished with value: 11.568680108990407 and parameters: {'lr': 0.003143883438518212, 'n_steps': 6, 'gamma': 1.4182982135079922, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.017772103158993746, 'weight_decay': 0.00015400980051966882, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 19 with value: 11.568680108990407.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
Best hyperparameters:  {'lr': 0.003143883438518212, 'n_steps': 6, 'gamma': 1.4182982135079922, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.017772103158993746, 'weight_decay': 0.00015400980051966882, 'batch_size': 512, 'virtual_batch_size': 64}
epoch 0  | loss: 8618.9168| val_0_rmse: 94.59824|  0:00:03s
epoch 1  | loss: 8655.0404| val_0_rmse: 94.1752 |  0:00:07s
epoch 2  | loss: 8640.35667| val_0_rmse: 94.05268|  0:00:10s
epoch 3  | loss: 8475.16022| val_0_rmse: 92.2238 |  0:00:14s
epoch 4  | loss: 7726.02594| val_0_rmse: 83.00046|  0:00:17s
epoch 5  | loss: 5708.9382| val_0_rmse: 62.38442|  0:00:21s
epoch 6  | loss: 2954.91685| val_0_rmse: 55.57893|  0:00:25s
epoch 7  | loss: 1405.40755| val_0_rmse: 36.56485|  0:00:28s
epoch 8  | loss: 1149.41167| val_0_rmse: 33.90384|  0:00:32s
epoch 9  | loss: 1070.65846| val_0_rmse: 33.09679|  0:00:35s
epoch 10 | loss: 1029.43929| val_0_rmse: 32.19976|  0:00:39s
epoch 11 | loss: 1000.79316| val_0_rmse: 32.12849|  0:00:42s
epoch 12 | loss: 971.28188| val_0_rmse: 31.06872|  0:00:46s
epoch 13 | loss: 945.11492| val_0_rmse: 30.61493|  0:00:50s
epoch 14 | loss: 928.9661| val_0_rmse: 30.34892|  0:00:53s
epoch 15 | loss: 918.23777| val_0_rmse: 29.81154|  0:00:57s
epoch 16 | loss: 902.66587| val_0_rmse: 30.08073|  0:01:00s
epoch 17 | loss: 899.77942| val_0_rmse: 29.9291 |  0:01:04s
epoch 18 | loss: 891.39482| val_0_rmse: 30.32139|  0:01:07s
epoch 19 | loss: 884.6241| val_0_rmse: 29.64875|  0:01:11s
epoch 20 | loss: 848.64225| val_0_rmse: 29.93706|  0:01:14s
epoch 21 | loss: 834.87226| val_0_rmse: 29.26258|  0:01:18s
epoch 22 | loss: 824.55725| val_0_rmse: 29.47941|  0:01:22s
epoch 23 | loss: 806.41779| val_0_rmse: 28.73486|  0:01:25s
epoch 24 | loss: 791.69511| val_0_rmse: 28.80467|  0:01:29s
epoch 25 | loss: 756.90466| val_0_rmse: 29.02861|  0:01:32s
epoch 26 | loss: 719.27614| val_0_rmse: 26.75495|  0:01:36s
epoch 27 | loss: 660.72982| val_0_rmse: 24.81241|  0:01:39s
epoch 28 | loss: 612.23335| val_0_rmse: 24.11039|  0:01:43s
epoch 29 | loss: 578.77008| val_0_rmse: 23.7754 |  0:01:46s
epoch 30 | loss: 547.96473| val_0_rmse: 23.3105 |  0:01:50s
epoch 31 | loss: 524.40915| val_0_rmse: 22.77693|  0:01:54s
epoch 32 | loss: 509.06179| val_0_rmse: 21.86311|  0:01:57s
epoch 33 | loss: 483.25545| val_0_rmse: 21.58778|  0:02:01s
epoch 34 | loss: 466.66015| val_0_rmse: 20.90676|  0:02:04s
epoch 35 | loss: 450.69848| val_0_rmse: 20.83711|  0:02:08s
epoch 36 | loss: 445.76943| val_0_rmse: 20.32379|  0:02:11s
epoch 37 | loss: 428.56941| val_0_rmse: 20.46523|  0:02:15s
epoch 38 | loss: 418.52029| val_0_rmse: 20.02058|  0:02:19s
epoch 39 | loss: 409.35626| val_0_rmse: 19.55642|  0:02:22s
epoch 40 | loss: 396.71787| val_0_rmse: 19.55099|  0:02:26s
epoch 41 | loss: 386.23298| val_0_rmse: 19.19057|  0:02:29s
epoch 42 | loss: 375.61333| val_0_rmse: 18.74044|  0:02:33s
epoch 43 | loss: 362.75937| val_0_rmse: 18.48735|  0:02:36s
epoch 44 | loss: 353.1204| val_0_rmse: 18.33006|  0:02:40s
epoch 45 | loss: 344.04628| val_0_rmse: 17.96263|  0:02:43s
epoch 46 | loss: 339.41375| val_0_rmse: 17.91364|  0:02:47s
epoch 47 | loss: 327.45531| val_0_rmse: 17.64339|  0:02:51s
epoch 48 | loss: 316.66423| val_0_rmse: 17.35371|  0:02:54s
epoch 49 | loss: 306.99226| val_0_rmse: 17.1497 |  0:02:58s
epoch 50 | loss: 303.60912| val_0_rmse: 16.89981|  0:03:01s
epoch 51 | loss: 291.29296| val_0_rmse: 16.88961|  0:03:05s
epoch 52 | loss: 290.89966| val_0_rmse: 16.55319|  0:03:08s
epoch 53 | loss: 280.21159| val_0_rmse: 16.50484|  0:03:12s
epoch 54 | loss: 274.56896| val_0_rmse: 16.09099|  0:03:15s
epoch 55 | loss: 264.68847| val_0_rmse: 16.47764|  0:03:19s
epoch 56 | loss: 258.02074| val_0_rmse: 15.98708|  0:03:23s
epoch 57 | loss: 258.72746| val_0_rmse: 15.85038|  0:03:26s
epoch 58 | loss: 248.86379| val_0_rmse: 16.06013|  0:03:30s
epoch 59 | loss: 242.49003| val_0_rmse: 15.65558|  0:03:33s
epoch 60 | loss: 237.26571| val_0_rmse: 15.25844|  0:03:37s
epoch 61 | loss: 242.13862| val_0_rmse: 15.29614|  0:03:40s
epoch 62 | loss: 230.49281| val_0_rmse: 15.30077|  0:03:44s
epoch 63 | loss: 226.25993| val_0_rmse: 15.18916|  0:03:47s
epoch 64 | loss: 223.33733| val_0_rmse: 14.8664 |  0:03:51s
epoch 65 | loss: 221.03033| val_0_rmse: 15.27031|  0:03:54s
epoch 66 | loss: 213.97146| val_0_rmse: 15.22964|  0:03:58s
epoch 67 | loss: 220.4042| val_0_rmse: 14.79569|  0:04:01s
epoch 68 | loss: 213.92217| val_0_rmse: 14.53562|  0:04:05s
epoch 69 | loss: 208.23761| val_0_rmse: 14.44104|  0:04:08s
epoch 70 | loss: 202.3592| val_0_rmse: 14.72704|  0:04:12s
epoch 71 | loss: 207.0685| val_0_rmse: 14.35189|  0:04:15s
epoch 72 | loss: 205.85202| val_0_rmse: 14.21053|  0:04:19s
epoch 73 | loss: 199.44877| val_0_rmse: 14.17239|  0:04:22s
epoch 74 | loss: 195.28244| val_0_rmse: 14.29459|  0:04:26s
epoch 75 | loss: 196.25495| val_0_rmse: 13.81426|  0:04:29s
epoch 76 | loss: 192.33265| val_0_rmse: 14.12573|  0:04:33s
epoch 77 | loss: 194.92943| val_0_rmse: 13.77548|  0:04:36s
epoch 78 | loss: 188.85735| val_0_rmse: 13.62868|  0:04:40s
epoch 79 | loss: 178.58094| val_0_rmse: 13.84002|  0:04:44s
epoch 80 | loss: 182.30633| val_0_rmse: 13.62225|  0:04:47s
epoch 81 | loss: 180.20704| val_0_rmse: 13.71055|  0:04:51s
epoch 82 | loss: 180.78495| val_0_rmse: 13.71972|  0:04:54s
epoch 83 | loss: 178.42611| val_0_rmse: 13.46738|  0:04:58s
epoch 84 | loss: 175.02331| val_0_rmse: 13.69113|  0:05:01s
epoch 85 | loss: 176.61414| val_0_rmse: 13.54752|  0:05:05s
epoch 86 | loss: 173.08566| val_0_rmse: 13.27052|  0:05:08s
epoch 87 | loss: 168.39283| val_0_rmse: 13.12081|  0:05:12s
epoch 88 | loss: 167.80441| val_0_rmse: 13.36661|  0:05:15s
epoch 89 | loss: 169.14559| val_0_rmse: 13.11139|  0:05:19s
epoch 90 | loss: 165.58199| val_0_rmse: 13.11531|  0:05:22s
epoch 91 | loss: 165.86601| val_0_rmse: 14.04445|  0:05:26s
epoch 92 | loss: 165.23648| val_0_rmse: 13.14578|  0:05:29s
epoch 93 | loss: 163.94907| val_0_rmse: 13.13684|  0:05:33s
epoch 94 | loss: 160.15969| val_0_rmse: 13.07581|  0:05:36s
epoch 95 | loss: 161.40013| val_0_rmse: 13.50231|  0:05:40s
epoch 96 | loss: 158.4576| val_0_rmse: 12.62982|  0:05:43s
epoch 97 | loss: 162.32994| val_0_rmse: 13.03481|  0:05:47s
epoch 98 | loss: 151.08783| val_0_rmse: 13.33075|  0:05:50s
epoch 99 | loss: 157.19045| val_0_rmse: 12.84289|  0:05:54s
epoch 100| loss: 155.95626| val_0_rmse: 12.89258|  0:05:57s
epoch 101| loss: 158.07888| val_0_rmse: 12.99724|  0:06:01s
epoch 102| loss: 152.23941| val_0_rmse: 12.68902|  0:06:04s
epoch 103| loss: 153.19747| val_0_rmse: 13.0613 |  0:06:08s
epoch 104| loss: 149.91062| val_0_rmse: 13.07685|  0:06:11s
epoch 105| loss: 149.63291| val_0_rmse: 12.95846|  0:06:15s
epoch 106| loss: 151.87557| val_0_rmse: 12.58583|  0:06:18s
epoch 107| loss: 147.31636| val_0_rmse: 12.63028|  0:06:22s
epoch 108| loss: 147.52828| val_0_rmse: 12.46695|  0:06:26s
epoch 109| loss: 143.21758| val_0_rmse: 12.66345|  0:06:29s
epoch 110| loss: 145.37099| val_0_rmse: 12.89716|  0:06:33s
epoch 111| loss: 144.66502| val_0_rmse: 12.64108|  0:06:36s
epoch 112| loss: 138.53569| val_0_rmse: 12.61575|  0:06:40s
epoch 113| loss: 138.75664| val_0_rmse: 12.57178|  0:06:43s
epoch 114| loss: 142.16842| val_0_rmse: 12.50966|  0:06:47s
epoch 115| loss: 140.46183| val_0_rmse: 12.23187|  0:06:50s
epoch 116| loss: 137.8815| val_0_rmse: 12.23479|  0:06:54s
epoch 117| loss: 133.67115| val_0_rmse: 12.31049|  0:06:57s
epoch 118| loss: 137.56918| val_0_rmse: 12.1859 |  0:07:01s
epoch 119| loss: 136.56551| val_0_rmse: 12.34356|  0:07:04s
epoch 120| loss: 133.26479| val_0_rmse: 12.24287|  0:07:08s
epoch 121| loss: 133.5441| val_0_rmse: 12.3308 |  0:07:11s
epoch 122| loss: 133.75189| val_0_rmse: 12.25913|  0:07:15s
epoch 123| loss: 132.44977| val_0_rmse: 11.9827 |  0:07:18s
epoch 124| loss: 132.36849| val_0_rmse: 11.85782|  0:07:22s
epoch 125| loss: 132.1487| val_0_rmse: 11.87321|  0:07:25s
epoch 126| loss: 132.5171| val_0_rmse: 12.10058|  0:07:29s
epoch 127| loss: 134.06974| val_0_rmse: 12.00155|  0:07:32s
epoch 128| loss: 133.04055| val_0_rmse: 11.84696|  0:07:36s
epoch 129| loss: 132.8634| val_0_rmse: 12.00153|  0:07:39s
epoch 130| loss: 128.63251| val_0_rmse: 11.90891|  0:07:43s
epoch 131| loss: 127.03509| val_0_rmse: 11.97737|  0:07:46s
epoch 132| loss: 124.22681| val_0_rmse: 11.92825|  0:07:50s
epoch 133| loss: 128.19533| val_0_rmse: 11.99729|  0:07:54s
epoch 134| loss: 127.12333| val_0_rmse: 11.84837|  0:07:57s
epoch 135| loss: 123.97109| val_0_rmse: 12.0508 |  0:08:01s
epoch 136| loss: 124.351 | val_0_rmse: 12.33517|  0:08:04s
epoch 137| loss: 120.33784| val_0_rmse: 12.03305|  0:08:08s
epoch 138| loss: 121.48416| val_0_rmse: 11.85397|  0:08:11s
epoch 139| loss: 122.68744| val_0_rmse: 12.17766|  0:08:15s
epoch 140| loss: 120.67258| val_0_rmse: 11.78166|  0:08:18s
epoch 141| loss: 118.51082| val_0_rmse: 11.98211|  0:08:22s
epoch 142| loss: 116.22076| val_0_rmse: 11.93398|  0:08:25s
epoch 143| loss: 118.89032| val_0_rmse: 11.94378|  0:08:29s
epoch 144| loss: 122.62248| val_0_rmse: 11.94284|  0:08:32s
epoch 145| loss: 116.2029| val_0_rmse: 11.74687|  0:08:36s
epoch 146| loss: 117.86226| val_0_rmse: 11.66691|  0:08:39s
epoch 147| loss: 114.46957| val_0_rmse: 11.63378|  0:08:43s
epoch 148| loss: 114.82492| val_0_rmse: 11.83213|  0:08:46s
epoch 149| loss: 116.8807| val_0_rmse: 11.59589|  0:08:50s
epoch 150| loss: 116.81288| val_0_rmse: 11.96269|  0:08:53s
epoch 151| loss: 116.55419| val_0_rmse: 12.04647|  0:08:57s
epoch 152| loss: 116.91617| val_0_rmse: 11.72422|  0:09:00s
epoch 153| loss: 115.08116| val_0_rmse: 12.00234|  0:09:04s
epoch 154| loss: 110.97759| val_0_rmse: 11.66123|  0:09:07s
epoch 155| loss: 112.46219| val_0_rmse: 11.64075|  0:09:11s
epoch 156| loss: 117.00217| val_0_rmse: 11.69113|  0:09:14s
epoch 157| loss: 115.6936| val_0_rmse: 11.9177 |  0:09:18s
epoch 158| loss: 108.81371| val_0_rmse: 11.80511|  0:09:21s
epoch 159| loss: 109.2146| val_0_rmse: 11.90531|  0:09:25s
epoch 160| loss: 111.54162| val_0_rmse: 11.83976|  0:09:28s
epoch 161| loss: 108.49148| val_0_rmse: 11.86392|  0:09:32s
epoch 162| loss: 112.42787| val_0_rmse: 11.61091|  0:09:35s
epoch 163| loss: 110.86319| val_0_rmse: 11.7596 |  0:09:39s
epoch 164| loss: 108.10683| val_0_rmse: 11.56868|  0:09:43s
epoch 165| loss: 111.93224| val_0_rmse: 11.73215|  0:09:46s
epoch 166| loss: 110.2509| val_0_rmse: 11.92048|  0:09:50s
epoch 167| loss: 110.27588| val_0_rmse: 11.83394|  0:09:53s
epoch 168| loss: 108.5361| val_0_rmse: 11.93012|  0:09:57s
epoch 169| loss: 110.25219| val_0_rmse: 11.72037|  0:10:00s
epoch 170| loss: 106.4309| val_0_rmse: 11.6264 |  0:10:04s
epoch 171| loss: 105.44923| val_0_rmse: 11.80347|  0:10:07s
epoch 172| loss: 105.8341| val_0_rmse: 11.73967|  0:10:11s
epoch 173| loss: 106.67424| val_0_rmse: 11.62444|  0:10:14s
epoch 174| loss: 107.43642| val_0_rmse: 11.62958|  0:10:18s
epoch 175| loss: 105.53889| val_0_rmse: 11.62381|  0:10:21s
epoch 176| loss: 104.71107| val_0_rmse: 11.84523|  0:10:25s
epoch 177| loss: 106.18786| val_0_rmse: 11.95533|  0:10:28s
epoch 178| loss: 105.63659| val_0_rmse: 11.94507|  0:10:32s
epoch 179| loss: 102.78493| val_0_rmse: 11.63866|  0:10:35s
epoch 180| loss: 104.13965| val_0_rmse: 11.7132 |  0:10:39s
epoch 181| loss: 104.79922| val_0_rmse: 11.67114|  0:10:42s
epoch 182| loss: 103.14825| val_0_rmse: 11.6975 |  0:10:46s
epoch 183| loss: 101.46633| val_0_rmse: 11.57243|  0:10:49s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 184| loss: 99.59362| val_0_rmse: 11.67124|  0:10:53s
Early stopping occurred at epoch 184 with best_epoch = 164 and best_val_0_rmse = 11.56868
Train MSE: 70.66046426253997
Validation MSE: 133.83435946415028
Test MSE: 954.3543564458708
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8608.38363| val_0_rmse: 94.31319|  0:00:03s
epoch 1  | loss: 8636.01205| val_0_rmse: 94.35652|  0:00:07s
epoch 2  | loss: 8550.4575| val_0_rmse: 93.20021|  0:00:10s
epoch 3  | loss: 8232.20826| val_0_rmse: 89.45079|  0:00:14s
epoch 4  | loss: 7178.64006| val_0_rmse: 81.35283|  0:00:17s
epoch 5  | loss: 5148.88026| val_0_rmse: 61.5144 |  0:00:21s
epoch 6  | loss: 2733.76922| val_0_rmse: 48.46026|  0:00:25s
epoch 7  | loss: 1560.29812| val_0_rmse: 37.32524|  0:00:28s
epoch 8  | loss: 1145.75952| val_0_rmse: 35.45667|  0:00:32s
epoch 9  | loss: 1084.62966| val_0_rmse: 32.62527|  0:00:35s
epoch 10 | loss: 1048.05986| val_0_rmse: 31.99584|  0:00:39s
epoch 11 | loss: 1030.89715| val_0_rmse: 32.26801|  0:00:42s
epoch 12 | loss: 1020.08095| val_0_rmse: 31.88557|  0:00:46s
epoch 13 | loss: 999.84357| val_0_rmse: 33.42883|  0:00:49s
epoch 14 | loss: 993.33743| val_0_rmse: 31.43993|  0:00:53s
epoch 15 | loss: 980.60081| val_0_rmse: 32.14194|  0:00:56s
epoch 16 | loss: 981.71611| val_0_rmse: 30.92672|  0:01:00s
epoch 17 | loss: 950.19642| val_0_rmse: 30.57537|  0:01:04s
epoch 18 | loss: 929.42141| val_0_rmse: 30.49276|  0:01:07s
epoch 19 | loss: 911.57798| val_0_rmse: 30.97486|  0:01:11s
epoch 20 | loss: 897.31613| val_0_rmse: 29.74685|  0:01:14s
epoch 21 | loss: 884.04328| val_0_rmse: 29.96201|  0:01:18s
epoch 22 | loss: 867.9286| val_0_rmse: 29.71434|  0:01:21s
epoch 23 | loss: 852.66587| val_0_rmse: 29.34613|  0:01:25s
epoch 24 | loss: 826.35173| val_0_rmse: 29.23927|  0:01:28s
epoch 25 | loss: 787.81727| val_0_rmse: 28.29648|  0:01:32s
epoch 26 | loss: 739.46219| val_0_rmse: 27.4178 |  0:01:35s
epoch 27 | loss: 694.69038| val_0_rmse: 27.61598|  0:01:39s
epoch 28 | loss: 651.54979| val_0_rmse: 25.72455|  0:01:43s
epoch 29 | loss: 627.87154| val_0_rmse: 25.06385|  0:01:46s
epoch 30 | loss: 606.24799| val_0_rmse: 24.42426|  0:01:50s
epoch 31 | loss: 580.71475| val_0_rmse: 24.04384|  0:01:53s
epoch 32 | loss: 547.72097| val_0_rmse: 23.34085|  0:01:57s
epoch 33 | loss: 525.26271| val_0_rmse: 22.60095|  0:02:00s
epoch 34 | loss: 510.02462| val_0_rmse: 22.1712 |  0:02:04s
epoch 35 | loss: 484.42864| val_0_rmse: 21.54909|  0:02:07s
epoch 36 | loss: 468.51503| val_0_rmse: 21.12996|  0:02:11s
epoch 37 | loss: 448.21724| val_0_rmse: 20.91365|  0:02:14s
epoch 38 | loss: 429.87493| val_0_rmse: 20.3684 |  0:02:18s
epoch 39 | loss: 413.70507| val_0_rmse: 20.04251|  0:02:21s
epoch 40 | loss: 400.20915| val_0_rmse: 19.96083|  0:02:25s
epoch 41 | loss: 388.73864| val_0_rmse: 19.71244|  0:02:28s
epoch 42 | loss: 376.9401| val_0_rmse: 19.36927|  0:02:32s
epoch 43 | loss: 368.78381| val_0_rmse: 18.95325|  0:02:36s
epoch 44 | loss: 357.66802| val_0_rmse: 18.77158|  0:02:39s
epoch 45 | loss: 347.41399| val_0_rmse: 18.56146|  0:02:43s
epoch 46 | loss: 339.4159| val_0_rmse: 18.48305|  0:02:46s
epoch 47 | loss: 331.2915| val_0_rmse: 18.49278|  0:02:50s
epoch 48 | loss: 324.23634| val_0_rmse: 18.03873|  0:02:53s
epoch 49 | loss: 318.45321| val_0_rmse: 17.98192|  0:02:57s
epoch 50 | loss: 315.72088| val_0_rmse: 17.88013|  0:03:00s
epoch 51 | loss: 307.56834| val_0_rmse: 17.68248|  0:03:04s
epoch 52 | loss: 299.53663| val_0_rmse: 17.61457|  0:03:07s
epoch 53 | loss: 290.02823| val_0_rmse: 17.28887|  0:03:11s
epoch 54 | loss: 286.14581| val_0_rmse: 16.96422|  0:03:14s
epoch 55 | loss: 280.35351| val_0_rmse: 16.78917|  0:03:18s
epoch 56 | loss: 269.02348| val_0_rmse: 16.86907|  0:03:21s
epoch 57 | loss: 271.18297| val_0_rmse: 16.22032|  0:03:25s
epoch 58 | loss: 259.19665| val_0_rmse: 16.14064|  0:03:28s
epoch 59 | loss: 254.25816| val_0_rmse: 16.30888|  0:03:32s
epoch 60 | loss: 244.38016| val_0_rmse: 16.16984|  0:03:35s
epoch 61 | loss: 249.02154| val_0_rmse: 15.89271|  0:03:39s
epoch 62 | loss: 243.12358| val_0_rmse: 16.27389|  0:03:42s
epoch 63 | loss: 238.96912| val_0_rmse: 16.37018|  0:03:46s
epoch 64 | loss: 234.61896| val_0_rmse: 15.92033|  0:03:49s
epoch 65 | loss: 233.06601| val_0_rmse: 16.1667 |  0:03:53s
epoch 66 | loss: 224.63229| val_0_rmse: 16.46562|  0:03:56s
epoch 67 | loss: 226.37939| val_0_rmse: 15.59967|  0:04:00s
epoch 68 | loss: 228.2053| val_0_rmse: 15.62584|  0:04:03s
epoch 69 | loss: 218.67537| val_0_rmse: 15.87868|  0:04:07s
epoch 70 | loss: 212.30153| val_0_rmse: 15.21095|  0:04:10s
epoch 71 | loss: 212.01886| val_0_rmse: 14.81279|  0:04:14s
epoch 72 | loss: 206.39895| val_0_rmse: 15.01684|  0:04:18s
epoch 73 | loss: 207.84502| val_0_rmse: 14.85109|  0:04:21s
epoch 74 | loss: 199.81088| val_0_rmse: 14.65005|  0:04:25s
epoch 75 | loss: 203.39964| val_0_rmse: 14.47648|  0:04:28s
epoch 76 | loss: 201.54068| val_0_rmse: 14.29451|  0:04:32s
epoch 77 | loss: 200.81166| val_0_rmse: 14.59517|  0:04:35s
epoch 78 | loss: 197.92185| val_0_rmse: 14.33407|  0:04:39s
epoch 79 | loss: 186.28416| val_0_rmse: 14.27502|  0:04:42s
epoch 80 | loss: 189.2252| val_0_rmse: 14.08742|  0:04:46s
epoch 81 | loss: 190.53053| val_0_rmse: 13.9825 |  0:04:49s
epoch 82 | loss: 185.15429| val_0_rmse: 14.28011|  0:04:53s
epoch 83 | loss: 185.14487| val_0_rmse: 14.06585|  0:04:56s
epoch 84 | loss: 183.4588| val_0_rmse: 13.98355|  0:05:00s
epoch 85 | loss: 182.05726| val_0_rmse: 14.00295|  0:05:03s
epoch 86 | loss: 175.11249| val_0_rmse: 13.92499|  0:05:07s
epoch 87 | loss: 174.72923| val_0_rmse: 13.97078|  0:05:10s
epoch 88 | loss: 172.20575| val_0_rmse: 13.95013|  0:05:14s
epoch 89 | loss: 173.62943| val_0_rmse: 13.61185|  0:05:17s
epoch 90 | loss: 170.62365| val_0_rmse: 13.77315|  0:05:21s
epoch 91 | loss: 168.22117| val_0_rmse: 13.81754|  0:05:25s
epoch 92 | loss: 164.41798| val_0_rmse: 13.60044|  0:05:28s
epoch 93 | loss: 165.6811| val_0_rmse: 13.78889|  0:05:32s
epoch 94 | loss: 165.81725| val_0_rmse: 13.85804|  0:05:35s
epoch 95 | loss: 163.474 | val_0_rmse: 13.71524|  0:05:39s
epoch 96 | loss: 161.35163| val_0_rmse: 13.2156 |  0:05:42s
epoch 97 | loss: 155.73246| val_0_rmse: 13.42833|  0:05:46s
epoch 98 | loss: 154.1201| val_0_rmse: 13.69241|  0:05:49s
epoch 99 | loss: 154.88157| val_0_rmse: 13.57847|  0:05:53s
epoch 100| loss: 151.34003| val_0_rmse: 13.42988|  0:05:56s
epoch 101| loss: 155.95268| val_0_rmse: 13.53869|  0:06:00s
epoch 102| loss: 151.82026| val_0_rmse: 13.41408|  0:06:03s
epoch 103| loss: 151.74686| val_0_rmse: 13.2351 |  0:06:07s
epoch 104| loss: 144.70657| val_0_rmse: 13.07471|  0:06:10s
epoch 105| loss: 148.17485| val_0_rmse: 13.57143|  0:06:14s
epoch 106| loss: 143.98958| val_0_rmse: 13.30711|  0:06:17s
epoch 107| loss: 143.95427| val_0_rmse: 13.03178|  0:06:21s
epoch 108| loss: 139.87257| val_0_rmse: 13.01808|  0:06:24s
epoch 109| loss: 140.18459| val_0_rmse: 13.24306|  0:06:28s
epoch 110| loss: 143.20543| val_0_rmse: 13.14291|  0:06:31s
epoch 111| loss: 139.92812| val_0_rmse: 13.44928|  0:06:35s
epoch 112| loss: 137.24699| val_0_rmse: 13.15556|  0:06:38s
epoch 113| loss: 136.29412| val_0_rmse: 13.38763|  0:06:42s
epoch 114| loss: 136.8855| val_0_rmse: 13.36191|  0:06:45s
epoch 115| loss: 132.6869| val_0_rmse: 13.13728|  0:06:49s
epoch 116| loss: 135.04633| val_0_rmse: 13.03993|  0:06:52s
epoch 117| loss: 131.70128| val_0_rmse: 13.01122|  0:06:56s
epoch 118| loss: 133.64025| val_0_rmse: 13.15401|  0:06:59s
epoch 119| loss: 136.43175| val_0_rmse: 12.8708 |  0:07:03s
epoch 120| loss: 132.38653| val_0_rmse: 13.01631|  0:07:06s
epoch 121| loss: 130.26822| val_0_rmse: 12.87446|  0:07:10s
epoch 122| loss: 130.62081| val_0_rmse: 12.84762|  0:07:13s
epoch 123| loss: 130.25068| val_0_rmse: 12.70854|  0:07:17s
epoch 124| loss: 127.67099| val_0_rmse: 12.94282|  0:07:20s
epoch 125| loss: 130.12871| val_0_rmse: 12.99134|  0:07:24s
epoch 126| loss: 131.32474| val_0_rmse: 12.83744|  0:07:27s
epoch 127| loss: 127.78049| val_0_rmse: 12.8123 |  0:07:31s
epoch 128| loss: 127.4884| val_0_rmse: 12.78005|  0:07:34s
epoch 129| loss: 125.48048| val_0_rmse: 12.64109|  0:07:38s
epoch 130| loss: 121.49258| val_0_rmse: 12.54566|  0:07:41s
epoch 131| loss: 120.4395| val_0_rmse: 13.21042|  0:07:45s
epoch 132| loss: 121.77385| val_0_rmse: 12.8471 |  0:07:48s
epoch 133| loss: 122.19948| val_0_rmse: 12.96764|  0:07:52s
epoch 134| loss: 121.36899| val_0_rmse: 12.49472|  0:07:55s
epoch 135| loss: 119.69669| val_0_rmse: 12.76752|  0:07:59s
epoch 136| loss: 121.24566| val_0_rmse: 12.76278|  0:08:02s
epoch 137| loss: 116.50348| val_0_rmse: 12.94873|  0:08:06s
epoch 138| loss: 112.82713| val_0_rmse: 12.89843|  0:08:09s
epoch 139| loss: 118.11623| val_0_rmse: 12.67201|  0:08:13s
epoch 140| loss: 113.93822| val_0_rmse: 12.89461|  0:08:16s
epoch 141| loss: 117.15014| val_0_rmse: 12.78689|  0:08:20s
epoch 142| loss: 115.84589| val_0_rmse: 12.8826 |  0:08:24s
epoch 143| loss: 117.24751| val_0_rmse: 12.70795|  0:08:27s
epoch 144| loss: 117.47459| val_0_rmse: 13.14497|  0:08:31s
epoch 145| loss: 116.32585| val_0_rmse: 13.03405|  0:08:34s
epoch 146| loss: 114.10528| val_0_rmse: 12.83367|  0:08:38s
epoch 147| loss: 112.03074| val_0_rmse: 12.75945|  0:08:41s
epoch 148| loss: 112.48698| val_0_rmse: 12.52535|  0:08:44s
epoch 149| loss: 111.87526| val_0_rmse: 12.38807|  0:08:48s
epoch 150| loss: 110.65671| val_0_rmse: 12.50479|  0:08:52s
epoch 151| loss: 111.96856| val_0_rmse: 12.54916|  0:08:55s
epoch 152| loss: 114.9691| val_0_rmse: 12.39973|  0:08:59s
epoch 153| loss: 111.53957| val_0_rmse: 12.22224|  0:09:02s
epoch 154| loss: 107.00752| val_0_rmse: 12.2867 |  0:09:06s
epoch 155| loss: 108.7059| val_0_rmse: 12.24896|  0:09:09s
epoch 156| loss: 110.60041| val_0_rmse: 12.51818|  0:09:13s
epoch 157| loss: 109.73601| val_0_rmse: 12.63173|  0:09:16s
epoch 158| loss: 108.67049| val_0_rmse: 12.57457|  0:09:20s
epoch 159| loss: 105.16868| val_0_rmse: 12.58991|  0:09:23s
epoch 160| loss: 108.72929| val_0_rmse: 12.4149 |  0:09:27s
epoch 161| loss: 106.73757| val_0_rmse: 12.24596|  0:09:30s
epoch 162| loss: 107.46493| val_0_rmse: 12.78228|  0:09:34s
epoch 163| loss: 104.94331| val_0_rmse: 12.6001 |  0:09:37s
epoch 164| loss: 104.13867| val_0_rmse: 12.61661|  0:09:41s
epoch 165| loss: 108.85845| val_0_rmse: 12.10986|  0:09:44s
epoch 166| loss: 108.14128| val_0_rmse: 12.47005|  0:09:48s
epoch 167| loss: 108.04546| val_0_rmse: 12.57628|  0:09:52s
epoch 168| loss: 104.76236| val_0_rmse: 12.3967 |  0:09:55s
epoch 169| loss: 108.64179| val_0_rmse: 12.46804|  0:09:58s
epoch 170| loss: 102.56744| val_0_rmse: 12.26234|  0:10:02s
epoch 171| loss: 105.4835| val_0_rmse: 12.1202 |  0:10:05s
epoch 172| loss: 101.44792| val_0_rmse: 12.4067 |  0:10:09s
epoch 173| loss: 101.59068| val_0_rmse: 12.31297|  0:10:12s
epoch 174| loss: 100.93943| val_0_rmse: 12.14089|  0:10:16s
epoch 175| loss: 102.99433| val_0_rmse: 12.0614 |  0:10:20s
epoch 176| loss: 100.65681| val_0_rmse: 12.05032|  0:10:23s
epoch 177| loss: 102.80085| val_0_rmse: 12.04215|  0:10:27s
epoch 178| loss: 104.71696| val_0_rmse: 12.28272|  0:10:30s
epoch 179| loss: 100.26587| val_0_rmse: 12.11309|  0:10:34s
epoch 180| loss: 102.94927| val_0_rmse: 11.87147|  0:10:37s
epoch 181| loss: 103.73143| val_0_rmse: 12.37728|  0:10:41s
epoch 182| loss: 99.80729| val_0_rmse: 12.30969|  0:10:44s
epoch 183| loss: 99.77146| val_0_rmse: 12.23776|  0:10:48s
epoch 184| loss: 102.44108| val_0_rmse: 12.70112|  0:10:51s
epoch 185| loss: 96.36287| val_0_rmse: 12.14343|  0:10:55s
epoch 186| loss: 100.64707| val_0_rmse: 13.08195|  0:10:58s
epoch 187| loss: 102.16998| val_0_rmse: 12.24592|  0:11:02s
epoch 188| loss: 95.90027| val_0_rmse: 12.41863|  0:11:05s
epoch 189| loss: 97.87399| val_0_rmse: 12.15044|  0:11:09s
epoch 190| loss: 99.75367| val_0_rmse: 12.47464|  0:11:12s
epoch 191| loss: 97.54326| val_0_rmse: 12.13685|  0:11:16s
epoch 192| loss: 96.56842| val_0_rmse: 11.89996|  0:11:19s
epoch 193| loss: 98.42005| val_0_rmse: 11.94162|  0:11:23s
epoch 194| loss: 97.45415| val_0_rmse: 12.36381|  0:11:26s
epoch 195| loss: 96.23008| val_0_rmse: 12.0345 |  0:11:30s
epoch 196| loss: 96.67515| val_0_rmse: 12.11805|  0:11:33s
epoch 197| loss: 98.9938 | val_0_rmse: 12.01128|  0:11:37s
epoch 198| loss: 97.5127 | val_0_rmse: 12.08623|  0:11:40s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 199| loss: 93.7732 | val_0_rmse: 12.32502|  0:11:44s
Stop training because you reached max_epochs = 200 with best_epoch = 180 and best_val_0_rmse = 11.87147
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 21:52:47,557] Trial 20 finished with value: 11.871467460224554 and parameters: {'lr': 0.003217203618852155, 'n_steps': 6, 'gamma': 1.4215426438153023, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.01508628711174725, 'weight_decay': 7.791747063309178e-05, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 19 with value: 11.568680108990407.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8617.45801| val_0_rmse: 94.41712|  0:00:03s
epoch 1  | loss: 8633.28258| val_0_rmse: 94.27827|  0:00:07s
epoch 2  | loss: 8555.95984| val_0_rmse: 93.42304|  0:00:10s
epoch 3  | loss: 8225.78574| val_0_rmse: 89.67424|  0:00:14s
epoch 4  | loss: 7192.76404| val_0_rmse: 78.56631|  0:00:17s
epoch 5  | loss: 5029.67964| val_0_rmse: 59.17723|  0:00:21s
epoch 6  | loss: 2783.88249| val_0_rmse: 45.00668|  0:00:25s
epoch 7  | loss: 1396.92052| val_0_rmse: 34.30856|  0:00:28s
epoch 8  | loss: 1110.74535| val_0_rmse: 33.09319|  0:00:32s
epoch 9  | loss: 1044.21688| val_0_rmse: 32.19412|  0:00:36s
epoch 10 | loss: 1023.83733| val_0_rmse: 32.54897|  0:00:39s
epoch 11 | loss: 1011.95735| val_0_rmse: 32.1916 |  0:00:43s
epoch 12 | loss: 1007.33341| val_0_rmse: 31.61095|  0:00:46s
epoch 13 | loss: 993.28652| val_0_rmse: 31.64246|  0:00:50s
epoch 14 | loss: 992.69368| val_0_rmse: 31.39663|  0:00:54s
epoch 15 | loss: 980.38842| val_0_rmse: 31.21938|  0:00:57s
epoch 16 | loss: 972.30391| val_0_rmse: 31.43467|  0:01:01s
epoch 17 | loss: 963.97389| val_0_rmse: 31.38725|  0:01:04s
epoch 18 | loss: 955.94469| val_0_rmse: 31.47854|  0:01:08s
epoch 19 | loss: 946.12657| val_0_rmse: 31.13995|  0:01:12s
epoch 20 | loss: 928.75958| val_0_rmse: 31.1229 |  0:01:15s
epoch 21 | loss: 930.41496| val_0_rmse: 31.51865|  0:01:19s
epoch 22 | loss: 922.09311| val_0_rmse: 30.87536|  0:01:22s
epoch 23 | loss: 912.79897| val_0_rmse: 30.70374|  0:01:26s
epoch 24 | loss: 912.25428| val_0_rmse: 30.62976|  0:01:30s
epoch 25 | loss: 900.42846| val_0_rmse: 31.31819|  0:01:33s
epoch 26 | loss: 892.06705| val_0_rmse: 30.84714|  0:01:37s
epoch 27 | loss: 878.8101| val_0_rmse: 30.6133 |  0:01:40s
epoch 28 | loss: 863.57778| val_0_rmse: 30.2997 |  0:01:44s
epoch 29 | loss: 852.71873| val_0_rmse: 29.61317|  0:01:48s
epoch 30 | loss: 836.12066| val_0_rmse: 29.02911|  0:01:51s
epoch 31 | loss: 818.20967| val_0_rmse: 29.6094 |  0:01:55s
epoch 32 | loss: 804.86037| val_0_rmse: 29.41651|  0:01:59s
epoch 33 | loss: 784.44312| val_0_rmse: 28.93329|  0:02:02s
epoch 34 | loss: 773.1598| val_0_rmse: 28.03259|  0:02:06s
epoch 35 | loss: 753.15555| val_0_rmse: 28.0242 |  0:02:10s
epoch 36 | loss: 742.2733| val_0_rmse: 27.78613|  0:02:13s
epoch 37 | loss: 721.99156| val_0_rmse: 26.7737 |  0:02:17s
epoch 38 | loss: 713.14592| val_0_rmse: 26.66394|  0:02:20s
epoch 39 | loss: 687.96576| val_0_rmse: 26.26368|  0:02:24s
epoch 40 | loss: 656.76903| val_0_rmse: 26.04051|  0:02:28s
epoch 41 | loss: 636.16772| val_0_rmse: 25.59703|  0:02:31s
epoch 42 | loss: 607.42504| val_0_rmse: 25.49814|  0:02:35s
epoch 43 | loss: 583.24587| val_0_rmse: 24.98823|  0:02:38s
epoch 44 | loss: 560.44452| val_0_rmse: 24.5001 |  0:02:42s
epoch 45 | loss: 535.14662| val_0_rmse: 24.15976|  0:02:46s
epoch 46 | loss: 524.4843| val_0_rmse: 24.02164|  0:02:49s
epoch 47 | loss: 501.89321| val_0_rmse: 23.86411|  0:02:53s
epoch 48 | loss: 487.69707| val_0_rmse: 23.24777|  0:02:56s
epoch 49 | loss: 464.95873| val_0_rmse: 22.77889|  0:03:00s
epoch 50 | loss: 454.23422| val_0_rmse: 22.97964|  0:03:04s
epoch 51 | loss: 439.66871| val_0_rmse: 22.61072|  0:03:07s
epoch 52 | loss: 426.59639| val_0_rmse: 22.13867|  0:03:11s
epoch 53 | loss: 419.11426| val_0_rmse: 22.00842|  0:03:14s
epoch 54 | loss: 404.04625| val_0_rmse: 21.95442|  0:03:18s
epoch 55 | loss: 395.94346| val_0_rmse: 21.74316|  0:03:22s
epoch 56 | loss: 380.11016| val_0_rmse: 21.56275|  0:03:25s
epoch 57 | loss: 373.59147| val_0_rmse: 20.95056|  0:03:29s
epoch 58 | loss: 363.17376| val_0_rmse: 21.03675|  0:03:33s
epoch 59 | loss: 354.28371| val_0_rmse: 21.16067|  0:03:36s
epoch 60 | loss: 346.50619| val_0_rmse: 20.458  |  0:03:40s
epoch 61 | loss: 343.15006| val_0_rmse: 20.14475|  0:03:43s
epoch 62 | loss: 328.7658| val_0_rmse: 20.80044|  0:03:47s
epoch 63 | loss: 320.13172| val_0_rmse: 19.87026|  0:03:50s
epoch 64 | loss: 316.70211| val_0_rmse: 19.94611|  0:03:54s
epoch 65 | loss: 310.88395| val_0_rmse: 19.28171|  0:03:58s
epoch 66 | loss: 303.04702| val_0_rmse: 18.63947|  0:04:01s
epoch 67 | loss: 299.3779| val_0_rmse: 18.25332|  0:04:05s
epoch 68 | loss: 293.63779| val_0_rmse: 18.37887|  0:04:09s
epoch 69 | loss: 287.32816| val_0_rmse: 17.85945|  0:04:12s
epoch 70 | loss: 280.27766| val_0_rmse: 18.17348|  0:04:16s
epoch 71 | loss: 274.17845| val_0_rmse: 17.71381|  0:04:19s
epoch 72 | loss: 273.26843| val_0_rmse: 17.76501|  0:04:23s
epoch 73 | loss: 261.65482| val_0_rmse: 17.35564|  0:04:26s
epoch 74 | loss: 258.4072| val_0_rmse: 17.59774|  0:04:30s
epoch 75 | loss: 259.28167| val_0_rmse: 17.28506|  0:04:34s
epoch 76 | loss: 252.50067| val_0_rmse: 16.62301|  0:04:37s
epoch 77 | loss: 251.40586| val_0_rmse: 16.54918|  0:04:41s
epoch 78 | loss: 243.28204| val_0_rmse: 16.57965|  0:04:45s
epoch 79 | loss: 239.85266| val_0_rmse: 16.74497|  0:04:48s
epoch 80 | loss: 237.31578| val_0_rmse: 16.42737|  0:04:52s
epoch 81 | loss: 235.75218| val_0_rmse: 15.90341|  0:04:55s
epoch 82 | loss: 233.64786| val_0_rmse: 15.76048|  0:04:59s
epoch 83 | loss: 232.01876| val_0_rmse: 15.77423|  0:05:03s
epoch 84 | loss: 223.42667| val_0_rmse: 15.48862|  0:05:06s
epoch 85 | loss: 224.85969| val_0_rmse: 15.13133|  0:05:10s
epoch 86 | loss: 221.06898| val_0_rmse: 15.07728|  0:05:13s
epoch 87 | loss: 210.71675| val_0_rmse: 15.00967|  0:05:17s
epoch 88 | loss: 213.27914| val_0_rmse: 15.1649 |  0:05:21s
epoch 89 | loss: 208.88202| val_0_rmse: 15.06193|  0:05:24s
epoch 90 | loss: 210.16598| val_0_rmse: 14.98176|  0:05:28s
epoch 91 | loss: 205.09785| val_0_rmse: 15.12525|  0:05:32s
epoch 92 | loss: 201.52391| val_0_rmse: 14.43209|  0:05:35s
epoch 93 | loss: 201.00322| val_0_rmse: 14.70171|  0:05:39s
epoch 94 | loss: 202.43433| val_0_rmse: 14.75271|  0:05:42s
epoch 95 | loss: 196.8408| val_0_rmse: 14.46173|  0:05:46s
epoch 96 | loss: 200.54705| val_0_rmse: 14.20048|  0:05:49s
epoch 97 | loss: 194.59804| val_0_rmse: 14.22731|  0:05:53s
epoch 98 | loss: 183.22551| val_0_rmse: 14.29652|  0:05:57s
epoch 99 | loss: 189.02174| val_0_rmse: 13.88129|  0:06:00s
epoch 100| loss: 189.86539| val_0_rmse: 13.56297|  0:06:04s
epoch 101| loss: 186.33696| val_0_rmse: 14.14624|  0:06:08s
epoch 102| loss: 179.77001| val_0_rmse: 13.42707|  0:06:12s
epoch 103| loss: 182.72403| val_0_rmse: 13.45659|  0:06:15s
epoch 104| loss: 180.47614| val_0_rmse: 13.46191|  0:06:19s
epoch 105| loss: 174.99812| val_0_rmse: 13.58407|  0:06:22s
epoch 106| loss: 177.95026| val_0_rmse: 13.66223|  0:06:26s
epoch 107| loss: 175.75904| val_0_rmse: 13.98048|  0:06:30s
epoch 108| loss: 173.07912| val_0_rmse: 13.58656|  0:06:33s
epoch 109| loss: 168.62796| val_0_rmse: 13.42972|  0:06:37s
epoch 110| loss: 169.8706| val_0_rmse: 13.48013|  0:06:40s
epoch 111| loss: 167.66521| val_0_rmse: 15.1004 |  0:06:44s
epoch 112| loss: 164.04054| val_0_rmse: 14.97358|  0:06:47s
epoch 113| loss: 168.90035| val_0_rmse: 14.04696|  0:06:51s
epoch 114| loss: 162.38003| val_0_rmse: 13.16232|  0:06:54s
epoch 115| loss: 160.30264| val_0_rmse: 13.48676|  0:06:58s
epoch 116| loss: 160.79205| val_0_rmse: 12.85487|  0:07:02s
epoch 117| loss: 157.78414| val_0_rmse: 13.1572 |  0:07:05s
epoch 118| loss: 160.29442| val_0_rmse: 13.13185|  0:07:09s
epoch 119| loss: 158.94078| val_0_rmse: 13.18801|  0:07:12s
epoch 120| loss: 155.52709| val_0_rmse: 12.99246|  0:07:16s
epoch 121| loss: 152.53618| val_0_rmse: 12.88683|  0:07:19s
epoch 122| loss: 150.85702| val_0_rmse: 13.07765|  0:07:23s
epoch 123| loss: 146.93209| val_0_rmse: 12.84794|  0:07:26s
epoch 124| loss: 148.10728| val_0_rmse: 12.83669|  0:07:30s
epoch 125| loss: 150.17785| val_0_rmse: 12.99362|  0:07:33s
epoch 126| loss: 147.18098| val_0_rmse: 12.74263|  0:07:37s
epoch 127| loss: 147.95308| val_0_rmse: 12.7292 |  0:07:40s
epoch 128| loss: 148.39077| val_0_rmse: 12.93636|  0:07:44s
epoch 129| loss: 144.56896| val_0_rmse: 13.12292|  0:07:48s
epoch 130| loss: 144.18946| val_0_rmse: 14.06115|  0:07:51s
epoch 131| loss: 144.95334| val_0_rmse: 12.41594|  0:07:55s
epoch 132| loss: 139.25144| val_0_rmse: 12.63276|  0:07:58s
epoch 133| loss: 139.47741| val_0_rmse: 12.58587|  0:08:02s
epoch 134| loss: 137.85617| val_0_rmse: 12.7955 |  0:08:05s
epoch 135| loss: 136.69459| val_0_rmse: 12.68115|  0:08:09s
epoch 136| loss: 134.72263| val_0_rmse: 12.69846|  0:08:12s
epoch 137| loss: 132.83824| val_0_rmse: 12.4586 |  0:08:16s
epoch 138| loss: 130.60977| val_0_rmse: 12.34758|  0:08:19s
epoch 139| loss: 133.03096| val_0_rmse: 12.25852|  0:08:23s
epoch 140| loss: 131.22642| val_0_rmse: 12.69591|  0:08:26s
epoch 141| loss: 132.92443| val_0_rmse: 12.33032|  0:08:30s
epoch 142| loss: 132.40737| val_0_rmse: 12.71162|  0:08:33s
epoch 143| loss: 128.63108| val_0_rmse: 12.63638|  0:08:37s
epoch 144| loss: 132.74353| val_0_rmse: 12.50905|  0:08:40s
epoch 145| loss: 124.48422| val_0_rmse: 12.26163|  0:08:44s
epoch 146| loss: 124.726 | val_0_rmse: 12.30836|  0:08:47s
epoch 147| loss: 124.83269| val_0_rmse: 12.19707|  0:08:51s
epoch 148| loss: 122.87178| val_0_rmse: 12.41319|  0:08:54s
epoch 149| loss: 123.69097| val_0_rmse: 12.19121|  0:08:58s
epoch 150| loss: 125.41898| val_0_rmse: 12.2493 |  0:09:02s
epoch 151| loss: 123.81537| val_0_rmse: 12.24547|  0:09:05s
epoch 152| loss: 123.32942| val_0_rmse: 12.29798|  0:09:09s
epoch 153| loss: 119.30275| val_0_rmse: 12.04627|  0:09:12s
epoch 154| loss: 119.36325| val_0_rmse: 11.99349|  0:09:16s
epoch 155| loss: 120.31798| val_0_rmse: 12.02239|  0:09:19s
epoch 156| loss: 119.80515| val_0_rmse: 12.02351|  0:09:23s
epoch 157| loss: 118.07829| val_0_rmse: 11.96173|  0:09:26s
epoch 158| loss: 114.64421| val_0_rmse: 12.10371|  0:09:30s
epoch 159| loss: 117.27586| val_0_rmse: 12.33345|  0:09:33s
epoch 160| loss: 115.77976| val_0_rmse: 12.30999|  0:09:37s
epoch 161| loss: 116.67596| val_0_rmse: 12.42488|  0:09:40s
epoch 162| loss: 116.77335| val_0_rmse: 12.11696|  0:09:44s
epoch 163| loss: 114.43713| val_0_rmse: 12.17359|  0:09:47s
epoch 164| loss: 113.17436| val_0_rmse: 12.14714|  0:09:51s
epoch 165| loss: 118.18083| val_0_rmse: 11.748  |  0:09:54s
epoch 166| loss: 114.30082| val_0_rmse: 11.86685|  0:09:58s
epoch 167| loss: 110.72943| val_0_rmse: 11.88143|  0:10:01s
epoch 168| loss: 114.7381| val_0_rmse: 12.04776|  0:10:05s
epoch 169| loss: 115.35736| val_0_rmse: 11.57972|  0:10:08s
epoch 170| loss: 107.65216| val_0_rmse: 11.50205|  0:10:12s
epoch 171| loss: 112.72971| val_0_rmse: 11.86559|  0:10:16s
epoch 172| loss: 111.27311| val_0_rmse: 11.61258|  0:10:19s
epoch 173| loss: 110.10432| val_0_rmse: 12.12808|  0:10:22s
epoch 174| loss: 114.66704| val_0_rmse: 11.59266|  0:10:26s
epoch 175| loss: 110.17636| val_0_rmse: 11.94384|  0:10:30s
epoch 176| loss: 108.52878| val_0_rmse: 11.7009 |  0:10:33s
epoch 177| loss: 110.28905| val_0_rmse: 11.75346|  0:10:37s
epoch 178| loss: 106.63654| val_0_rmse: 11.54644|  0:10:40s
epoch 179| loss: 109.37137| val_0_rmse: 11.49401|  0:10:44s
epoch 180| loss: 109.21728| val_0_rmse: 11.58172|  0:10:48s
epoch 181| loss: 104.84037| val_0_rmse: 11.53145|  0:10:51s
epoch 182| loss: 105.49577| val_0_rmse: 11.79831|  0:10:55s
epoch 183| loss: 105.35321| val_0_rmse: 11.73018|  0:10:58s
epoch 184| loss: 109.7641| val_0_rmse: 12.14215|  0:11:02s
epoch 185| loss: 102.52504| val_0_rmse: 11.8289 |  0:11:05s
epoch 186| loss: 111.33172| val_0_rmse: 11.76749|  0:11:09s
epoch 187| loss: 103.27834| val_0_rmse: 11.66304|  0:11:12s
epoch 188| loss: 102.04151| val_0_rmse: 11.75663|  0:11:16s
epoch 189| loss: 100.57662| val_0_rmse: 11.74336|  0:11:19s
epoch 190| loss: 101.13226| val_0_rmse: 11.81259|  0:11:23s
epoch 191| loss: 101.87198| val_0_rmse: 11.69618|  0:11:26s
epoch 192| loss: 105.15449| val_0_rmse: 11.423  |  0:11:30s
epoch 193| loss: 100.95403| val_0_rmse: 11.51051|  0:11:33s
epoch 194| loss: 100.13005| val_0_rmse: 11.81464|  0:11:37s
epoch 195| loss: 105.53982| val_0_rmse: 11.59934|  0:11:40s
epoch 196| loss: 100.94519| val_0_rmse: 11.48361|  0:11:44s
epoch 197| loss: 99.23211| val_0_rmse: 11.58652|  0:11:47s
epoch 198| loss: 100.96146| val_0_rmse: 11.48489|  0:11:51s
epoch 199| loss: 99.46053| val_0_rmse: 11.64768|  0:11:54s
Stop training because you reached max_epochs = 200 with best_epoch = 192 and best_val_0_rmse = 11.423
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 22:04:50,104] Trial 21 finished with value: 11.422998560747544 and parameters: {'lr': 0.003125799102640622, 'n_steps': 6, 'gamma': 1.4272449155206772, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.010619535976877537, 'weight_decay': 7.260077124984901e-05, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8627.93433| val_0_rmse: 92.52199|  0:00:03s
epoch 1  | loss: 8670.34273| val_0_rmse: 94.13919|  0:00:07s
epoch 2  | loss: 8688.02245| val_0_rmse: 94.46986|  0:00:11s
epoch 3  | loss: 8646.92917| val_0_rmse: 89.55743|  0:00:15s
epoch 4  | loss: 8334.43212| val_0_rmse: 89.37489|  0:00:19s
epoch 5  | loss: 6940.9179| val_0_rmse: 66.34131|  0:00:23s
epoch 6  | loss: 3698.28931| val_0_rmse: 44.08601|  0:00:27s
epoch 7  | loss: 1541.3692| val_0_rmse: 36.44663|  0:00:31s
epoch 8  | loss: 1048.16016| val_0_rmse: 32.20741|  0:00:35s
epoch 9  | loss: 1013.30616| val_0_rmse: 32.90236|  0:00:39s
epoch 10 | loss: 982.90817| val_0_rmse: 32.40235|  0:00:43s
epoch 11 | loss: 966.59927| val_0_rmse: 32.58956|  0:00:47s
epoch 12 | loss: 958.12295| val_0_rmse: 30.93039|  0:00:51s
epoch 13 | loss: 953.02712| val_0_rmse: 31.01805|  0:00:55s
epoch 14 | loss: 947.14871| val_0_rmse: 30.958  |  0:00:59s
epoch 15 | loss: 912.09465| val_0_rmse: 30.73161|  0:01:03s
epoch 16 | loss: 906.28852| val_0_rmse: 30.23652|  0:01:07s
epoch 17 | loss: 892.8485| val_0_rmse: 29.85436|  0:01:11s
epoch 18 | loss: 899.08219| val_0_rmse: 30.04175|  0:01:15s
epoch 19 | loss: 883.36455| val_0_rmse: 35.85098|  0:01:19s
epoch 20 | loss: 869.16713| val_0_rmse: 29.31662|  0:01:23s
epoch 21 | loss: 864.68067| val_0_rmse: 29.42951|  0:01:27s
epoch 22 | loss: 867.96511| val_0_rmse: 29.3897 |  0:01:31s
epoch 23 | loss: 857.31456| val_0_rmse: 28.97404|  0:01:35s
epoch 24 | loss: 829.8802| val_0_rmse: 38.14046|  0:01:39s
epoch 25 | loss: 809.41151| val_0_rmse: 28.95005|  0:01:43s
epoch 26 | loss: 800.88654| val_0_rmse: 28.42133|  0:01:47s
epoch 27 | loss: 787.90742| val_0_rmse: 28.40197|  0:01:51s
epoch 28 | loss: 775.85524| val_0_rmse: 27.86339|  0:01:55s
epoch 29 | loss: 769.91696| val_0_rmse: 28.78707|  0:01:59s
epoch 30 | loss: 750.22396| val_0_rmse: 29.2461 |  0:02:03s
epoch 31 | loss: 736.44735| val_0_rmse: 29.00201|  0:02:07s
epoch 32 | loss: 714.14988| val_0_rmse: 27.35754|  0:02:11s
epoch 33 | loss: 686.3237| val_0_rmse: 27.66046|  0:02:15s
epoch 34 | loss: 664.7943| val_0_rmse: 26.28833|  0:02:19s
epoch 35 | loss: 636.78294| val_0_rmse: 25.07495|  0:02:23s
epoch 36 | loss: 607.28971| val_0_rmse: 24.27826|  0:02:27s
epoch 37 | loss: 581.42123| val_0_rmse: 23.62074|  0:02:31s
epoch 38 | loss: 557.94326| val_0_rmse: 23.4722 |  0:02:35s
epoch 39 | loss: 538.4386| val_0_rmse: 22.72838|  0:02:39s
epoch 40 | loss: 513.96219| val_0_rmse: 22.38946|  0:02:43s
epoch 41 | loss: 504.98677| val_0_rmse: 22.04422|  0:02:47s
epoch 42 | loss: 480.25532| val_0_rmse: 21.59584|  0:02:51s
epoch 43 | loss: 462.81084| val_0_rmse: 21.26008|  0:02:55s
epoch 44 | loss: 455.00982| val_0_rmse: 21.06385|  0:02:59s
epoch 45 | loss: 436.06061| val_0_rmse: 20.53709|  0:03:03s
epoch 46 | loss: 430.43421| val_0_rmse: 20.57762|  0:03:07s
epoch 47 | loss: 420.81086| val_0_rmse: 20.22912|  0:03:11s
epoch 48 | loss: 409.51569| val_0_rmse: 21.41933|  0:03:15s
epoch 49 | loss: 404.19798| val_0_rmse: 19.84596|  0:03:19s
epoch 50 | loss: 393.73338| val_0_rmse: 19.82913|  0:03:23s
epoch 51 | loss: 386.73025| val_0_rmse: 19.50713|  0:03:27s
epoch 52 | loss: 375.54731| val_0_rmse: 19.31446|  0:03:31s
epoch 53 | loss: 360.38617| val_0_rmse: 19.33633|  0:03:35s
epoch 54 | loss: 360.25491| val_0_rmse: 19.51105|  0:03:39s
epoch 55 | loss: 347.41412| val_0_rmse: 18.7648 |  0:03:43s
epoch 56 | loss: 345.41913| val_0_rmse: 18.47668|  0:03:47s
epoch 57 | loss: 338.21983| val_0_rmse: 18.47194|  0:03:51s
epoch 58 | loss: 330.6661| val_0_rmse: 18.04451|  0:03:54s
epoch 59 | loss: 323.08319| val_0_rmse: 18.05021|  0:03:58s
epoch 60 | loss: 313.85065| val_0_rmse: 17.6646 |  0:04:02s
epoch 61 | loss: 310.98259| val_0_rmse: 17.86863|  0:04:06s
epoch 62 | loss: 300.71187| val_0_rmse: 17.33009|  0:04:10s
epoch 63 | loss: 292.92132| val_0_rmse: 16.93698|  0:04:14s
epoch 64 | loss: 287.87827| val_0_rmse: 16.92012|  0:04:18s
epoch 65 | loss: 284.61906| val_0_rmse: 17.05663|  0:04:22s
epoch 66 | loss: 277.78638| val_0_rmse: 16.79052|  0:04:26s
epoch 67 | loss: 271.96343| val_0_rmse: 16.62222|  0:04:30s
epoch 68 | loss: 268.5844| val_0_rmse: 16.37318|  0:04:34s
epoch 69 | loss: 262.47655| val_0_rmse: 16.62304|  0:04:38s
epoch 70 | loss: 257.39588| val_0_rmse: 16.50504|  0:04:41s
epoch 71 | loss: 253.41663| val_0_rmse: 16.39117|  0:04:45s
epoch 72 | loss: 251.64918| val_0_rmse: 16.057  |  0:04:49s
epoch 73 | loss: 245.78765| val_0_rmse: 16.10141|  0:04:53s
epoch 74 | loss: 247.03295| val_0_rmse: 16.04807|  0:04:57s
epoch 75 | loss: 247.49486| val_0_rmse: 15.69518|  0:05:01s
epoch 76 | loss: 244.08712| val_0_rmse: 15.75544|  0:05:05s
epoch 77 | loss: 237.24924| val_0_rmse: 15.69186|  0:05:09s
epoch 78 | loss: 233.52449| val_0_rmse: 15.62151|  0:05:13s
epoch 79 | loss: 228.78105| val_0_rmse: 15.59779|  0:05:17s
epoch 80 | loss: 230.86916| val_0_rmse: 15.47699|  0:05:21s
epoch 81 | loss: 225.56162| val_0_rmse: 14.98845|  0:05:24s
epoch 82 | loss: 219.06067| val_0_rmse: 14.78666|  0:05:28s
epoch 83 | loss: 218.1386| val_0_rmse: 15.65767|  0:05:32s
epoch 84 | loss: 217.23762| val_0_rmse: 14.78945|  0:05:36s
epoch 85 | loss: 211.17385| val_0_rmse: 14.74422|  0:05:40s
epoch 86 | loss: 208.77862| val_0_rmse: 14.7042 |  0:05:44s
epoch 87 | loss: 204.83649| val_0_rmse: 14.59252|  0:05:48s
epoch 88 | loss: 203.34338| val_0_rmse: 14.79265|  0:05:52s
epoch 89 | loss: 204.56388| val_0_rmse: 14.60785|  0:05:56s
epoch 90 | loss: 199.81937| val_0_rmse: 14.53147|  0:06:00s
epoch 91 | loss: 205.07366| val_0_rmse: 14.84177|  0:06:04s
epoch 92 | loss: 199.23882| val_0_rmse: 14.49499|  0:06:07s
epoch 93 | loss: 197.49328| val_0_rmse: 14.21122|  0:06:11s
epoch 94 | loss: 192.09782| val_0_rmse: 14.5249 |  0:06:15s
epoch 95 | loss: 197.15058| val_0_rmse: 14.29773|  0:06:19s
epoch 96 | loss: 186.66206| val_0_rmse: 14.1146 |  0:06:23s
epoch 97 | loss: 186.72036| val_0_rmse: 14.16577|  0:06:27s
epoch 98 | loss: 187.00949| val_0_rmse: 14.32168|  0:06:31s
epoch 99 | loss: 183.96792| val_0_rmse: 14.26635|  0:06:35s
epoch 100| loss: 174.33266| val_0_rmse: 14.29926|  0:06:39s
epoch 101| loss: 182.92893| val_0_rmse: 14.0292 |  0:06:43s
epoch 102| loss: 179.28411| val_0_rmse: 13.96244|  0:06:46s
epoch 103| loss: 177.22299| val_0_rmse: 14.28919|  0:06:50s
epoch 104| loss: 174.64571| val_0_rmse: 14.27006|  0:06:54s
epoch 105| loss: 174.53225| val_0_rmse: 13.8501 |  0:06:58s
epoch 106| loss: 172.52124| val_0_rmse: 14.09817|  0:07:02s
epoch 107| loss: 168.19321| val_0_rmse: 13.831  |  0:07:06s
epoch 108| loss: 169.64155| val_0_rmse: 13.72381|  0:07:10s
epoch 109| loss: 161.37528| val_0_rmse: 13.69657|  0:07:14s
epoch 110| loss: 161.25234| val_0_rmse: 13.50556|  0:07:18s
epoch 111| loss: 164.77135| val_0_rmse: 13.58931|  0:07:22s
epoch 112| loss: 162.94867| val_0_rmse: 13.52613|  0:07:26s
epoch 113| loss: 159.46599| val_0_rmse: 13.78112|  0:07:30s
epoch 114| loss: 160.88756| val_0_rmse: 13.87864|  0:07:33s
epoch 115| loss: 161.2311| val_0_rmse: 13.79932|  0:07:37s
epoch 116| loss: 155.65698| val_0_rmse: 13.77227|  0:07:41s
epoch 117| loss: 156.01944| val_0_rmse: 13.87928|  0:07:45s
epoch 118| loss: 155.58028| val_0_rmse: 13.50888|  0:07:49s
epoch 119| loss: 152.4418| val_0_rmse: 13.42033|  0:07:53s
epoch 120| loss: 156.93046| val_0_rmse: 13.29025|  0:07:57s
epoch 121| loss: 147.37502| val_0_rmse: 13.40783|  0:08:01s
epoch 122| loss: 150.13141| val_0_rmse: 13.17128|  0:08:05s
epoch 123| loss: 148.70287| val_0_rmse: 13.6267 |  0:08:09s
epoch 124| loss: 148.29926| val_0_rmse: 13.27401|  0:08:12s
epoch 125| loss: 143.01288| val_0_rmse: 13.89074|  0:08:16s
epoch 126| loss: 143.39726| val_0_rmse: 13.00765|  0:08:20s
epoch 127| loss: 143.65366| val_0_rmse: 13.21407|  0:08:24s
epoch 128| loss: 140.68205| val_0_rmse: 13.19878|  0:08:28s
epoch 129| loss: 139.34496| val_0_rmse: 13.482  |  0:08:32s
epoch 130| loss: 143.46406| val_0_rmse: 13.50352|  0:08:36s
epoch 131| loss: 138.06731| val_0_rmse: 13.23322|  0:08:40s
epoch 132| loss: 133.59185| val_0_rmse: 13.17094|  0:08:44s
epoch 133| loss: 133.08345| val_0_rmse: 13.34414|  0:08:48s
epoch 134| loss: 136.7154| val_0_rmse: 12.97434|  0:08:51s
epoch 135| loss: 132.4585| val_0_rmse: 13.22535|  0:08:55s
epoch 136| loss: 133.57202| val_0_rmse: 13.3821 |  0:08:59s
epoch 137| loss: 130.52305| val_0_rmse: 13.15187|  0:09:03s
epoch 138| loss: 132.80563| val_0_rmse: 13.38953|  0:09:07s
epoch 139| loss: 132.06014| val_0_rmse: 13.24374|  0:09:11s
epoch 140| loss: 125.74941| val_0_rmse: 13.18195|  0:09:15s
epoch 141| loss: 130.13547| val_0_rmse: 13.58294|  0:09:19s
epoch 142| loss: 122.10793| val_0_rmse: 12.82056|  0:09:23s
epoch 143| loss: 125.0027| val_0_rmse: 13.0575 |  0:09:27s
epoch 144| loss: 125.49037| val_0_rmse: 13.2712 |  0:09:31s
epoch 145| loss: 123.07626| val_0_rmse: 13.08514|  0:09:34s
epoch 146| loss: 124.67384| val_0_rmse: 13.25301|  0:09:38s
epoch 147| loss: 122.39497| val_0_rmse: 12.80593|  0:09:42s
epoch 148| loss: 119.61511| val_0_rmse: 12.97069|  0:09:46s
epoch 149| loss: 123.36401| val_0_rmse: 12.93061|  0:09:50s
epoch 150| loss: 118.06871| val_0_rmse: 12.89985|  0:09:54s
epoch 151| loss: 119.06426| val_0_rmse: 12.96333|  0:09:58s
epoch 152| loss: 115.17281| val_0_rmse: 12.76754|  0:10:02s
epoch 153| loss: 119.36721| val_0_rmse: 12.60424|  0:10:06s
epoch 154| loss: 118.58721| val_0_rmse: 12.45573|  0:10:10s
epoch 155| loss: 114.0352| val_0_rmse: 12.95698|  0:10:13s
epoch 156| loss: 115.17834| val_0_rmse: 12.97742|  0:10:17s
epoch 157| loss: 114.54387| val_0_rmse: 12.86877|  0:10:21s
epoch 158| loss: 116.68934| val_0_rmse: 12.75157|  0:10:25s
epoch 159| loss: 111.90208| val_0_rmse: 12.87172|  0:10:29s
epoch 160| loss: 113.97099| val_0_rmse: 12.97722|  0:10:33s
epoch 161| loss: 111.23938| val_0_rmse: 12.78115|  0:10:37s
epoch 162| loss: 115.05367| val_0_rmse: 12.69319|  0:10:41s
epoch 163| loss: 114.10637| val_0_rmse: 12.67936|  0:10:44s
epoch 164| loss: 110.34871| val_0_rmse: 12.55273|  0:10:48s
epoch 165| loss: 111.50851| val_0_rmse: 12.78252|  0:10:52s
epoch 166| loss: 106.39238| val_0_rmse: 12.58479|  0:10:56s
epoch 167| loss: 111.53645| val_0_rmse: 12.39736|  0:11:00s
epoch 168| loss: 112.17443| val_0_rmse: 12.69383|  0:11:04s
epoch 169| loss: 111.32728| val_0_rmse: 12.72138|  0:11:08s
epoch 170| loss: 106.81065| val_0_rmse: 12.41151|  0:11:12s
epoch 171| loss: 108.47924| val_0_rmse: 12.80896|  0:11:16s
epoch 172| loss: 108.4716| val_0_rmse: 12.71348|  0:11:20s
epoch 173| loss: 105.41372| val_0_rmse: 12.57998|  0:11:23s
epoch 174| loss: 106.6152| val_0_rmse: 12.24266|  0:11:28s
epoch 175| loss: 109.53466| val_0_rmse: 12.55689|  0:11:31s
epoch 176| loss: 106.40893| val_0_rmse: 12.43437|  0:11:35s
epoch 177| loss: 105.62931| val_0_rmse: 12.32069|  0:11:39s
epoch 178| loss: 102.33637| val_0_rmse: 12.44571|  0:11:43s
epoch 179| loss: 112.85882| val_0_rmse: 12.54364|  0:11:47s
epoch 180| loss: 101.61795| val_0_rmse: 12.0611 |  0:11:51s
epoch 181| loss: 102.06287| val_0_rmse: 12.41636|  0:11:55s
epoch 182| loss: 102.84078| val_0_rmse: 12.589  |  0:11:59s
epoch 183| loss: 102.94094| val_0_rmse: 12.25683|  0:12:03s
epoch 184| loss: 104.91399| val_0_rmse: 12.52943|  0:12:06s
epoch 185| loss: 102.33362| val_0_rmse: 12.15677|  0:12:10s
epoch 186| loss: 100.53868| val_0_rmse: 12.29248|  0:12:14s
epoch 187| loss: 103.78271| val_0_rmse: 12.23769|  0:12:18s
epoch 188| loss: 100.94838| val_0_rmse: 12.61005|  0:12:22s
epoch 189| loss: 100.89632| val_0_rmse: 12.33744|  0:12:26s
epoch 190| loss: 102.27042| val_0_rmse: 12.1693 |  0:12:30s
epoch 191| loss: 100.44644| val_0_rmse: 11.92574|  0:12:34s
epoch 192| loss: 101.15683| val_0_rmse: 12.40031|  0:12:38s
epoch 193| loss: 101.31122| val_0_rmse: 12.49865|  0:12:42s
epoch 194| loss: 100.3742| val_0_rmse: 11.91438|  0:12:45s
epoch 195| loss: 97.46953| val_0_rmse: 12.2329 |  0:12:49s
epoch 196| loss: 97.62949| val_0_rmse: 12.41661|  0:12:53s
epoch 197| loss: 97.04405| val_0_rmse: 11.99563|  0:12:57s
epoch 198| loss: 97.91258| val_0_rmse: 11.97366|  0:13:01s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 199| loss: 101.64288| val_0_rmse: 12.06003|  0:13:05s
Stop training because you reached max_epochs = 200 with best_epoch = 194 and best_val_0_rmse = 11.91438
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 22:18:04,294] Trial 22 finished with value: 11.91438074217846 and parameters: {'lr': 0.003666938588023351, 'n_steps': 7, 'gamma': 1.4312623779706661, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.010230106880927119, 'weight_decay': 0.0001336140123597241, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8616.6984| val_0_rmse: 93.33751|  0:00:03s
epoch 1  | loss: 8644.47658| val_0_rmse: 94.17513|  0:00:07s
epoch 2  | loss: 8646.88214| val_0_rmse: 93.78887|  0:00:10s
epoch 3  | loss: 8569.96092| val_0_rmse: 92.15552|  0:00:14s
epoch 4  | loss: 8247.04737| val_0_rmse: 90.528  |  0:00:18s
epoch 5  | loss: 7414.1172| val_0_rmse: 83.57955|  0:00:21s
epoch 6  | loss: 5692.28923| val_0_rmse: 68.44961|  0:00:25s
epoch 7  | loss: 3532.83244| val_0_rmse: 51.45201|  0:00:28s
epoch 8  | loss: 1942.24451| val_0_rmse: 39.97338|  0:00:32s
epoch 9  | loss: 1386.9946| val_0_rmse: 35.9316 |  0:00:36s
epoch 10 | loss: 1191.41065| val_0_rmse: 33.3519 |  0:00:39s
epoch 11 | loss: 1073.87664| val_0_rmse: 33.07799|  0:00:43s
epoch 12 | loss: 1028.65231| val_0_rmse: 31.42577|  0:00:47s
epoch 13 | loss: 980.79843| val_0_rmse: 31.86441|  0:00:50s
epoch 14 | loss: 970.05597| val_0_rmse: 31.14442|  0:00:54s
epoch 15 | loss: 947.60689| val_0_rmse: 31.34869|  0:00:57s
epoch 16 | loss: 941.78598| val_0_rmse: 31.48748|  0:01:01s
epoch 17 | loss: 926.00062| val_0_rmse: 30.9337 |  0:01:04s
epoch 18 | loss: 919.82553| val_0_rmse: 30.34728|  0:01:08s
epoch 19 | loss: 917.69723| val_0_rmse: 30.92073|  0:01:12s
epoch 20 | loss: 899.78035| val_0_rmse: 30.45831|  0:01:15s
epoch 21 | loss: 890.84861| val_0_rmse: 29.80378|  0:01:19s
epoch 22 | loss: 876.03281| val_0_rmse: 30.04156|  0:01:23s
epoch 23 | loss: 868.00831| val_0_rmse: 30.08288|  0:01:26s
epoch 24 | loss: 864.59454| val_0_rmse: 30.13407|  0:01:30s
epoch 25 | loss: 847.31895| val_0_rmse: 31.65463|  0:01:33s
epoch 26 | loss: 846.70391| val_0_rmse: 30.00714|  0:01:37s
epoch 27 | loss: 842.3653| val_0_rmse: 29.70759|  0:01:40s
epoch 28 | loss: 829.9634| val_0_rmse: 29.26912|  0:01:44s
epoch 29 | loss: 822.13405| val_0_rmse: 30.44279|  0:01:48s
epoch 30 | loss: 808.85492| val_0_rmse: 29.45536|  0:01:51s
epoch 31 | loss: 793.46473| val_0_rmse: 29.01387|  0:01:55s
epoch 32 | loss: 772.30648| val_0_rmse: 28.25358|  0:01:59s
epoch 33 | loss: 748.6417| val_0_rmse: 28.62501|  0:02:02s
epoch 34 | loss: 720.7518| val_0_rmse: 27.12647|  0:02:06s
epoch 35 | loss: 685.42283| val_0_rmse: 27.30908|  0:02:09s
epoch 36 | loss: 672.16144| val_0_rmse: 25.87008|  0:02:13s
epoch 37 | loss: 645.73785| val_0_rmse: 25.45242|  0:02:17s
epoch 38 | loss: 619.7711| val_0_rmse: 24.64367|  0:02:20s
epoch 39 | loss: 596.51406| val_0_rmse: 24.26445|  0:02:24s
epoch 40 | loss: 567.07599| val_0_rmse: 24.01362|  0:02:27s
epoch 41 | loss: 543.75616| val_0_rmse: 23.1521 |  0:02:31s
epoch 42 | loss: 520.34881| val_0_rmse: 22.78926|  0:02:35s
epoch 43 | loss: 499.91028| val_0_rmse: 22.42814|  0:02:38s
epoch 44 | loss: 481.16336| val_0_rmse: 22.09295|  0:02:42s
epoch 45 | loss: 461.00316| val_0_rmse: 21.92679|  0:02:46s
epoch 46 | loss: 450.35439| val_0_rmse: 21.70611|  0:02:49s
epoch 47 | loss: 434.91862| val_0_rmse: 21.80921|  0:02:53s
epoch 48 | loss: 424.12041| val_0_rmse: 21.79846|  0:02:56s
epoch 49 | loss: 412.26564| val_0_rmse: 21.9216 |  0:03:00s
epoch 50 | loss: 403.66442| val_0_rmse: 19.7438 |  0:03:04s
epoch 51 | loss: 396.96015| val_0_rmse: 19.44807|  0:03:07s
epoch 52 | loss: 390.36837| val_0_rmse: 19.36243|  0:03:11s
epoch 53 | loss: 371.78077| val_0_rmse: 19.01919|  0:03:15s
epoch 54 | loss: 365.94333| val_0_rmse: 18.75336|  0:03:18s
epoch 55 | loss: 360.86215| val_0_rmse: 19.09477|  0:03:22s
epoch 56 | loss: 348.72333| val_0_rmse: 18.37112|  0:03:25s
epoch 57 | loss: 343.99263| val_0_rmse: 18.34286|  0:03:29s
epoch 58 | loss: 343.18216| val_0_rmse: 18.35275|  0:03:33s
epoch 59 | loss: 326.7273| val_0_rmse: 17.68312|  0:03:36s
epoch 60 | loss: 324.03564| val_0_rmse: 17.519  |  0:03:40s
epoch 61 | loss: 321.44159| val_0_rmse: 17.1202 |  0:03:44s
epoch 62 | loss: 304.88494| val_0_rmse: 17.47375|  0:03:47s
epoch 63 | loss: 300.13136| val_0_rmse: 17.47183|  0:03:51s
epoch 64 | loss: 298.74585| val_0_rmse: 16.96987|  0:03:54s
epoch 65 | loss: 298.30729| val_0_rmse: 16.62722|  0:03:58s
epoch 66 | loss: 286.37238| val_0_rmse: 16.53467|  0:04:02s
epoch 67 | loss: 287.74397| val_0_rmse: 16.60264|  0:04:05s
epoch 68 | loss: 283.05335| val_0_rmse: 16.32014|  0:04:09s
epoch 69 | loss: 274.07251| val_0_rmse: 15.99559|  0:04:13s
epoch 70 | loss: 267.76086| val_0_rmse: 16.38308|  0:04:16s
epoch 71 | loss: 264.07352| val_0_rmse: 16.3316 |  0:04:20s
epoch 72 | loss: 262.57264| val_0_rmse: 15.97641|  0:04:23s
epoch 73 | loss: 256.91808| val_0_rmse: 15.80122|  0:04:27s
epoch 74 | loss: 253.56254| val_0_rmse: 16.23594|  0:04:30s
epoch 75 | loss: 250.18313| val_0_rmse: 15.82789|  0:04:34s
epoch 76 | loss: 248.68818| val_0_rmse: 15.45111|  0:04:38s
epoch 77 | loss: 251.99027| val_0_rmse: 15.51214|  0:04:41s
epoch 78 | loss: 244.53948| val_0_rmse: 15.24226|  0:04:45s
epoch 79 | loss: 232.59942| val_0_rmse: 15.22527|  0:04:49s
epoch 80 | loss: 236.35568| val_0_rmse: 14.99312|  0:04:52s
epoch 81 | loss: 233.3164| val_0_rmse: 15.0321 |  0:04:56s
epoch 82 | loss: 236.41186| val_0_rmse: 14.80636|  0:04:59s
epoch 83 | loss: 237.33503| val_0_rmse: 15.10456|  0:05:03s
epoch 84 | loss: 231.31858| val_0_rmse: 15.39817|  0:05:07s
epoch 85 | loss: 230.46364| val_0_rmse: 15.11429|  0:05:10s
epoch 86 | loss: 223.68181| val_0_rmse: 15.166  |  0:05:14s
epoch 87 | loss: 223.20185| val_0_rmse: 14.83075|  0:05:17s
epoch 88 | loss: 216.82815| val_0_rmse: 15.60434|  0:05:21s
epoch 89 | loss: 211.91291| val_0_rmse: 15.39357|  0:05:25s
epoch 90 | loss: 214.79487| val_0_rmse: 14.80955|  0:05:28s
epoch 91 | loss: 211.18709| val_0_rmse: 14.73111|  0:05:32s
epoch 92 | loss: 206.39396| val_0_rmse: 14.78077|  0:05:35s
epoch 93 | loss: 208.82334| val_0_rmse: 14.34421|  0:05:39s
epoch 94 | loss: 207.89107| val_0_rmse: 14.48729|  0:05:43s
epoch 95 | loss: 199.60678| val_0_rmse: 14.55725|  0:05:46s
epoch 96 | loss: 200.11106| val_0_rmse: 13.87844|  0:05:50s
epoch 97 | loss: 198.60128| val_0_rmse: 14.04652|  0:05:54s
epoch 98 | loss: 194.30031| val_0_rmse: 14.33978|  0:05:57s
epoch 99 | loss: 192.16814| val_0_rmse: 13.75292|  0:06:01s
epoch 100| loss: 189.13933| val_0_rmse: 13.87453|  0:06:04s
epoch 101| loss: 192.07261| val_0_rmse: 14.08693|  0:06:08s
epoch 102| loss: 188.23107| val_0_rmse: 13.91639|  0:06:12s
epoch 103| loss: 191.6239| val_0_rmse: 13.88437|  0:06:15s
epoch 104| loss: 186.49785| val_0_rmse: 13.67355|  0:06:19s
epoch 105| loss: 185.68813| val_0_rmse: 13.73922|  0:06:22s
epoch 106| loss: 182.0924| val_0_rmse: 13.65766|  0:06:26s
epoch 107| loss: 181.79212| val_0_rmse: 13.75379|  0:06:30s
epoch 108| loss: 180.13486| val_0_rmse: 13.60574|  0:06:33s
epoch 109| loss: 174.06153| val_0_rmse: 13.43601|  0:06:37s
epoch 110| loss: 177.55794| val_0_rmse: 13.46141|  0:06:40s
epoch 111| loss: 172.05269| val_0_rmse: 13.57701|  0:06:44s
epoch 112| loss: 170.44321| val_0_rmse: 13.18492|  0:06:48s
epoch 113| loss: 169.08232| val_0_rmse: 13.18086|  0:06:51s
epoch 114| loss: 168.39416| val_0_rmse: 13.45884|  0:06:55s
epoch 115| loss: 168.80692| val_0_rmse: 13.49349|  0:06:58s
epoch 116| loss: 166.91277| val_0_rmse: 13.46473|  0:07:02s
epoch 117| loss: 165.43679| val_0_rmse: 13.08968|  0:07:06s
epoch 118| loss: 165.80132| val_0_rmse: 13.2638 |  0:07:09s
epoch 119| loss: 165.1095| val_0_rmse: 12.99977|  0:07:13s
epoch 120| loss: 162.97084| val_0_rmse: 12.95623|  0:07:17s
epoch 121| loss: 159.83666| val_0_rmse: 12.91521|  0:07:20s
epoch 122| loss: 160.04559| val_0_rmse: 13.15021|  0:07:24s
epoch 123| loss: 158.36656| val_0_rmse: 13.06791|  0:07:27s
epoch 124| loss: 160.4099| val_0_rmse: 13.41242|  0:07:31s
epoch 125| loss: 161.30486| val_0_rmse: 13.02228|  0:07:35s
epoch 126| loss: 158.02152| val_0_rmse: 13.15839|  0:07:38s
epoch 127| loss: 159.82304| val_0_rmse: 12.92112|  0:07:42s
epoch 128| loss: 154.43531| val_0_rmse: 12.8683 |  0:07:45s
epoch 129| loss: 155.82824| val_0_rmse: 13.07308|  0:07:49s
epoch 130| loss: 154.23095| val_0_rmse: 13.11647|  0:07:53s
epoch 131| loss: 152.63867| val_0_rmse: 12.98254|  0:07:56s
epoch 132| loss: 151.09796| val_0_rmse: 12.93929|  0:08:00s
epoch 133| loss: 150.18242| val_0_rmse: 13.10081|  0:08:03s
epoch 134| loss: 150.14809| val_0_rmse: 12.83269|  0:08:07s
epoch 135| loss: 151.46053| val_0_rmse: 13.09915|  0:08:11s
epoch 136| loss: 152.26573| val_0_rmse: 12.71802|  0:08:15s
epoch 137| loss: 148.13816| val_0_rmse: 12.7326 |  0:08:18s
epoch 138| loss: 146.05619| val_0_rmse: 12.49732|  0:08:22s
epoch 139| loss: 142.50214| val_0_rmse: 12.82033|  0:08:25s
epoch 140| loss: 142.20391| val_0_rmse: 12.66802|  0:08:29s
epoch 141| loss: 144.50516| val_0_rmse: 12.99523|  0:08:33s
epoch 142| loss: 144.31042| val_0_rmse: 12.7025 |  0:08:36s
epoch 143| loss: 141.21547| val_0_rmse: 12.68173|  0:08:40s
epoch 144| loss: 149.50403| val_0_rmse: 12.78623|  0:08:44s
epoch 145| loss: 139.08863| val_0_rmse: 12.65981|  0:08:47s
epoch 146| loss: 141.41406| val_0_rmse: 12.46026|  0:08:51s
epoch 147| loss: 140.66804| val_0_rmse: 12.6615 |  0:08:54s
epoch 148| loss: 137.92044| val_0_rmse: 12.85965|  0:08:58s
epoch 149| loss: 141.15016| val_0_rmse: 12.56633|  0:09:01s
epoch 150| loss: 137.47457| val_0_rmse: 12.62694|  0:09:05s
epoch 151| loss: 136.45588| val_0_rmse: 12.93858|  0:09:08s
epoch 152| loss: 140.51183| val_0_rmse: 12.79197|  0:09:12s
epoch 153| loss: 135.85609| val_0_rmse: 12.56043|  0:09:16s
epoch 154| loss: 135.83871| val_0_rmse: 12.57301|  0:09:19s
epoch 155| loss: 136.51486| val_0_rmse: 12.33388|  0:09:23s
epoch 156| loss: 135.45109| val_0_rmse: 12.67061|  0:09:26s
epoch 157| loss: 135.09892| val_0_rmse: 12.74378|  0:09:30s
epoch 158| loss: 133.91951| val_0_rmse: 12.84357|  0:09:33s
epoch 159| loss: 132.74818| val_0_rmse: 12.49351|  0:09:37s
epoch 160| loss: 134.12002| val_0_rmse: 12.63979|  0:09:40s
epoch 161| loss: 132.99064| val_0_rmse: 12.37838|  0:09:44s
epoch 162| loss: 128.44091| val_0_rmse: 12.39586|  0:09:47s
epoch 163| loss: 129.71676| val_0_rmse: 12.33065|  0:09:51s
epoch 164| loss: 132.1735| val_0_rmse: 12.61223|  0:09:54s
epoch 165| loss: 133.70777| val_0_rmse: 12.39792|  0:09:58s
epoch 166| loss: 127.99232| val_0_rmse: 12.46548|  0:10:01s
epoch 167| loss: 129.39849| val_0_rmse: 12.38545|  0:10:05s
epoch 168| loss: 127.33798| val_0_rmse: 12.45603|  0:10:08s
epoch 169| loss: 130.46153| val_0_rmse: 12.33837|  0:10:12s
epoch 170| loss: 128.16158| val_0_rmse: 12.52724|  0:10:15s
epoch 171| loss: 132.08211| val_0_rmse: 12.88978|  0:10:19s
epoch 172| loss: 125.93061| val_0_rmse: 12.25918|  0:10:23s
epoch 173| loss: 126.40278| val_0_rmse: 12.21952|  0:10:26s
epoch 174| loss: 125.64345| val_0_rmse: 12.32845|  0:10:30s
epoch 175| loss: 125.00255| val_0_rmse: 12.80571|  0:10:33s
epoch 176| loss: 122.64328| val_0_rmse: 12.57025|  0:10:37s
epoch 177| loss: 125.69767| val_0_rmse: 12.24975|  0:10:40s
epoch 178| loss: 121.41926| val_0_rmse: 12.61679|  0:10:44s
epoch 179| loss: 118.83659| val_0_rmse: 12.57425|  0:10:47s
epoch 180| loss: 123.18274| val_0_rmse: 12.12467|  0:10:51s
epoch 181| loss: 124.78901| val_0_rmse: 12.07814|  0:10:54s
epoch 182| loss: 117.97014| val_0_rmse: 12.65041|  0:10:58s
epoch 183| loss: 124.63367| val_0_rmse: 12.34746|  0:11:01s
epoch 184| loss: 119.93769| val_0_rmse: 12.52029|  0:11:05s
epoch 185| loss: 121.15143| val_0_rmse: 12.13108|  0:11:08s
epoch 186| loss: 118.81276| val_0_rmse: 12.02995|  0:11:12s
epoch 187| loss: 118.04512| val_0_rmse: 12.33458|  0:11:16s
epoch 188| loss: 116.1231| val_0_rmse: 12.30141|  0:11:19s
epoch 189| loss: 118.10882| val_0_rmse: 12.27112|  0:11:23s
epoch 190| loss: 116.29824| val_0_rmse: 12.28966|  0:11:26s
epoch 191| loss: 116.26534| val_0_rmse: 12.222  |  0:11:30s
epoch 192| loss: 117.06403| val_0_rmse: 12.31495|  0:11:33s
epoch 193| loss: 119.02452| val_0_rmse: 12.2222 |  0:11:37s
epoch 194| loss: 116.69829| val_0_rmse: 12.39354|  0:11:40s
epoch 195| loss: 116.82219| val_0_rmse: 12.75892|  0:11:44s
epoch 196| loss: 116.68411| val_0_rmse: 12.44234|  0:11:47s
epoch 197| loss: 116.92893| val_0_rmse: 12.18462|  0:11:51s
epoch 198| loss: 114.47965| val_0_rmse: 12.48375|  0:11:54s
epoch 199| loss: 117.99376| val_0_rmse: 12.29372|  0:11:58s
Stop training because you reached max_epochs = 200 with best_epoch = 186 and best_val_0_rmse = 12.02995
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 22:30:10,023] Trial 23 finished with value: 12.029952728946382 and parameters: {'lr': 0.002676391632282943, 'n_steps': 6, 'gamma': 1.3796599940471324, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.0219892141883805, 'weight_decay': 0.00014849052370054594, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 9066.49898| val_0_rmse: 97.27282|  0:00:04s
epoch 1  | loss: 8980.31284| val_0_rmse: 96.82118|  0:00:08s
epoch 2  | loss: 8919.92484| val_0_rmse: 95.48102|  0:00:13s
epoch 3  | loss: 8875.9223| val_0_rmse: 96.69154|  0:00:17s
epoch 4  | loss: 8835.76515| val_0_rmse: 94.79765|  0:00:21s
epoch 5  | loss: 8777.2483| val_0_rmse: 94.37972|  0:00:26s
epoch 6  | loss: 8688.6341| val_0_rmse: 93.31701|  0:00:30s
epoch 7  | loss: 8489.76362| val_0_rmse: 91.4767 |  0:00:34s
epoch 8  | loss: 7254.99974| val_0_rmse: 80.24317|  0:00:39s
epoch 9  | loss: 4800.23204| val_0_rmse: 58.20096|  0:00:43s
epoch 10 | loss: 2378.13148| val_0_rmse: 43.72121|  0:00:48s
epoch 11 | loss: 1491.3396| val_0_rmse: 35.43493|  0:00:52s
epoch 12 | loss: 1121.70896| val_0_rmse: 32.92218|  0:00:56s
epoch 13 | loss: 1024.70455| val_0_rmse: 32.0402 |  0:01:01s
epoch 14 | loss: 1006.01064| val_0_rmse: 31.53284|  0:01:05s
epoch 15 | loss: 995.45153| val_0_rmse: 31.72605|  0:01:10s
epoch 16 | loss: 978.00948| val_0_rmse: 31.69817|  0:01:14s
epoch 17 | loss: 956.75466| val_0_rmse: 30.89933|  0:01:18s
epoch 18 | loss: 940.67111| val_0_rmse: 31.47151|  0:01:23s
epoch 19 | loss: 939.17898| val_0_rmse: 30.59127|  0:01:27s
epoch 20 | loss: 931.69384| val_0_rmse: 31.18507|  0:01:31s
epoch 21 | loss: 918.64429| val_0_rmse: 31.05708|  0:01:36s
epoch 22 | loss: 909.70442| val_0_rmse: 30.46623|  0:01:40s
epoch 23 | loss: 907.47728| val_0_rmse: 30.43353|  0:01:44s
epoch 24 | loss: 903.12209| val_0_rmse: 29.94205|  0:01:49s
epoch 25 | loss: 908.33894| val_0_rmse: 30.24731|  0:01:53s
epoch 26 | loss: 901.52236| val_0_rmse: 30.4464 |  0:01:57s
epoch 27 | loss: 904.55989| val_0_rmse: 30.22846|  0:02:02s
epoch 28 | loss: 894.88022| val_0_rmse: 30.65753|  0:02:06s
epoch 29 | loss: 888.42408| val_0_rmse: 30.13765|  0:02:11s
epoch 30 | loss: 883.67904| val_0_rmse: 29.79083|  0:02:15s
epoch 31 | loss: 887.75448| val_0_rmse: 30.16139|  0:02:19s
epoch 32 | loss: 887.81632| val_0_rmse: 29.85987|  0:02:23s
epoch 33 | loss: 883.06111| val_0_rmse: 30.03965|  0:02:28s
epoch 34 | loss: 866.93284| val_0_rmse: 29.25259|  0:02:32s
epoch 35 | loss: 855.15553| val_0_rmse: 29.27913|  0:02:37s
epoch 36 | loss: 852.07147| val_0_rmse: 30.74864|  0:02:41s
epoch 37 | loss: 839.2355| val_0_rmse: 28.91704|  0:02:45s
epoch 38 | loss: 844.81709| val_0_rmse: 28.88996|  0:02:50s
epoch 39 | loss: 831.00663| val_0_rmse: 29.27763|  0:02:54s
epoch 40 | loss: 828.17019| val_0_rmse: 28.69107|  0:02:58s
epoch 41 | loss: 811.37538| val_0_rmse: 28.94347|  0:03:03s
epoch 42 | loss: 796.29598| val_0_rmse: 28.72584|  0:03:07s
epoch 43 | loss: 784.64925| val_0_rmse: 28.76916|  0:03:12s
epoch 44 | loss: 762.52957| val_0_rmse: 28.0569 |  0:03:16s
epoch 45 | loss: 723.62073| val_0_rmse: 27.79232|  0:03:20s
epoch 46 | loss: 709.26837| val_0_rmse: 26.90052|  0:03:25s
epoch 47 | loss: 674.88749| val_0_rmse: 26.22294|  0:03:29s
epoch 48 | loss: 655.84181| val_0_rmse: 25.3617 |  0:03:33s
epoch 49 | loss: 633.42655| val_0_rmse: 26.23964|  0:03:38s
epoch 50 | loss: 606.18892| val_0_rmse: 27.08642|  0:03:42s
epoch 51 | loss: 574.97656| val_0_rmse: 26.14964|  0:03:46s
epoch 52 | loss: 564.80169| val_0_rmse: 25.19002|  0:03:51s
epoch 53 | loss: 544.43268| val_0_rmse: 23.75546|  0:03:55s
epoch 54 | loss: 520.26693| val_0_rmse: 23.0728 |  0:03:59s
epoch 55 | loss: 503.04669| val_0_rmse: 22.72227|  0:04:04s
epoch 56 | loss: 495.24035| val_0_rmse: 21.8594 |  0:04:08s
epoch 57 | loss: 477.17768| val_0_rmse: 21.65648|  0:04:13s
epoch 58 | loss: 472.10816| val_0_rmse: 21.43119|  0:04:17s
epoch 59 | loss: 458.48983| val_0_rmse: 21.3497 |  0:04:21s
epoch 60 | loss: 444.35386| val_0_rmse: 21.00263|  0:04:26s
epoch 61 | loss: 441.2969| val_0_rmse: 20.89367|  0:04:30s
epoch 62 | loss: 425.52303| val_0_rmse: 20.44694|  0:04:34s
epoch 63 | loss: 424.50638| val_0_rmse: 20.09634|  0:04:39s
epoch 64 | loss: 411.16774| val_0_rmse: 21.17751|  0:04:43s
epoch 65 | loss: 400.98371| val_0_rmse: 20.32595|  0:04:48s
epoch 66 | loss: 395.47212| val_0_rmse: 19.63805|  0:04:52s
epoch 67 | loss: 383.18389| val_0_rmse: 24.81677|  0:04:56s
epoch 68 | loss: 375.28084| val_0_rmse: 24.19564|  0:05:01s
epoch 69 | loss: 371.91205| val_0_rmse: 19.45959|  0:05:05s
epoch 70 | loss: 357.8473| val_0_rmse: 20.06965|  0:05:09s
epoch 71 | loss: 349.89694| val_0_rmse: 19.86057|  0:05:14s
epoch 72 | loss: 343.87189| val_0_rmse: 18.75221|  0:05:18s
epoch 73 | loss: 332.22666| val_0_rmse: 18.14994|  0:05:22s
epoch 74 | loss: 331.82926| val_0_rmse: 18.61433|  0:05:27s
epoch 75 | loss: 325.70912| val_0_rmse: 18.67264|  0:05:31s
epoch 76 | loss: 314.22537| val_0_rmse: 19.44299|  0:05:35s
epoch 77 | loss: 305.93155| val_0_rmse: 18.40044|  0:05:40s
epoch 78 | loss: 299.35105| val_0_rmse: 18.67508|  0:05:44s
epoch 79 | loss: 294.70032| val_0_rmse: 18.08904|  0:05:49s
epoch 80 | loss: 285.1874| val_0_rmse: 17.03728|  0:05:53s
epoch 81 | loss: 283.5413| val_0_rmse: 16.77073|  0:05:57s
epoch 82 | loss: 279.72629| val_0_rmse: 16.55094|  0:06:02s
epoch 83 | loss: 277.16211| val_0_rmse: 16.49588|  0:06:06s
epoch 84 | loss: 272.86276| val_0_rmse: 16.58365|  0:06:10s
epoch 85 | loss: 268.43517| val_0_rmse: 16.57542|  0:06:15s
epoch 86 | loss: 258.67722| val_0_rmse: 16.16834|  0:06:19s
epoch 87 | loss: 259.01087| val_0_rmse: 16.32665|  0:06:23s
epoch 88 | loss: 257.79087| val_0_rmse: 16.01445|  0:06:28s
epoch 89 | loss: 247.74282| val_0_rmse: 16.03909|  0:06:32s
epoch 90 | loss: 247.67156| val_0_rmse: 16.16284|  0:06:36s
epoch 91 | loss: 245.53113| val_0_rmse: 15.94169|  0:06:40s
epoch 92 | loss: 235.97731| val_0_rmse: 15.53412|  0:06:45s
epoch 93 | loss: 233.13607| val_0_rmse: 15.35637|  0:06:49s
epoch 94 | loss: 230.53358| val_0_rmse: 15.34727|  0:06:53s
epoch 95 | loss: 229.6126| val_0_rmse: 15.6782 |  0:06:57s
epoch 96 | loss: 225.24409| val_0_rmse: 15.21506|  0:07:02s
epoch 97 | loss: 225.12602| val_0_rmse: 15.56934|  0:07:06s
epoch 98 | loss: 217.79807| val_0_rmse: 15.51409|  0:07:10s
epoch 99 | loss: 216.03564| val_0_rmse: 15.13152|  0:07:15s
epoch 100| loss: 211.42327| val_0_rmse: 15.02301|  0:07:19s
epoch 101| loss: 212.40396| val_0_rmse: 15.17462|  0:07:23s
epoch 102| loss: 208.91803| val_0_rmse: 15.04365|  0:07:28s
epoch 103| loss: 204.4507| val_0_rmse: 14.72235|  0:07:32s
epoch 104| loss: 204.98585| val_0_rmse: 14.69455|  0:07:36s
epoch 105| loss: 200.11218| val_0_rmse: 14.50423|  0:07:40s
epoch 106| loss: 200.2104| val_0_rmse: 14.19968|  0:07:45s
epoch 107| loss: 198.54407| val_0_rmse: 14.74825|  0:07:49s
epoch 108| loss: 197.32711| val_0_rmse: 14.7893 |  0:07:53s
epoch 109| loss: 191.8313| val_0_rmse: 14.29181|  0:07:57s
epoch 110| loss: 192.49978| val_0_rmse: 14.45018|  0:08:02s
epoch 111| loss: 193.86565| val_0_rmse: 14.21336|  0:08:06s
epoch 112| loss: 186.37851| val_0_rmse: 14.23761|  0:08:10s
epoch 113| loss: 192.01447| val_0_rmse: 14.20507|  0:08:15s
epoch 114| loss: 184.4529| val_0_rmse: 14.02525|  0:08:19s
epoch 115| loss: 186.07912| val_0_rmse: 14.20239|  0:08:23s
epoch 116| loss: 178.398 | val_0_rmse: 13.98551|  0:08:27s
epoch 117| loss: 177.25093| val_0_rmse: 13.78683|  0:08:32s
epoch 118| loss: 179.69229| val_0_rmse: 14.00924|  0:08:36s
epoch 119| loss: 181.09548| val_0_rmse: 13.74283|  0:08:40s
epoch 120| loss: 179.34069| val_0_rmse: 13.76185|  0:08:45s
epoch 121| loss: 171.83419| val_0_rmse: 14.26084|  0:08:49s
epoch 122| loss: 171.90972| val_0_rmse: 13.65647|  0:08:53s
epoch 123| loss: 170.07938| val_0_rmse: 13.75691|  0:08:57s
epoch 124| loss: 171.20699| val_0_rmse: 14.13439|  0:09:02s
epoch 125| loss: 168.62009| val_0_rmse: 13.24921|  0:09:06s
epoch 126| loss: 165.28102| val_0_rmse: 13.38858|  0:09:10s
epoch 127| loss: 169.17566| val_0_rmse: 13.48119|  0:09:15s
epoch 128| loss: 160.2325| val_0_rmse: 13.63761|  0:09:19s
epoch 129| loss: 159.26718| val_0_rmse: 13.40656|  0:09:23s
epoch 130| loss: 160.66036| val_0_rmse: 13.41272|  0:09:27s
epoch 131| loss: 161.69351| val_0_rmse: 13.37287|  0:09:32s
epoch 132| loss: 160.53091| val_0_rmse: 13.39668|  0:09:36s
epoch 133| loss: 156.26332| val_0_rmse: 13.32072|  0:09:40s
epoch 134| loss: 157.1095| val_0_rmse: 13.24   |  0:09:45s
epoch 135| loss: 158.34927| val_0_rmse: 13.597  |  0:09:49s
epoch 136| loss: 154.47731| val_0_rmse: 13.6083 |  0:09:53s
epoch 137| loss: 152.56524| val_0_rmse: 13.48452|  0:09:57s
epoch 138| loss: 151.61355| val_0_rmse: 13.63807|  0:10:01s
epoch 139| loss: 150.40373| val_0_rmse: 13.64719|  0:10:06s
epoch 140| loss: 148.45345| val_0_rmse: 13.29282|  0:10:10s
epoch 141| loss: 149.12323| val_0_rmse: 13.2905 |  0:10:14s
epoch 142| loss: 147.99652| val_0_rmse: 13.29307|  0:10:19s
epoch 143| loss: 146.20998| val_0_rmse: 13.11289|  0:10:23s
epoch 144| loss: 142.54164| val_0_rmse: 13.21893|  0:10:27s
epoch 145| loss: 142.56062| val_0_rmse: 13.45309|  0:10:31s
epoch 146| loss: 145.42251| val_0_rmse: 13.43768|  0:10:36s
epoch 147| loss: 139.15693| val_0_rmse: 13.48683|  0:10:40s
epoch 148| loss: 144.56516| val_0_rmse: 13.47022|  0:10:44s
epoch 149| loss: 145.36148| val_0_rmse: 13.30583|  0:10:48s
epoch 150| loss: 142.93628| val_0_rmse: 13.22831|  0:10:53s
epoch 151| loss: 138.89747| val_0_rmse: 13.33355|  0:10:57s
epoch 152| loss: 130.81445| val_0_rmse: 13.24955|  0:11:01s
epoch 153| loss: 135.39535| val_0_rmse: 13.91641|  0:11:05s
epoch 154| loss: 132.89435| val_0_rmse: 13.75554|  0:11:10s
epoch 155| loss: 135.13103| val_0_rmse: 13.48418|  0:11:14s
epoch 156| loss: 133.0516| val_0_rmse: 13.3594 |  0:11:18s
epoch 157| loss: 131.68664| val_0_rmse: 13.15122|  0:11:22s
epoch 158| loss: 133.19256| val_0_rmse: 13.09968|  0:11:27s
epoch 159| loss: 133.92238| val_0_rmse: 13.01141|  0:11:31s
epoch 160| loss: 130.86677| val_0_rmse: 13.00488|  0:11:35s
epoch 161| loss: 130.43642| val_0_rmse: 13.20449|  0:11:40s
epoch 162| loss: 130.195 | val_0_rmse: 12.7858 |  0:11:44s
epoch 163| loss: 128.64397| val_0_rmse: 13.37725|  0:11:48s
epoch 164| loss: 129.2586| val_0_rmse: 13.01327|  0:11:52s
epoch 165| loss: 126.55777| val_0_rmse: 12.94767|  0:11:57s
epoch 166| loss: 128.65321| val_0_rmse: 13.39882|  0:12:01s
epoch 167| loss: 125.21821| val_0_rmse: 13.08173|  0:12:05s
epoch 168| loss: 126.6181| val_0_rmse: 13.01526|  0:12:10s
epoch 169| loss: 130.22507| val_0_rmse: 12.58631|  0:12:14s
epoch 170| loss: 125.30424| val_0_rmse: 12.8325 |  0:12:18s
epoch 171| loss: 124.03803| val_0_rmse: 12.60383|  0:12:22s
epoch 172| loss: 126.70286| val_0_rmse: 12.80963|  0:12:27s
epoch 173| loss: 121.21548| val_0_rmse: 12.79897|  0:12:31s
epoch 174| loss: 119.32504| val_0_rmse: 12.84227|  0:12:35s
epoch 175| loss: 123.41862| val_0_rmse: 12.83697|  0:12:39s
epoch 176| loss: 119.81588| val_0_rmse: 12.77794|  0:12:44s
epoch 177| loss: 121.41605| val_0_rmse: 12.74985|  0:12:48s
epoch 178| loss: 120.03485| val_0_rmse: 12.72928|  0:12:52s
epoch 179| loss: 120.2058| val_0_rmse: 12.79315|  0:12:56s
epoch 180| loss: 118.0281| val_0_rmse: 12.86933|  0:13:01s
epoch 181| loss: 117.03394| val_0_rmse: 12.96276|  0:13:05s
epoch 182| loss: 120.96915| val_0_rmse: 12.82452|  0:13:09s
epoch 183| loss: 116.27506| val_0_rmse: 12.79127|  0:13:14s
epoch 184| loss: 114.20942| val_0_rmse: 12.87504|  0:13:18s
epoch 185| loss: 115.56483| val_0_rmse: 12.52159|  0:13:22s
epoch 186| loss: 113.14815| val_0_rmse: 12.41062|  0:13:26s
epoch 187| loss: 114.27962| val_0_rmse: 12.79215|  0:13:31s
epoch 188| loss: 113.00136| val_0_rmse: 12.51213|  0:13:35s
epoch 189| loss: 113.28595| val_0_rmse: 12.64622|  0:13:39s
epoch 190| loss: 111.32592| val_0_rmse: 12.42898|  0:13:44s
epoch 191| loss: 115.42962| val_0_rmse: 12.57514|  0:13:48s
epoch 192| loss: 112.09035| val_0_rmse: 12.52696|  0:13:52s
epoch 193| loss: 109.88552| val_0_rmse: 12.52803|  0:13:56s
epoch 194| loss: 113.17321| val_0_rmse: 12.44114|  0:14:01s
epoch 195| loss: 109.69456| val_0_rmse: 12.79649|  0:14:05s
epoch 196| loss: 111.20814| val_0_rmse: 12.57991|  0:14:09s
epoch 197| loss: 110.88453| val_0_rmse: 12.4131 |  0:14:13s
epoch 198| loss: 108.88066| val_0_rmse: 12.46409|  0:14:18s
epoch 199| loss: 107.143 | val_0_rmse: 12.3308 |  0:14:22s
Stop training because you reached max_epochs = 200 with best_epoch = 199 and best_val_0_rmse = 12.3308
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 22:44:42,230] Trial 24 finished with value: 12.330798007716659 and parameters: {'lr': 0.0026521281068127914, 'n_steps': 8, 'gamma': 1.4424748841822737, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.034096899496192386, 'weight_decay': 6.741824465490987e-05, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8494.08028| val_0_rmse: 93.89973|  0:00:03s
epoch 1  | loss: 8651.26233| val_0_rmse: 94.4109 |  0:00:07s
epoch 2  | loss: 8662.08268| val_0_rmse: 94.20032|  0:00:11s
epoch 3  | loss: 8482.24956| val_0_rmse: 91.15609|  0:00:15s
epoch 4  | loss: 7073.33247| val_0_rmse: 72.52259|  0:00:19s
epoch 5  | loss: 3183.76017| val_0_rmse: 40.5411 |  0:00:23s
epoch 6  | loss: 1190.57687| val_0_rmse: 33.76928|  0:00:27s
epoch 7  | loss: 1096.55795| val_0_rmse: 34.07743|  0:00:30s
epoch 8  | loss: 1086.76044| val_0_rmse: 33.67338|  0:00:34s
epoch 9  | loss: 1070.88207| val_0_rmse: 32.81392|  0:00:38s
epoch 10 | loss: 1059.46996| val_0_rmse: 32.70922|  0:00:42s
epoch 11 | loss: 1028.98766| val_0_rmse: 31.21522|  0:00:46s
epoch 12 | loss: 988.68829| val_0_rmse: 31.04237|  0:00:50s
epoch 13 | loss: 973.3347| val_0_rmse: 31.34226|  0:00:54s
epoch 14 | loss: 968.08652| val_0_rmse: 30.81513|  0:00:58s
epoch 15 | loss: 945.1889| val_0_rmse: 30.69026|  0:01:01s
epoch 16 | loss: 944.31968| val_0_rmse: 31.32621|  0:01:06s
epoch 17 | loss: 933.08972| val_0_rmse: 30.70161|  0:01:09s
epoch 18 | loss: 918.41301| val_0_rmse: 30.34458|  0:01:13s
epoch 19 | loss: 885.51578| val_0_rmse: 29.92065|  0:01:17s
epoch 20 | loss: 850.4465| val_0_rmse: 29.71009|  0:01:21s
epoch 21 | loss: 802.97005| val_0_rmse: 28.72775|  0:01:25s
epoch 22 | loss: 751.83841| val_0_rmse: 27.9242 |  0:01:28s
epoch 23 | loss: 687.41218| val_0_rmse: 27.61163|  0:01:32s
epoch 24 | loss: 638.47148| val_0_rmse: 27.76424|  0:01:36s
epoch 25 | loss: 588.87691| val_0_rmse: 27.05557|  0:01:40s
epoch 26 | loss: 546.59704| val_0_rmse: 24.75201|  0:01:44s
epoch 27 | loss: 514.47895| val_0_rmse: 31.40085|  0:01:47s
epoch 28 | loss: 491.69116| val_0_rmse: 27.23787|  0:01:51s
epoch 29 | loss: 468.94133| val_0_rmse: 31.43242|  0:01:55s
epoch 30 | loss: 445.2508| val_0_rmse: 25.84112|  0:01:59s
epoch 31 | loss: 420.72302| val_0_rmse: 26.59556|  0:02:02s
epoch 32 | loss: 403.31571| val_0_rmse: 24.26609|  0:02:06s
epoch 33 | loss: 388.68009| val_0_rmse: 23.2927 |  0:02:10s
epoch 34 | loss: 372.14247| val_0_rmse: 23.02504|  0:02:14s
epoch 35 | loss: 353.6704| val_0_rmse: 23.61838|  0:02:18s
epoch 36 | loss: 334.19092| val_0_rmse: 25.25097|  0:02:21s
epoch 37 | loss: 323.67892| val_0_rmse: 20.34379|  0:02:25s
epoch 38 | loss: 321.20739| val_0_rmse: 23.07667|  0:02:29s
epoch 39 | loss: 305.07132| val_0_rmse: 25.65664|  0:02:33s
epoch 40 | loss: 298.88005| val_0_rmse: 23.47009|  0:02:37s
epoch 41 | loss: 286.91505| val_0_rmse: 21.79006|  0:02:41s
epoch 42 | loss: 279.72071| val_0_rmse: 22.69256|  0:02:44s
epoch 43 | loss: 275.699 | val_0_rmse: 23.56218|  0:02:48s
epoch 44 | loss: 267.84657| val_0_rmse: 24.64465|  0:02:52s
epoch 45 | loss: 261.09109| val_0_rmse: 21.06437|  0:02:56s
epoch 46 | loss: 260.98417| val_0_rmse: 20.56638|  0:02:59s
epoch 47 | loss: 249.06608| val_0_rmse: 21.02611|  0:03:03s
epoch 48 | loss: 247.20168| val_0_rmse: 23.58047|  0:03:07s
epoch 49 | loss: 239.56996| val_0_rmse: 18.26007|  0:03:11s
epoch 50 | loss: 232.38426| val_0_rmse: 17.52466|  0:03:15s
epoch 51 | loss: 230.97982| val_0_rmse: 20.08669|  0:03:18s
epoch 52 | loss: 232.51513| val_0_rmse: 21.96554|  0:03:22s
epoch 53 | loss: 228.21704| val_0_rmse: 21.41077|  0:03:26s
epoch 54 | loss: 215.89441| val_0_rmse: 20.39176|  0:03:30s
epoch 55 | loss: 217.27135| val_0_rmse: 24.02703|  0:03:34s
epoch 56 | loss: 215.27031| val_0_rmse: 24.66452|  0:03:37s
epoch 57 | loss: 207.29902| val_0_rmse: 25.65671|  0:03:41s
epoch 58 | loss: 213.94287| val_0_rmse: 21.61105|  0:03:45s
epoch 59 | loss: 203.62893| val_0_rmse: 22.47433|  0:03:49s
epoch 60 | loss: 202.64754| val_0_rmse: 20.21879|  0:03:52s
epoch 61 | loss: 197.05094| val_0_rmse: 20.44464|  0:03:56s
epoch 62 | loss: 199.2584| val_0_rmse: 19.69342|  0:04:00s
epoch 63 | loss: 194.45233| val_0_rmse: 19.46766|  0:04:04s
epoch 64 | loss: 187.68219| val_0_rmse: 18.49549|  0:04:08s
epoch 65 | loss: 184.91697| val_0_rmse: 18.67999|  0:04:12s
epoch 66 | loss: 186.90897| val_0_rmse: 18.19779|  0:04:15s
epoch 67 | loss: 184.84157| val_0_rmse: 19.15959|  0:04:19s
epoch 68 | loss: 182.02734| val_0_rmse: 21.4104 |  0:04:23s
epoch 69 | loss: 176.07077| val_0_rmse: 17.89362|  0:04:27s
epoch 70 | loss: 176.3623| val_0_rmse: 16.49078|  0:04:30s
epoch 71 | loss: 178.50951| val_0_rmse: 17.9944 |  0:04:34s
epoch 72 | loss: 172.72404| val_0_rmse: 17.62032|  0:04:38s
epoch 73 | loss: 166.36826| val_0_rmse: 13.89296|  0:04:42s
epoch 74 | loss: 170.09197| val_0_rmse: 13.56707|  0:04:46s
epoch 75 | loss: 175.98386| val_0_rmse: 22.50371|  0:04:49s
epoch 76 | loss: 162.11549| val_0_rmse: 20.93935|  0:04:53s
epoch 77 | loss: 165.61188| val_0_rmse: 20.44304|  0:04:57s
epoch 78 | loss: 160.6449| val_0_rmse: 22.23012|  0:05:01s
epoch 79 | loss: 159.23831| val_0_rmse: 20.52078|  0:05:05s
epoch 80 | loss: 162.30873| val_0_rmse: 20.02544|  0:05:08s
epoch 81 | loss: 163.23003| val_0_rmse: 20.0801 |  0:05:12s
epoch 82 | loss: 158.04549| val_0_rmse: 13.08367|  0:05:16s
epoch 83 | loss: 153.63911| val_0_rmse: 13.31922|  0:05:20s
epoch 84 | loss: 157.03095| val_0_rmse: 13.05069|  0:05:23s
epoch 85 | loss: 152.57089| val_0_rmse: 13.10654|  0:05:27s
epoch 86 | loss: 156.50344| val_0_rmse: 13.26521|  0:05:31s
epoch 87 | loss: 154.11766| val_0_rmse: 12.88873|  0:05:35s
epoch 88 | loss: 151.16028| val_0_rmse: 13.05284|  0:05:39s
epoch 89 | loss: 146.59442| val_0_rmse: 13.0533 |  0:05:43s
epoch 90 | loss: 147.92523| val_0_rmse: 12.91862|  0:05:46s
epoch 91 | loss: 142.78489| val_0_rmse: 12.96583|  0:05:50s
epoch 92 | loss: 144.70317| val_0_rmse: 13.02018|  0:05:54s
epoch 93 | loss: 145.03056| val_0_rmse: 12.97753|  0:05:58s
epoch 94 | loss: 144.95854| val_0_rmse: 12.94093|  0:06:01s
epoch 95 | loss: 143.97886| val_0_rmse: 12.75153|  0:06:05s
epoch 96 | loss: 142.76518| val_0_rmse: 12.77973|  0:06:09s
epoch 97 | loss: 147.43463| val_0_rmse: 12.78714|  0:06:13s
epoch 98 | loss: 139.59275| val_0_rmse: 12.92607|  0:06:17s
epoch 99 | loss: 138.60419| val_0_rmse: 13.06173|  0:06:20s
epoch 100| loss: 138.71174| val_0_rmse: 12.87367|  0:06:24s
epoch 101| loss: 140.62805| val_0_rmse: 12.77131|  0:06:28s
epoch 102| loss: 132.15224| val_0_rmse: 12.97139|  0:06:32s
epoch 103| loss: 133.14961| val_0_rmse: 12.94254|  0:06:36s
epoch 104| loss: 136.23143| val_0_rmse: 13.22435|  0:06:39s
epoch 105| loss: 134.48434| val_0_rmse: 12.95279|  0:06:43s
epoch 106| loss: 131.84515| val_0_rmse: 12.73799|  0:06:47s
epoch 107| loss: 130.3145| val_0_rmse: 12.68538|  0:06:51s
epoch 108| loss: 127.41727| val_0_rmse: 12.90399|  0:06:55s
epoch 109| loss: 127.49825| val_0_rmse: 12.79587|  0:06:58s
epoch 110| loss: 128.25737| val_0_rmse: 12.76779|  0:07:02s
epoch 111| loss: 129.62297| val_0_rmse: 12.52276|  0:07:06s
epoch 112| loss: 124.92156| val_0_rmse: 12.61937|  0:07:10s
epoch 113| loss: 124.83434| val_0_rmse: 12.78412|  0:07:14s
epoch 114| loss: 123.0736| val_0_rmse: 12.61858|  0:07:17s
epoch 115| loss: 126.11675| val_0_rmse: 12.65972|  0:07:21s
epoch 116| loss: 125.57235| val_0_rmse: 12.67099|  0:07:25s
epoch 117| loss: 125.76819| val_0_rmse: 12.56971|  0:07:29s
epoch 118| loss: 121.22917| val_0_rmse: 12.47434|  0:07:33s
epoch 119| loss: 124.47964| val_0_rmse: 12.99631|  0:07:37s
epoch 120| loss: 122.49384| val_0_rmse: 12.93071|  0:07:40s
epoch 121| loss: 123.16935| val_0_rmse: 12.57888|  0:07:44s
epoch 122| loss: 120.18113| val_0_rmse: 12.27386|  0:07:48s
epoch 123| loss: 118.37407| val_0_rmse: 12.22577|  0:07:52s
epoch 124| loss: 120.33898| val_0_rmse: 12.63838|  0:07:55s
epoch 125| loss: 118.75703| val_0_rmse: 12.5473 |  0:07:59s
epoch 126| loss: 120.86409| val_0_rmse: 12.71793|  0:08:03s
epoch 127| loss: 117.99357| val_0_rmse: 12.51256|  0:08:07s
epoch 128| loss: 116.54126| val_0_rmse: 12.57899|  0:08:11s
epoch 129| loss: 114.9525| val_0_rmse: 12.53065|  0:08:14s
epoch 130| loss: 113.09766| val_0_rmse: 12.44131|  0:08:18s
epoch 131| loss: 115.00219| val_0_rmse: 12.446  |  0:08:22s
epoch 132| loss: 111.52734| val_0_rmse: 12.48388|  0:08:26s
epoch 133| loss: 116.25698| val_0_rmse: 12.91621|  0:08:29s
epoch 134| loss: 116.27235| val_0_rmse: 12.22841|  0:08:33s
epoch 135| loss: 115.12576| val_0_rmse: 12.60504|  0:08:37s
epoch 136| loss: 112.65646| val_0_rmse: 12.63305|  0:08:41s
epoch 137| loss: 113.02088| val_0_rmse: 12.55219|  0:08:45s
epoch 138| loss: 116.10417| val_0_rmse: 12.49222|  0:08:49s
epoch 139| loss: 109.83703| val_0_rmse: 12.43451|  0:08:52s
epoch 140| loss: 113.1635| val_0_rmse: 12.44479|  0:08:56s
epoch 141| loss: 113.67834| val_0_rmse: 12.22783|  0:09:00s
epoch 142| loss: 114.32488| val_0_rmse: 12.40771|  0:09:04s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 143| loss: 111.05611| val_0_rmse: 12.69879|  0:09:08s
Early stopping occurred at epoch 143 with best_epoch = 123 and best_val_0_rmse = 12.22577
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 22:53:59,357] Trial 25 finished with value: 12.22576744352823 and parameters: {'lr': 0.004644704592363735, 'n_steps': 8, 'gamma': 1.2910575675760294, 'n_independent': 1, 'n_shared': 3, 'momentum': 0.016950658094217794, 'weight_decay': 4.972712749022371e-05, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8800.41448| val_0_rmse: 93.90914|  0:00:05s
epoch 1  | loss: 8520.89427| val_0_rmse: 91.41406|  0:00:10s
epoch 2  | loss: 6879.33225| val_0_rmse: 70.90572|  0:00:15s
epoch 3  | loss: 2834.45773| val_0_rmse: 41.45144|  0:00:20s
epoch 4  | loss: 1164.06125| val_0_rmse: 32.85298|  0:00:25s
epoch 5  | loss: 1033.42276| val_0_rmse: 33.90759|  0:00:31s
epoch 6  | loss: 1001.8123| val_0_rmse: 31.78474|  0:00:36s
epoch 7  | loss: 977.32534| val_0_rmse: 33.46201|  0:00:41s
epoch 8  | loss: 960.84836| val_0_rmse: 33.75896|  0:00:46s
epoch 9  | loss: 943.50286| val_0_rmse: 32.39658|  0:00:51s
epoch 10 | loss: 926.34845| val_0_rmse: 32.36602|  0:00:56s
epoch 11 | loss: 888.98688| val_0_rmse: 31.81656|  0:01:02s
epoch 12 | loss: 866.77536| val_0_rmse: 31.40109|  0:01:07s
epoch 13 | loss: 848.07745| val_0_rmse: 31.27943|  0:01:12s
epoch 14 | loss: 788.42574| val_0_rmse: 28.43756|  0:01:17s
epoch 15 | loss: 684.03744| val_0_rmse: 32.50908|  0:01:22s
epoch 16 | loss: 620.69688| val_0_rmse: 25.58584|  0:01:27s
epoch 17 | loss: 566.03326| val_0_rmse: 23.86261|  0:01:32s
epoch 18 | loss: 519.8092| val_0_rmse: 32.52069|  0:01:37s
epoch 19 | loss: 489.28135| val_0_rmse: 38.78079|  0:01:42s
epoch 20 | loss: 463.2987| val_0_rmse: 25.1098 |  0:01:48s
epoch 21 | loss: 442.10462| val_0_rmse: 33.20037|  0:01:53s
epoch 22 | loss: 420.11427| val_0_rmse: 20.40657|  0:01:58s
epoch 23 | loss: 395.32192| val_0_rmse: 26.89334|  0:02:03s
epoch 24 | loss: 374.86746| val_0_rmse: 26.56084|  0:02:08s
epoch 25 | loss: 358.61224| val_0_rmse: 26.87889|  0:02:13s
epoch 26 | loss: 345.7671| val_0_rmse: 25.63048|  0:02:19s
epoch 27 | loss: 327.5266| val_0_rmse: 23.40363|  0:02:24s
epoch 28 | loss: 318.79919| val_0_rmse: 20.57597|  0:02:29s
epoch 29 | loss: 309.64224| val_0_rmse: 23.56265|  0:02:34s
epoch 30 | loss: 295.71953| val_0_rmse: 17.7804 |  0:02:39s
epoch 31 | loss: 286.58829| val_0_rmse: 17.06854|  0:02:44s
epoch 32 | loss: 277.25965| val_0_rmse: 18.03416|  0:02:49s
epoch 33 | loss: 268.31535| val_0_rmse: 16.87539|  0:02:54s
epoch 34 | loss: 264.67696| val_0_rmse: 16.94376|  0:02:59s
epoch 35 | loss: 255.83056| val_0_rmse: 18.40268|  0:03:04s
epoch 36 | loss: 247.53875| val_0_rmse: 15.92145|  0:03:09s
epoch 37 | loss: 239.65093| val_0_rmse: 16.04673|  0:03:15s
epoch 38 | loss: 236.5823| val_0_rmse: 18.04922|  0:03:20s
epoch 39 | loss: 233.31802| val_0_rmse: 15.73513|  0:03:25s
epoch 40 | loss: 229.94282| val_0_rmse: 16.05487|  0:03:30s
epoch 41 | loss: 217.13049| val_0_rmse: 15.49926|  0:03:35s
epoch 42 | loss: 213.76667| val_0_rmse: 15.4383 |  0:03:40s
epoch 43 | loss: 213.22844| val_0_rmse: 14.86429|  0:03:45s
epoch 44 | loss: 205.88347| val_0_rmse: 15.37268|  0:03:50s
epoch 45 | loss: 203.64611| val_0_rmse: 15.21097|  0:03:55s
epoch 46 | loss: 197.21016| val_0_rmse: 14.78672|  0:04:00s
epoch 47 | loss: 197.28066| val_0_rmse: 14.83676|  0:04:05s
epoch 48 | loss: 193.40123| val_0_rmse: 14.94917|  0:04:11s
epoch 49 | loss: 190.34887| val_0_rmse: 15.1539 |  0:04:16s
epoch 50 | loss: 190.44884| val_0_rmse: 15.05062|  0:04:21s
epoch 51 | loss: 185.77946| val_0_rmse: 14.75086|  0:04:26s
epoch 52 | loss: 187.28797| val_0_rmse: 14.26503|  0:04:31s
epoch 53 | loss: 178.83907| val_0_rmse: 15.51177|  0:04:36s
epoch 54 | loss: 178.61263| val_0_rmse: 14.60173|  0:04:41s
epoch 55 | loss: 175.90324| val_0_rmse: 14.84103|  0:04:46s
epoch 56 | loss: 169.93995| val_0_rmse: 14.32459|  0:04:51s
epoch 57 | loss: 171.67242| val_0_rmse: 14.56148|  0:04:56s
epoch 58 | loss: 170.71247| val_0_rmse: 14.02331|  0:05:01s
epoch 59 | loss: 165.75925| val_0_rmse: 14.63341|  0:05:06s
epoch 60 | loss: 161.55215| val_0_rmse: 14.73053|  0:05:12s
epoch 61 | loss: 159.78808| val_0_rmse: 13.93081|  0:05:17s
epoch 62 | loss: 164.68905| val_0_rmse: 13.90119|  0:05:22s
epoch 63 | loss: 161.27139| val_0_rmse: 13.87156|  0:05:27s
epoch 64 | loss: 167.09783| val_0_rmse: 14.14426|  0:05:32s
epoch 65 | loss: 155.91523| val_0_rmse: 13.93939|  0:05:37s
epoch 66 | loss: 152.20616| val_0_rmse: 13.78709|  0:05:42s
epoch 67 | loss: 153.36568| val_0_rmse: 13.79244|  0:05:48s
epoch 68 | loss: 151.02948| val_0_rmse: 13.56214|  0:05:53s
epoch 69 | loss: 153.25804| val_0_rmse: 13.57542|  0:05:58s
epoch 70 | loss: 151.41491| val_0_rmse: 13.71487|  0:06:03s
epoch 71 | loss: 150.68919| val_0_rmse: 13.50146|  0:06:08s
epoch 72 | loss: 147.84575| val_0_rmse: 13.81422|  0:06:13s
epoch 73 | loss: 145.4727| val_0_rmse: 13.58811|  0:06:18s
epoch 74 | loss: 144.88886| val_0_rmse: 14.05639|  0:06:23s
epoch 75 | loss: 142.5259| val_0_rmse: 13.26339|  0:06:29s
epoch 76 | loss: 138.5485| val_0_rmse: 13.69956|  0:06:34s
epoch 77 | loss: 137.46422| val_0_rmse: 13.3456 |  0:06:39s
epoch 78 | loss: 136.94151| val_0_rmse: 13.69021|  0:06:44s
epoch 79 | loss: 138.46147| val_0_rmse: 13.57071|  0:06:49s
epoch 80 | loss: 135.02519| val_0_rmse: 13.29708|  0:06:54s
epoch 81 | loss: 135.74394| val_0_rmse: 13.50405|  0:06:59s
epoch 82 | loss: 133.15457| val_0_rmse: 13.3697 |  0:07:04s
epoch 83 | loss: 131.21414| val_0_rmse: 13.88662|  0:07:09s
epoch 84 | loss: 129.86954| val_0_rmse: 14.11256|  0:07:14s
epoch 85 | loss: 129.90902| val_0_rmse: 14.11029|  0:07:20s
epoch 86 | loss: 134.92739| val_0_rmse: 13.29024|  0:07:25s
epoch 87 | loss: 130.59213| val_0_rmse: 13.40099|  0:07:30s
epoch 88 | loss: 124.47025| val_0_rmse: 13.06782|  0:07:35s
epoch 89 | loss: 126.39253| val_0_rmse: 13.12247|  0:07:40s
epoch 90 | loss: 125.80915| val_0_rmse: 13.28318|  0:07:45s
epoch 91 | loss: 128.42409| val_0_rmse: 13.47681|  0:07:50s
epoch 92 | loss: 123.53602| val_0_rmse: 13.07197|  0:07:55s
epoch 93 | loss: 121.98074| val_0_rmse: 13.26263|  0:08:00s
epoch 94 | loss: 123.58295| val_0_rmse: 13.46769|  0:08:05s
epoch 95 | loss: 121.27022| val_0_rmse: 12.89074|  0:08:10s
epoch 96 | loss: 121.70732| val_0_rmse: 13.4294 |  0:08:15s
epoch 97 | loss: 123.35862| val_0_rmse: 13.212  |  0:08:21s
epoch 98 | loss: 120.63188| val_0_rmse: 13.0727 |  0:08:26s
epoch 99 | loss: 120.08826| val_0_rmse: 13.09962|  0:08:31s
epoch 100| loss: 119.86862| val_0_rmse: 13.258  |  0:08:36s
epoch 101| loss: 123.8984| val_0_rmse: 12.894  |  0:08:41s
epoch 102| loss: 119.02785| val_0_rmse: 12.97652|  0:08:46s
epoch 103| loss: 117.20682| val_0_rmse: 13.21133|  0:08:51s
epoch 104| loss: 118.9186| val_0_rmse: 14.39233|  0:08:56s
epoch 105| loss: 114.10225| val_0_rmse: 13.04927|  0:09:01s
epoch 106| loss: 119.52439| val_0_rmse: 13.01333|  0:09:06s
epoch 107| loss: 115.31525| val_0_rmse: 12.7566 |  0:09:11s
epoch 108| loss: 114.59767| val_0_rmse: 12.64099|  0:09:16s
epoch 109| loss: 114.34282| val_0_rmse: 12.79136|  0:09:22s
epoch 110| loss: 111.81398| val_0_rmse: 12.68764|  0:09:27s
epoch 111| loss: 113.90908| val_0_rmse: 12.69326|  0:09:32s
epoch 112| loss: 115.34881| val_0_rmse: 12.76905|  0:09:37s
epoch 113| loss: 113.24964| val_0_rmse: 12.56554|  0:09:42s
epoch 114| loss: 117.49141| val_0_rmse: 12.84742|  0:09:47s
epoch 115| loss: 111.80313| val_0_rmse: 12.94736|  0:09:52s
epoch 116| loss: 113.2181| val_0_rmse: 12.74257|  0:09:57s
epoch 117| loss: 112.62273| val_0_rmse: 12.76282|  0:10:02s
epoch 118| loss: 112.85132| val_0_rmse: 13.10067|  0:10:07s
epoch 119| loss: 111.03104| val_0_rmse: 12.60373|  0:10:13s
epoch 120| loss: 111.6583| val_0_rmse: 12.60313|  0:10:18s
epoch 121| loss: 110.41386| val_0_rmse: 12.82061|  0:10:23s
epoch 122| loss: 109.71669| val_0_rmse: 12.75527|  0:10:28s
epoch 123| loss: 109.15634| val_0_rmse: 12.60431|  0:10:33s
epoch 124| loss: 109.26367| val_0_rmse: 12.87322|  0:10:38s
epoch 125| loss: 110.27925| val_0_rmse: 12.59156|  0:10:43s
epoch 126| loss: 107.25187| val_0_rmse: 13.84397|  0:10:49s
epoch 127| loss: 104.80027| val_0_rmse: 12.7759 |  0:10:54s
epoch 128| loss: 107.21481| val_0_rmse: 12.61898|  0:10:59s
epoch 129| loss: 106.68013| val_0_rmse: 12.56992|  0:11:04s
epoch 130| loss: 106.15029| val_0_rmse: 12.6265 |  0:11:09s
epoch 131| loss: 104.60917| val_0_rmse: 12.30044|  0:11:14s
epoch 132| loss: 103.15587| val_0_rmse: 12.34966|  0:11:19s
epoch 133| loss: 103.23246| val_0_rmse: 12.27334|  0:11:24s
epoch 134| loss: 107.55196| val_0_rmse: 12.66899|  0:11:29s
epoch 135| loss: 103.82057| val_0_rmse: 12.40138|  0:11:34s
epoch 136| loss: 105.81135| val_0_rmse: 12.31624|  0:11:39s
epoch 137| loss: 102.83092| val_0_rmse: 12.75535|  0:11:44s
epoch 138| loss: 105.49415| val_0_rmse: 12.3308 |  0:11:50s
epoch 139| loss: 102.23827| val_0_rmse: 12.48004|  0:11:55s
epoch 140| loss: 102.25442| val_0_rmse: 12.40167|  0:12:00s
epoch 141| loss: 99.01067| val_0_rmse: 12.93373|  0:12:05s
epoch 142| loss: 104.97974| val_0_rmse: 12.38908|  0:12:10s
epoch 143| loss: 100.09342| val_0_rmse: 12.05002|  0:12:15s
epoch 144| loss: 104.6325| val_0_rmse: 12.26456|  0:12:20s
epoch 145| loss: 99.21632| val_0_rmse: 12.44941|  0:12:25s
epoch 146| loss: 105.45506| val_0_rmse: 12.94853|  0:12:30s
epoch 147| loss: 101.25  | val_0_rmse: 13.47746|  0:12:35s
epoch 148| loss: 100.35649| val_0_rmse: 12.58345|  0:12:40s
epoch 149| loss: 102.69938| val_0_rmse: 12.04972|  0:12:45s
epoch 150| loss: 98.35718| val_0_rmse: 11.97389|  0:12:51s
epoch 151| loss: 97.1497 | val_0_rmse: 11.87172|  0:12:56s
epoch 152| loss: 98.65216| val_0_rmse: 12.01647|  0:13:01s
epoch 153| loss: 99.19968| val_0_rmse: 12.44277|  0:13:06s
epoch 154| loss: 100.89112| val_0_rmse: 12.95985|  0:13:11s
epoch 155| loss: 96.74448| val_0_rmse: 11.91409|  0:13:16s
epoch 156| loss: 98.34566| val_0_rmse: 11.9516 |  0:13:21s
epoch 157| loss: 102.20764| val_0_rmse: 12.38097|  0:13:26s
epoch 158| loss: 99.66824| val_0_rmse: 12.02957|  0:13:31s
epoch 159| loss: 102.19142| val_0_rmse: 12.64293|  0:13:36s
epoch 160| loss: 100.30827| val_0_rmse: 12.67672|  0:13:41s
epoch 161| loss: 93.94795| val_0_rmse: 12.15475|  0:13:46s
epoch 162| loss: 97.87589| val_0_rmse: 12.07709|  0:13:52s
epoch 163| loss: 94.08674| val_0_rmse: 11.98671|  0:13:57s
epoch 164| loss: 95.43138| val_0_rmse: 12.1447 |  0:14:02s
epoch 165| loss: 95.08918| val_0_rmse: 11.98941|  0:14:07s
epoch 166| loss: 94.08072| val_0_rmse: 12.20445|  0:14:12s
epoch 167| loss: 91.30065| val_0_rmse: 12.03147|  0:14:17s
epoch 168| loss: 96.67489| val_0_rmse: 11.98508|  0:14:22s
epoch 169| loss: 98.15712| val_0_rmse: 12.05227|  0:14:27s
epoch 170| loss: 96.31589| val_0_rmse: 11.99178|  0:14:32s
epoch 171| loss: 94.20509| val_0_rmse: 11.92094|  0:14:37s
Early stopping occurred at epoch 171 with best_epoch = 151 and best_val_0_rmse = 11.87172
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 23:08:50,368] Trial 26 finished with value: 11.87172487743081 and parameters: {'lr': 0.003167880867316747, 'n_steps': 6, 'gamma': 1.3594576223031543, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.010613798634156615, 'weight_decay': 9.431989543756016e-05, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8486.36775| val_0_rmse: 90.40179|  0:00:06s
epoch 1  | loss: 6872.48373| val_0_rmse: 74.44403|  0:00:12s
epoch 2  | loss: 3359.79795| val_0_rmse: 43.97459|  0:00:18s
epoch 3  | loss: 1306.47052| val_0_rmse: 35.0399 |  0:00:24s
epoch 4  | loss: 1114.47805| val_0_rmse: 35.05095|  0:00:31s
epoch 5  | loss: 1081.43478| val_0_rmse: 44.39675|  0:00:37s
epoch 6  | loss: 1031.37243| val_0_rmse: 37.16318|  0:00:43s
epoch 7  | loss: 984.80035| val_0_rmse: 37.62884|  0:00:49s
epoch 8  | loss: 938.25414| val_0_rmse: 31.63581|  0:00:56s
epoch 9  | loss: 910.23305| val_0_rmse: 31.72573|  0:01:02s
epoch 10 | loss: 897.18776| val_0_rmse: 33.048  |  0:01:08s
epoch 11 | loss: 847.20973| val_0_rmse: 33.13243|  0:01:14s
epoch 12 | loss: 750.55707| val_0_rmse: 65.25249|  0:01:20s
epoch 13 | loss: 661.94257| val_0_rmse: 27.46807|  0:01:27s
epoch 14 | loss: 588.71105| val_0_rmse: 24.45318|  0:01:33s
epoch 15 | loss: 531.74812| val_0_rmse: 23.7424 |  0:01:39s
epoch 16 | loss: 495.4812| val_0_rmse: 23.81482|  0:01:46s
epoch 17 | loss: 468.11278| val_0_rmse: 23.20632|  0:01:52s
epoch 18 | loss: 434.02385| val_0_rmse: 21.52785|  0:01:58s
epoch 19 | loss: 405.43926| val_0_rmse: 21.98488|  0:02:04s
epoch 20 | loss: 381.30639| val_0_rmse: 21.82658|  0:02:11s
epoch 21 | loss: 365.47969| val_0_rmse: 20.21794|  0:02:17s
epoch 22 | loss: 345.83162| val_0_rmse: 19.65609|  0:02:23s
epoch 23 | loss: 330.99846| val_0_rmse: 19.62102|  0:02:30s
epoch 24 | loss: 321.10094| val_0_rmse: 23.50668|  0:02:36s
epoch 25 | loss: 312.6128| val_0_rmse: 19.00751|  0:02:42s
epoch 26 | loss: 302.07812| val_0_rmse: 19.40631|  0:02:48s
epoch 27 | loss: 286.46869| val_0_rmse: 19.19513|  0:02:54s
epoch 28 | loss: 284.97312| val_0_rmse: 17.83542|  0:03:01s
epoch 29 | loss: 277.58804| val_0_rmse: 21.88303|  0:03:07s
epoch 30 | loss: 264.47559| val_0_rmse: 22.62059|  0:03:13s
epoch 31 | loss: 263.06518| val_0_rmse: 20.61417|  0:03:19s
epoch 32 | loss: 254.45069| val_0_rmse: 17.09156|  0:03:26s
epoch 33 | loss: 252.0719| val_0_rmse: 16.92677|  0:03:32s
epoch 34 | loss: 245.32592| val_0_rmse: 19.98633|  0:03:38s
epoch 35 | loss: 235.40511| val_0_rmse: 18.74859|  0:03:44s
epoch 36 | loss: 234.35227| val_0_rmse: 17.36948|  0:03:51s
epoch 37 | loss: 226.38579| val_0_rmse: 18.9636 |  0:03:57s
epoch 38 | loss: 221.0279| val_0_rmse: 19.13972|  0:04:03s
epoch 39 | loss: 220.55293| val_0_rmse: 18.41262|  0:04:10s
epoch 40 | loss: 220.40241| val_0_rmse: 17.95867|  0:04:16s
epoch 41 | loss: 215.10022| val_0_rmse: 17.76539|  0:04:22s
epoch 42 | loss: 212.71915| val_0_rmse: 17.14484|  0:04:29s
epoch 43 | loss: 208.937 | val_0_rmse: 17.97772|  0:04:35s
epoch 44 | loss: 203.58547| val_0_rmse: 17.11929|  0:04:41s
epoch 45 | loss: 201.20892| val_0_rmse: 16.01266|  0:04:47s
epoch 46 | loss: 200.96808| val_0_rmse: 16.75292|  0:04:53s
epoch 47 | loss: 196.02136| val_0_rmse: 16.77816|  0:05:00s
epoch 48 | loss: 189.61151| val_0_rmse: 17.12535|  0:05:06s
epoch 49 | loss: 193.54681| val_0_rmse: 18.73255|  0:05:12s
epoch 50 | loss: 186.9871| val_0_rmse: 17.61355|  0:05:18s
epoch 51 | loss: 182.52553| val_0_rmse: 16.01926|  0:05:24s
epoch 52 | loss: 177.00223| val_0_rmse: 18.24026|  0:05:31s
epoch 53 | loss: 176.77354| val_0_rmse: 17.64224|  0:05:37s
epoch 54 | loss: 183.82831| val_0_rmse: 17.08215|  0:05:43s
epoch 55 | loss: 180.31121| val_0_rmse: 17.3132 |  0:05:49s
epoch 56 | loss: 174.316 | val_0_rmse: 16.98016|  0:05:56s
epoch 57 | loss: 178.29826| val_0_rmse: 17.74327|  0:06:02s
epoch 58 | loss: 168.0808| val_0_rmse: 16.61726|  0:06:08s
epoch 59 | loss: 166.86348| val_0_rmse: 16.1047 |  0:06:14s
epoch 60 | loss: 165.87757| val_0_rmse: 15.8394 |  0:06:20s
epoch 61 | loss: 165.37743| val_0_rmse: 16.17067|  0:06:27s
epoch 62 | loss: 160.82428| val_0_rmse: 16.04005|  0:06:33s
epoch 63 | loss: 158.58608| val_0_rmse: 16.51651|  0:06:39s
epoch 64 | loss: 160.65839| val_0_rmse: 18.25412|  0:06:46s
epoch 65 | loss: 162.17802| val_0_rmse: 18.07078|  0:06:52s
epoch 66 | loss: 157.07178| val_0_rmse: 15.88752|  0:06:58s
epoch 67 | loss: 153.66276| val_0_rmse: 16.78319|  0:07:04s
epoch 68 | loss: 152.54526| val_0_rmse: 22.08289|  0:07:10s
epoch 69 | loss: 150.07689| val_0_rmse: 15.48066|  0:07:16s
epoch 70 | loss: 149.99005| val_0_rmse: 17.79035|  0:07:23s
epoch 71 | loss: 149.15677| val_0_rmse: 16.85569|  0:07:29s
epoch 72 | loss: 143.68702| val_0_rmse: 17.20008|  0:07:35s
epoch 73 | loss: 146.42822| val_0_rmse: 15.68358|  0:07:41s
epoch 74 | loss: 143.11189| val_0_rmse: 15.28437|  0:07:47s
epoch 75 | loss: 142.24001| val_0_rmse: 16.45984|  0:07:53s
epoch 76 | loss: 140.98746| val_0_rmse: 15.55938|  0:08:00s
epoch 77 | loss: 140.2414| val_0_rmse: 19.36585|  0:08:06s
epoch 78 | loss: 138.29043| val_0_rmse: 17.79642|  0:08:12s
epoch 79 | loss: 138.3376| val_0_rmse: 15.80325|  0:08:18s
epoch 80 | loss: 134.33733| val_0_rmse: 16.11286|  0:08:24s
epoch 81 | loss: 139.59854| val_0_rmse: 15.86713|  0:08:30s
epoch 82 | loss: 136.14982| val_0_rmse: 15.2525 |  0:08:37s
epoch 83 | loss: 134.05981| val_0_rmse: 15.42391|  0:08:43s
epoch 84 | loss: 133.92618| val_0_rmse: 15.34588|  0:08:49s
epoch 85 | loss: 131.10746| val_0_rmse: 18.47995|  0:08:55s
epoch 86 | loss: 130.40455| val_0_rmse: 16.23522|  0:09:02s
epoch 87 | loss: 132.7157| val_0_rmse: 15.83761|  0:09:08s
epoch 88 | loss: 129.45148| val_0_rmse: 15.1715 |  0:09:14s
epoch 89 | loss: 129.72453| val_0_rmse: 15.51902|  0:09:20s
epoch 90 | loss: 130.90869| val_0_rmse: 16.50028|  0:09:26s
epoch 91 | loss: 126.57552| val_0_rmse: 14.36044|  0:09:32s
epoch 92 | loss: 128.19392| val_0_rmse: 14.83687|  0:09:38s
epoch 93 | loss: 125.90587| val_0_rmse: 16.65537|  0:09:45s
epoch 94 | loss: 123.03025| val_0_rmse: 18.09052|  0:09:51s
epoch 95 | loss: 127.88582| val_0_rmse: 16.67844|  0:09:57s
epoch 96 | loss: 124.13735| val_0_rmse: 18.825  |  0:10:03s
epoch 97 | loss: 125.12748| val_0_rmse: 28.26652|  0:10:09s
epoch 98 | loss: 121.39378| val_0_rmse: 33.48973|  0:10:16s
epoch 99 | loss: 118.82393| val_0_rmse: 21.22072|  0:10:22s
epoch 100| loss: 125.24112| val_0_rmse: 14.44468|  0:10:28s
epoch 101| loss: 121.24267| val_0_rmse: 15.12669|  0:10:34s
epoch 102| loss: 121.87011| val_0_rmse: 15.48037|  0:10:40s
epoch 103| loss: 119.54409| val_0_rmse: 14.73199|  0:10:47s
epoch 104| loss: 121.22644| val_0_rmse: 14.44776|  0:10:53s
epoch 105| loss: 117.49959| val_0_rmse: 15.92972|  0:10:59s
epoch 106| loss: 116.12148| val_0_rmse: 14.8026 |  0:11:05s
epoch 107| loss: 118.13715| val_0_rmse: 14.46884|  0:11:11s
epoch 108| loss: 117.88889| val_0_rmse: 14.69342|  0:11:17s
epoch 109| loss: 116.56473| val_0_rmse: 16.23949|  0:11:23s
epoch 110| loss: 113.92227| val_0_rmse: 16.60625|  0:11:30s
epoch 111| loss: 115.15675| val_0_rmse: 18.96542|  0:11:36s
Early stopping occurred at epoch 111 with best_epoch = 91 and best_val_0_rmse = 14.36044
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 23:20:43,412] Trial 27 finished with value: 14.36044090449084 and parameters: {'lr': 0.002141947180399736, 'n_steps': 4, 'gamma': 1.2173717704353926, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.0579636968059912, 'weight_decay': 2.2171725354656363e-05, 'batch_size': 128, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 9158.82805| val_0_rmse: 96.67911|  0:00:04s
epoch 1  | loss: 9036.20889| val_0_rmse: 95.03886|  0:00:07s
epoch 2  | loss: 8929.40716| val_0_rmse: 96.0748 |  0:00:11s
epoch 3  | loss: 8785.30416| val_0_rmse: 93.77446|  0:00:14s
epoch 4  | loss: 8303.60775| val_0_rmse: 89.24242|  0:00:18s
epoch 5  | loss: 6259.80271| val_0_rmse: 65.40466|  0:00:21s
epoch 6  | loss: 2802.51164| val_0_rmse: 42.1853 |  0:00:25s
epoch 7  | loss: 1193.50676| val_0_rmse: 35.55101|  0:00:28s
epoch 8  | loss: 1043.42539| val_0_rmse: 33.44773|  0:00:32s
epoch 9  | loss: 995.22906| val_0_rmse: 32.62818|  0:00:36s
epoch 10 | loss: 952.67811| val_0_rmse: 31.2104 |  0:00:39s
epoch 11 | loss: 926.39765| val_0_rmse: 30.42171|  0:00:43s
epoch 12 | loss: 906.16259| val_0_rmse: 30.14155|  0:00:47s
epoch 13 | loss: 882.67134| val_0_rmse: 29.71467|  0:00:50s
epoch 14 | loss: 848.57055| val_0_rmse: 29.41253|  0:00:54s
epoch 15 | loss: 840.19839| val_0_rmse: 29.22152|  0:00:57s
epoch 16 | loss: 837.44911| val_0_rmse: 29.27092|  0:01:01s
epoch 17 | loss: 815.91205| val_0_rmse: 28.8052 |  0:01:05s
epoch 18 | loss: 810.92488| val_0_rmse: 28.84213|  0:01:08s
epoch 19 | loss: 803.71713| val_0_rmse: 28.99671|  0:01:12s
epoch 20 | loss: 803.24819| val_0_rmse: 28.64432|  0:01:15s
epoch 21 | loss: 797.54999| val_0_rmse: 28.74822|  0:01:19s
epoch 22 | loss: 793.5082| val_0_rmse: 28.73534|  0:01:22s
epoch 23 | loss: 777.05236| val_0_rmse: 29.252  |  0:01:26s
epoch 24 | loss: 772.23578| val_0_rmse: 29.02839|  0:01:29s
epoch 25 | loss: 759.10081| val_0_rmse: 28.69874|  0:01:33s
epoch 26 | loss: 745.27562| val_0_rmse: 28.23658|  0:01:37s
epoch 27 | loss: 725.95343| val_0_rmse: 28.23139|  0:01:40s
epoch 28 | loss: 710.68148| val_0_rmse: 28.26855|  0:01:44s
epoch 29 | loss: 696.75884| val_0_rmse: 26.8114 |  0:01:47s
epoch 30 | loss: 680.51468| val_0_rmse: 26.95204|  0:01:51s
epoch 31 | loss: 666.03919| val_0_rmse: 26.20665|  0:01:54s
epoch 32 | loss: 642.44494| val_0_rmse: 25.70358|  0:01:58s
epoch 33 | loss: 607.37381| val_0_rmse: 25.64539|  0:02:02s
epoch 34 | loss: 584.19378| val_0_rmse: 28.41426|  0:02:05s
epoch 35 | loss: 568.74318| val_0_rmse: 24.28481|  0:02:09s
epoch 36 | loss: 534.4606| val_0_rmse: 23.7322 |  0:02:13s
epoch 37 | loss: 513.29182| val_0_rmse: 23.08588|  0:02:16s
epoch 38 | loss: 487.81649| val_0_rmse: 22.84528|  0:02:20s
epoch 39 | loss: 473.77129| val_0_rmse: 22.82049|  0:02:23s
epoch 40 | loss: 463.00056| val_0_rmse: 21.55906|  0:02:27s
epoch 41 | loss: 443.5874| val_0_rmse: 21.63077|  0:02:30s
epoch 42 | loss: 430.26794| val_0_rmse: 21.35321|  0:02:34s
epoch 43 | loss: 421.34407| val_0_rmse: 20.90832|  0:02:38s
epoch 44 | loss: 412.11229| val_0_rmse: 20.75784|  0:02:41s
epoch 45 | loss: 402.05909| val_0_rmse: 19.82802|  0:02:45s
epoch 46 | loss: 387.17542| val_0_rmse: 20.54851|  0:02:48s
epoch 47 | loss: 375.43216| val_0_rmse: 20.31041|  0:02:52s
epoch 48 | loss: 368.30611| val_0_rmse: 19.21914|  0:02:55s
epoch 49 | loss: 356.5758| val_0_rmse: 18.67759|  0:02:59s
epoch 50 | loss: 345.64229| val_0_rmse: 18.5812 |  0:03:03s
epoch 51 | loss: 328.20221| val_0_rmse: 18.05801|  0:03:06s
epoch 52 | loss: 323.3398| val_0_rmse: 17.99053|  0:03:10s
epoch 53 | loss: 315.41218| val_0_rmse: 17.57272|  0:03:13s
epoch 54 | loss: 300.93161| val_0_rmse: 17.4308 |  0:03:17s
epoch 55 | loss: 301.80296| val_0_rmse: 17.56334|  0:03:20s
epoch 56 | loss: 292.14363| val_0_rmse: 17.05658|  0:03:24s
epoch 57 | loss: 283.46101| val_0_rmse: 17.26874|  0:03:28s
epoch 58 | loss: 272.64278| val_0_rmse: 16.26444|  0:03:31s
epoch 59 | loss: 267.36595| val_0_rmse: 17.15475|  0:03:35s
epoch 60 | loss: 267.94197| val_0_rmse: 16.26914|  0:03:38s
epoch 61 | loss: 261.56493| val_0_rmse: 16.18885|  0:03:42s
epoch 62 | loss: 251.90179| val_0_rmse: 15.88926|  0:03:45s
epoch 63 | loss: 252.043 | val_0_rmse: 15.7171 |  0:03:49s
epoch 64 | loss: 240.61331| val_0_rmse: 15.40989|  0:03:53s
epoch 65 | loss: 243.35124| val_0_rmse: 15.11056|  0:03:56s
epoch 66 | loss: 233.00902| val_0_rmse: 15.0843 |  0:04:00s
epoch 67 | loss: 234.68383| val_0_rmse: 15.01277|  0:04:03s
epoch 68 | loss: 225.28339| val_0_rmse: 14.92452|  0:04:07s
epoch 69 | loss: 226.38879| val_0_rmse: 14.95928|  0:04:11s
epoch 70 | loss: 215.32641| val_0_rmse: 14.77382|  0:04:14s
epoch 71 | loss: 212.35161| val_0_rmse: 14.77581|  0:04:18s
epoch 72 | loss: 213.56977| val_0_rmse: 14.84803|  0:04:21s
epoch 73 | loss: 209.1004| val_0_rmse: 14.66774|  0:04:25s
epoch 74 | loss: 207.5491| val_0_rmse: 14.43825|  0:04:28s
epoch 75 | loss: 210.62181| val_0_rmse: 14.64072|  0:04:32s
epoch 76 | loss: 208.80148| val_0_rmse: 14.15617|  0:04:36s
epoch 77 | loss: 202.37951| val_0_rmse: 14.47551|  0:04:39s
epoch 78 | loss: 202.54254| val_0_rmse: 14.18539|  0:04:42s
epoch 79 | loss: 198.46892| val_0_rmse: 14.1761 |  0:04:46s
epoch 80 | loss: 196.43262| val_0_rmse: 14.1038 |  0:04:49s
epoch 81 | loss: 195.45265| val_0_rmse: 13.96708|  0:04:53s
epoch 82 | loss: 190.95771| val_0_rmse: 14.07527|  0:04:56s
epoch 83 | loss: 186.17765| val_0_rmse: 13.92953|  0:05:00s
epoch 84 | loss: 181.09494| val_0_rmse: 13.98051|  0:05:04s
epoch 85 | loss: 184.86472| val_0_rmse: 13.80642|  0:05:07s
epoch 86 | loss: 179.13929| val_0_rmse: 14.05226|  0:05:11s
epoch 87 | loss: 182.0148| val_0_rmse: 13.58371|  0:05:14s
epoch 88 | loss: 178.82651| val_0_rmse: 13.63424|  0:05:18s
epoch 89 | loss: 174.72316| val_0_rmse: 13.67585|  0:05:21s
epoch 90 | loss: 170.66174| val_0_rmse: 13.93049|  0:05:24s
epoch 91 | loss: 169.01828| val_0_rmse: 13.66096|  0:05:28s
epoch 92 | loss: 172.58084| val_0_rmse: 13.46833|  0:05:31s
epoch 93 | loss: 172.20631| val_0_rmse: 13.53472|  0:05:35s
epoch 94 | loss: 170.13255| val_0_rmse: 13.53634|  0:05:39s
epoch 95 | loss: 166.32277| val_0_rmse: 13.50767|  0:05:42s
epoch 96 | loss: 165.79897| val_0_rmse: 13.48559|  0:05:45s
epoch 97 | loss: 159.88185| val_0_rmse: 13.29237|  0:05:49s
epoch 98 | loss: 161.30448| val_0_rmse: 13.27095|  0:05:52s
epoch 99 | loss: 162.83784| val_0_rmse: 13.37578|  0:05:56s
epoch 100| loss: 165.38715| val_0_rmse: 13.30098|  0:05:59s
epoch 101| loss: 155.57266| val_0_rmse: 13.1484 |  0:06:03s
epoch 102| loss: 156.20399| val_0_rmse: 13.24258|  0:06:07s
epoch 103| loss: 155.15335| val_0_rmse: 13.16372|  0:06:10s
epoch 104| loss: 154.45179| val_0_rmse: 12.88909|  0:06:14s
epoch 105| loss: 151.20392| val_0_rmse: 13.05948|  0:06:17s
epoch 106| loss: 155.12259| val_0_rmse: 12.93751|  0:06:20s
epoch 107| loss: 154.12749| val_0_rmse: 12.97318|  0:06:24s
epoch 108| loss: 149.41671| val_0_rmse: 13.2548 |  0:06:27s
epoch 109| loss: 150.43378| val_0_rmse: 12.91942|  0:06:31s
epoch 110| loss: 148.28145| val_0_rmse: 12.80352|  0:06:35s
epoch 111| loss: 150.23769| val_0_rmse: 12.69874|  0:06:38s
epoch 112| loss: 141.96492| val_0_rmse: 12.71067|  0:06:42s
epoch 113| loss: 139.79589| val_0_rmse: 12.88482|  0:06:45s
epoch 114| loss: 148.12442| val_0_rmse: 13.17954|  0:06:49s
epoch 115| loss: 144.7622| val_0_rmse: 12.77038|  0:06:52s
epoch 116| loss: 146.28844| val_0_rmse: 13.11256|  0:06:55s
epoch 117| loss: 143.91972| val_0_rmse: 13.08487|  0:06:59s
epoch 118| loss: 140.01116| val_0_rmse: 12.72659|  0:07:03s
epoch 119| loss: 139.49591| val_0_rmse: 12.72952|  0:07:06s
epoch 120| loss: 139.31727| val_0_rmse: 12.90001|  0:07:10s
epoch 121| loss: 139.42509| val_0_rmse: 12.69494|  0:07:13s
epoch 122| loss: 139.97721| val_0_rmse: 12.64301|  0:07:17s
epoch 123| loss: 135.34783| val_0_rmse: 12.68339|  0:07:20s
epoch 124| loss: 135.23907| val_0_rmse: 12.59084|  0:07:24s
epoch 125| loss: 136.10723| val_0_rmse: 12.60517|  0:07:27s
epoch 126| loss: 128.93996| val_0_rmse: 12.53893|  0:07:30s
epoch 127| loss: 135.21037| val_0_rmse: 12.62528|  0:07:34s
epoch 128| loss: 130.41602| val_0_rmse: 12.3607 |  0:07:38s
epoch 129| loss: 128.81753| val_0_rmse: 12.51174|  0:07:41s
epoch 130| loss: 129.00755| val_0_rmse: 12.23927|  0:07:45s
epoch 131| loss: 130.48318| val_0_rmse: 12.36684|  0:07:48s
epoch 132| loss: 127.65088| val_0_rmse: 12.58529|  0:07:52s
epoch 133| loss: 129.94296| val_0_rmse: 12.2179 |  0:07:55s
epoch 134| loss: 124.20577| val_0_rmse: 12.47411|  0:07:59s
epoch 135| loss: 124.35158| val_0_rmse: 12.34203|  0:08:02s
epoch 136| loss: 123.05377| val_0_rmse: 12.53409|  0:08:06s
epoch 137| loss: 120.52033| val_0_rmse: 12.02023|  0:08:09s
epoch 138| loss: 125.00001| val_0_rmse: 12.36062|  0:08:13s
epoch 139| loss: 120.29308| val_0_rmse: 11.7696 |  0:08:16s
epoch 140| loss: 124.99177| val_0_rmse: 12.3263 |  0:08:20s
epoch 141| loss: 127.3435| val_0_rmse: 12.1306 |  0:08:23s
epoch 142| loss: 119.3989| val_0_rmse: 12.18926|  0:08:27s
epoch 143| loss: 123.61917| val_0_rmse: 12.17558|  0:08:30s
epoch 144| loss: 119.85675| val_0_rmse: 12.28637|  0:08:34s
epoch 145| loss: 121.796 | val_0_rmse: 12.13251|  0:08:37s
epoch 146| loss: 120.6799| val_0_rmse: 12.21687|  0:08:41s
epoch 147| loss: 117.6992| val_0_rmse: 12.24338|  0:08:44s
epoch 148| loss: 122.90574| val_0_rmse: 12.12547|  0:08:48s
epoch 149| loss: 116.31608| val_0_rmse: 12.2245 |  0:08:51s
epoch 150| loss: 117.05358| val_0_rmse: 11.97743|  0:08:55s
epoch 151| loss: 116.92478| val_0_rmse: 12.434  |  0:08:58s
epoch 152| loss: 116.02175| val_0_rmse: 12.22512|  0:09:02s
epoch 153| loss: 112.34075| val_0_rmse: 12.28204|  0:09:06s
epoch 154| loss: 121.31354| val_0_rmse: 12.24795|  0:09:09s
epoch 155| loss: 115.26931| val_0_rmse: 12.12595|  0:09:13s
epoch 156| loss: 113.41101| val_0_rmse: 12.16702|  0:09:16s
epoch 157| loss: 111.43633| val_0_rmse: 12.02712|  0:09:20s
epoch 158| loss: 113.51562| val_0_rmse: 11.87369|  0:09:23s
epoch 159| loss: 109.01  | val_0_rmse: 11.92426|  0:09:26s
Early stopping occurred at epoch 159 with best_epoch = 139 and best_val_0_rmse = 11.7696
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 23:30:18,977] Trial 28 finished with value: 11.769600019815005 and parameters: {'lr': 0.0035094634864575825, 'n_steps': 7, 'gamma': 1.1674163505220165, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.04716666108770833, 'weight_decay': 0.0003038136993190354, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8062.253| val_0_rmse: 85.1828 |  0:00:03s
epoch 1  | loss: 4308.62132| val_0_rmse: 44.85322|  0:00:07s
epoch 2  | loss: 1138.23485| val_0_rmse: 32.79673|  0:00:11s
epoch 3  | loss: 933.08911| val_0_rmse: 32.37305|  0:00:15s
epoch 4  | loss: 789.26991| val_0_rmse: 30.20655|  0:00:19s
epoch 5  | loss: 665.50115| val_0_rmse: 25.14329|  0:00:22s
epoch 6  | loss: 571.34705| val_0_rmse: 22.48349|  0:00:26s
epoch 7  | loss: 513.16624| val_0_rmse: 21.63549|  0:00:30s
epoch 8  | loss: 473.85435| val_0_rmse: 21.4697 |  0:00:34s
epoch 9  | loss: 427.18032| val_0_rmse: 20.16568|  0:00:38s
epoch 10 | loss: 404.38829| val_0_rmse: 20.19248|  0:00:42s
epoch 11 | loss: 375.01848| val_0_rmse: 19.49417|  0:00:46s
epoch 12 | loss: 351.19377| val_0_rmse: 18.56034|  0:00:50s
epoch 13 | loss: 337.01074| val_0_rmse: 18.06054|  0:00:53s
epoch 14 | loss: 311.25329| val_0_rmse: 17.58284|  0:00:58s
epoch 15 | loss: 305.71398| val_0_rmse: 17.52544|  0:01:01s
epoch 16 | loss: 294.98781| val_0_rmse: 17.50148|  0:01:05s
epoch 17 | loss: 281.10092| val_0_rmse: 17.1147 |  0:01:09s
epoch 18 | loss: 271.30138| val_0_rmse: 16.52117|  0:01:13s
epoch 19 | loss: 268.21818| val_0_rmse: 16.39324|  0:01:17s
epoch 20 | loss: 252.30954| val_0_rmse: 16.34054|  0:01:21s
epoch 21 | loss: 244.37027| val_0_rmse: 15.83092|  0:01:24s
epoch 22 | loss: 237.07936| val_0_rmse: 15.74289|  0:01:29s
epoch 23 | loss: 233.84214| val_0_rmse: 15.71459|  0:01:32s
epoch 24 | loss: 231.56559| val_0_rmse: 15.44071|  0:01:36s
epoch 25 | loss: 220.14521| val_0_rmse: 15.06776|  0:01:40s
epoch 26 | loss: 216.68131| val_0_rmse: 15.14819|  0:01:44s
epoch 27 | loss: 211.66095| val_0_rmse: 15.29853|  0:01:48s
epoch 28 | loss: 206.85713| val_0_rmse: 15.49289|  0:01:52s
epoch 29 | loss: 203.7489| val_0_rmse: 14.88286|  0:01:55s
epoch 30 | loss: 201.98791| val_0_rmse: 15.16195|  0:02:00s
epoch 31 | loss: 197.75417| val_0_rmse: 14.96987|  0:02:03s
epoch 32 | loss: 193.16727| val_0_rmse: 14.91312|  0:02:07s
epoch 33 | loss: 194.17005| val_0_rmse: 14.99804|  0:02:11s
epoch 34 | loss: 185.21043| val_0_rmse: 16.17811|  0:02:15s
epoch 35 | loss: 181.37874| val_0_rmse: 14.82304|  0:02:19s
epoch 36 | loss: 182.52865| val_0_rmse: 14.55408|  0:02:22s
epoch 37 | loss: 182.52742| val_0_rmse: 14.3043 |  0:02:26s
epoch 38 | loss: 178.46038| val_0_rmse: 14.47001|  0:02:30s
epoch 39 | loss: 172.44618| val_0_rmse: 14.18111|  0:02:34s
epoch 40 | loss: 178.42637| val_0_rmse: 14.223  |  0:02:38s
epoch 41 | loss: 172.94547| val_0_rmse: 14.17569|  0:02:42s
epoch 42 | loss: 167.72982| val_0_rmse: 14.23595|  0:02:46s
epoch 43 | loss: 165.5816| val_0_rmse: 14.03823|  0:02:50s
epoch 44 | loss: 168.85733| val_0_rmse: 13.8872 |  0:02:53s
epoch 45 | loss: 163.73223| val_0_rmse: 13.93051|  0:02:58s
epoch 46 | loss: 155.96072| val_0_rmse: 14.34105|  0:03:01s
epoch 47 | loss: 159.42168| val_0_rmse: 13.98801|  0:03:05s
epoch 48 | loss: 155.55328| val_0_rmse: 13.91771|  0:03:09s
epoch 49 | loss: 154.92502| val_0_rmse: 13.56265|  0:03:13s
epoch 50 | loss: 152.07464| val_0_rmse: 13.55036|  0:03:17s
epoch 51 | loss: 151.90668| val_0_rmse: 16.58733|  0:03:21s
epoch 52 | loss: 151.20819| val_0_rmse: 13.31467|  0:03:24s
epoch 53 | loss: 148.96555| val_0_rmse: 13.45811|  0:03:29s
epoch 54 | loss: 147.24171| val_0_rmse: 13.25493|  0:03:32s
epoch 55 | loss: 149.12013| val_0_rmse: 13.23289|  0:03:36s
epoch 56 | loss: 146.55263| val_0_rmse: 13.2198 |  0:03:40s
epoch 57 | loss: 144.37532| val_0_rmse: 13.53705|  0:03:44s
epoch 58 | loss: 139.43665| val_0_rmse: 13.47385|  0:03:48s
epoch 59 | loss: 144.57982| val_0_rmse: 16.31207|  0:03:51s
epoch 60 | loss: 140.36296| val_0_rmse: 13.30748|  0:03:55s
epoch 61 | loss: 138.18747| val_0_rmse: 14.14503|  0:03:59s
epoch 62 | loss: 134.29236| val_0_rmse: 13.38654|  0:04:03s
epoch 63 | loss: 136.38718| val_0_rmse: 14.07093|  0:04:07s
epoch 64 | loss: 135.75014| val_0_rmse: 13.56991|  0:04:11s
epoch 65 | loss: 130.98194| val_0_rmse: 14.77697|  0:04:14s
epoch 66 | loss: 138.87438| val_0_rmse: 15.16056|  0:04:18s
epoch 67 | loss: 134.78059| val_0_rmse: 14.88244|  0:04:22s
epoch 68 | loss: 131.67041| val_0_rmse: 13.85374|  0:04:26s
epoch 69 | loss: 130.64673| val_0_rmse: 14.3194 |  0:04:30s
epoch 70 | loss: 129.46917| val_0_rmse: 14.88555|  0:04:33s
epoch 71 | loss: 124.06936| val_0_rmse: 13.97385|  0:04:37s
epoch 72 | loss: 124.90773| val_0_rmse: 14.69711|  0:04:41s
epoch 73 | loss: 126.77905| val_0_rmse: 15.58385|  0:04:45s
epoch 74 | loss: 126.06111| val_0_rmse: 14.83795|  0:04:48s
epoch 75 | loss: 128.41169| val_0_rmse: 13.5237 |  0:04:52s
epoch 76 | loss: 120.45569| val_0_rmse: 16.49002|  0:04:56s
Early stopping occurred at epoch 76 with best_epoch = 56 and best_val_0_rmse = 13.2198
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 23:35:23,363] Trial 29 finished with value: 13.21979609756007 and parameters: {'lr': 0.0049419279003503165, 'n_steps': 3, 'gamma': 1.4530986622667965, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.039665654186842814, 'weight_decay': 5.783229890697869e-05, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8830.19635| val_0_rmse: 81.71556|  0:00:04s
epoch 1  | loss: 8551.1773| val_0_rmse: 45.4603 |  0:00:09s
epoch 2  | loss: 7436.57496| val_0_rmse: 55.69571|  0:00:13s
epoch 3  | loss: 4217.48161| val_0_rmse: 47.24686|  0:00:18s
epoch 4  | loss: 1694.56536| val_0_rmse: 36.90756|  0:00:23s
epoch 5  | loss: 1073.57774| val_0_rmse: 37.62164|  0:00:27s
epoch 6  | loss: 935.57697| val_0_rmse: 33.93111|  0:00:32s
epoch 7  | loss: 897.90687| val_0_rmse: 30.88906|  0:00:37s
epoch 8  | loss: 884.64169| val_0_rmse: 29.77252|  0:00:41s
epoch 9  | loss: 864.09936| val_0_rmse: 29.32048|  0:00:46s
epoch 10 | loss: 844.40614| val_0_rmse: 29.38782|  0:00:50s
epoch 11 | loss: 820.32214| val_0_rmse: 28.39248|  0:00:55s
epoch 12 | loss: 804.68392| val_0_rmse: 27.99107|  0:01:00s
epoch 13 | loss: 777.4924| val_0_rmse: 28.22414|  0:01:05s
epoch 14 | loss: 756.63326| val_0_rmse: 27.4836 |  0:01:09s
epoch 15 | loss: 715.44338| val_0_rmse: 27.53531|  0:01:14s
epoch 16 | loss: 659.02073| val_0_rmse: 25.51379|  0:01:18s
epoch 17 | loss: 609.12461| val_0_rmse: 25.51063|  0:01:23s
epoch 18 | loss: 560.82602| val_0_rmse: 23.73937|  0:01:28s
epoch 19 | loss: 520.17851| val_0_rmse: 24.40607|  0:01:33s
epoch 20 | loss: 481.85132| val_0_rmse: 22.4745 |  0:01:37s
epoch 21 | loss: 452.12966| val_0_rmse: 21.76575|  0:01:42s
epoch 22 | loss: 428.3522| val_0_rmse: 21.16439|  0:01:46s
epoch 23 | loss: 410.69383| val_0_rmse: 21.11091|  0:01:51s
epoch 24 | loss: 385.60665| val_0_rmse: 20.47319|  0:01:56s
epoch 25 | loss: 374.62024| val_0_rmse: 20.15654|  0:02:01s
epoch 26 | loss: 356.83186| val_0_rmse: 19.83587|  0:02:05s
epoch 27 | loss: 344.00381| val_0_rmse: 18.97961|  0:02:10s
epoch 28 | loss: 334.24953| val_0_rmse: 18.88011|  0:02:14s
epoch 29 | loss: 316.09353| val_0_rmse: 18.4006 |  0:02:19s
epoch 30 | loss: 311.58983| val_0_rmse: 17.62213|  0:02:24s
epoch 31 | loss: 293.75001| val_0_rmse: 17.55084|  0:02:29s
epoch 32 | loss: 285.25883| val_0_rmse: 17.06116|  0:02:33s
epoch 33 | loss: 280.26587| val_0_rmse: 17.22958|  0:02:38s
epoch 34 | loss: 265.23439| val_0_rmse: 16.91335|  0:02:42s
epoch 35 | loss: 261.85821| val_0_rmse: 16.27786|  0:02:47s
epoch 36 | loss: 261.74578| val_0_rmse: 16.16409|  0:02:52s
epoch 37 | loss: 250.22511| val_0_rmse: 16.56629|  0:02:57s
epoch 38 | loss: 248.11139| val_0_rmse: 16.04994|  0:03:01s
epoch 39 | loss: 235.81427| val_0_rmse: 15.89035|  0:03:06s
epoch 40 | loss: 238.81195| val_0_rmse: 15.8684 |  0:03:10s
epoch 41 | loss: 231.75256| val_0_rmse: 15.84173|  0:03:15s
epoch 42 | loss: 225.14244| val_0_rmse: 16.13129|  0:03:20s
epoch 43 | loss: 217.12106| val_0_rmse: 15.4256 |  0:03:25s
epoch 44 | loss: 215.11464| val_0_rmse: 15.37082|  0:03:29s
epoch 45 | loss: 217.46565| val_0_rmse: 15.05128|  0:03:34s
epoch 46 | loss: 212.20714| val_0_rmse: 15.27717|  0:03:39s
epoch 47 | loss: 209.11005| val_0_rmse: 15.49457|  0:03:43s
epoch 48 | loss: 208.67316| val_0_rmse: 15.43186|  0:03:48s
epoch 49 | loss: 204.27557| val_0_rmse: 15.26194|  0:03:52s
epoch 50 | loss: 198.89119| val_0_rmse: 14.94179|  0:03:57s
epoch 51 | loss: 201.99553| val_0_rmse: 15.02238|  0:04:02s
epoch 52 | loss: 196.65762| val_0_rmse: 15.41315|  0:04:06s
epoch 53 | loss: 195.58869| val_0_rmse: 14.83385|  0:04:11s
epoch 54 | loss: 191.30308| val_0_rmse: 14.63515|  0:04:16s
epoch 55 | loss: 187.9426| val_0_rmse: 14.84729|  0:04:20s
epoch 56 | loss: 191.71822| val_0_rmse: 14.54204|  0:04:25s
epoch 57 | loss: 189.63632| val_0_rmse: 14.87467|  0:04:30s
epoch 58 | loss: 180.60153| val_0_rmse: 15.07212|  0:04:35s
epoch 59 | loss: 180.35097| val_0_rmse: 14.7919 |  0:04:39s
epoch 60 | loss: 178.33449| val_0_rmse: 14.86288|  0:04:44s
epoch 61 | loss: 182.10588| val_0_rmse: 14.66804|  0:04:48s
epoch 62 | loss: 179.7997| val_0_rmse: 14.77839|  0:04:53s
epoch 63 | loss: 172.75645| val_0_rmse: 14.44474|  0:04:58s
epoch 64 | loss: 173.09409| val_0_rmse: 14.57821|  0:05:02s
epoch 65 | loss: 176.8885| val_0_rmse: 14.98581|  0:05:07s
epoch 66 | loss: 174.64321| val_0_rmse: 14.1351 |  0:05:12s
epoch 67 | loss: 174.62868| val_0_rmse: 14.23985|  0:05:16s
epoch 68 | loss: 173.07554| val_0_rmse: 14.465  |  0:05:21s
epoch 69 | loss: 170.19849| val_0_rmse: 13.851  |  0:05:26s
epoch 70 | loss: 175.41865| val_0_rmse: 13.91285|  0:05:30s
epoch 71 | loss: 167.23556| val_0_rmse: 14.30136|  0:05:35s
epoch 72 | loss: 170.45238| val_0_rmse: 14.14669|  0:05:39s
epoch 73 | loss: 166.07107| val_0_rmse: 14.0588 |  0:05:44s
epoch 74 | loss: 166.39611| val_0_rmse: 13.87616|  0:05:49s
epoch 75 | loss: 167.23096| val_0_rmse: 14.05728|  0:05:54s
epoch 76 | loss: 160.10957| val_0_rmse: 14.93995|  0:05:58s
epoch 77 | loss: 162.95948| val_0_rmse: 14.1064 |  0:06:03s
epoch 78 | loss: 162.56487| val_0_rmse: 13.47439|  0:06:07s
epoch 79 | loss: 159.35697| val_0_rmse: 13.49902|  0:06:12s
epoch 80 | loss: 160.56047| val_0_rmse: 13.40668|  0:06:17s
epoch 81 | loss: 155.0041| val_0_rmse: 13.5322 |  0:06:21s
epoch 82 | loss: 160.73243| val_0_rmse: 13.30763|  0:06:26s
epoch 83 | loss: 157.70698| val_0_rmse: 13.49878|  0:06:31s
epoch 84 | loss: 155.87045| val_0_rmse: 13.4947 |  0:06:35s
epoch 85 | loss: 159.42337| val_0_rmse: 13.6826 |  0:06:40s
epoch 86 | loss: 158.04529| val_0_rmse: 13.77543|  0:06:44s
epoch 87 | loss: 155.48414| val_0_rmse: 14.51078|  0:06:49s
epoch 88 | loss: 157.64014| val_0_rmse: 13.65447|  0:06:54s
epoch 89 | loss: 152.30808| val_0_rmse: 13.62347|  0:06:59s
epoch 90 | loss: 150.31216| val_0_rmse: 13.5503 |  0:07:03s
epoch 91 | loss: 149.90251| val_0_rmse: 13.1925 |  0:07:08s
epoch 92 | loss: 149.28759| val_0_rmse: 13.72075|  0:07:12s
epoch 93 | loss: 150.3657| val_0_rmse: 13.5802 |  0:07:17s
epoch 94 | loss: 154.11046| val_0_rmse: 14.64681|  0:07:22s
epoch 95 | loss: 149.70276| val_0_rmse: 13.38678|  0:07:27s
epoch 96 | loss: 152.26098| val_0_rmse: 13.28121|  0:07:31s
epoch 97 | loss: 150.29527| val_0_rmse: 13.27599|  0:07:36s
epoch 98 | loss: 152.36774| val_0_rmse: 13.58903|  0:07:40s
epoch 99 | loss: 152.65728| val_0_rmse: 13.3224 |  0:07:45s
epoch 100| loss: 152.05568| val_0_rmse: 13.56013|  0:07:49s
epoch 101| loss: 151.27511| val_0_rmse: 13.77933|  0:07:54s
epoch 102| loss: 146.00879| val_0_rmse: 13.48577|  0:07:59s
epoch 103| loss: 141.30311| val_0_rmse: 13.18097|  0:08:04s
epoch 104| loss: 145.6289| val_0_rmse: 13.12533|  0:08:08s
epoch 105| loss: 151.29411| val_0_rmse: 14.35946|  0:08:13s
epoch 106| loss: 153.22882| val_0_rmse: 13.46276|  0:08:17s
epoch 107| loss: 145.05165| val_0_rmse: 13.33289|  0:08:22s
epoch 108| loss: 141.65585| val_0_rmse: 13.79514|  0:08:27s
epoch 109| loss: 145.70419| val_0_rmse: 13.41591|  0:08:31s
epoch 110| loss: 142.9838| val_0_rmse: 15.2386 |  0:08:36s
epoch 111| loss: 148.87501| val_0_rmse: 13.42407|  0:08:40s
epoch 112| loss: 150.46065| val_0_rmse: 13.46752|  0:08:45s
epoch 113| loss: 144.2233| val_0_rmse: 13.17649|  0:08:49s
epoch 114| loss: 141.34597| val_0_rmse: 12.57141|  0:08:54s
epoch 115| loss: 146.29864| val_0_rmse: 13.33624|  0:08:59s
epoch 116| loss: 143.19221| val_0_rmse: 13.33008|  0:09:03s
epoch 117| loss: 140.40537| val_0_rmse: 13.50006|  0:09:08s
epoch 118| loss: 146.0352| val_0_rmse: 13.13323|  0:09:12s
epoch 119| loss: 140.44034| val_0_rmse: 12.64057|  0:09:17s
epoch 120| loss: 139.04153| val_0_rmse: 13.78343|  0:09:21s
epoch 121| loss: 142.70861| val_0_rmse: 13.06838|  0:09:26s
epoch 122| loss: 139.77792| val_0_rmse: 13.35186|  0:09:31s
epoch 123| loss: 136.04225| val_0_rmse: 19.1007 |  0:09:35s
epoch 124| loss: 137.6375| val_0_rmse: 13.42028|  0:09:40s
epoch 125| loss: 140.08865| val_0_rmse: 13.03013|  0:09:44s
epoch 126| loss: 143.22099| val_0_rmse: 12.98542|  0:09:49s
epoch 127| loss: 137.88362| val_0_rmse: 14.37339|  0:09:53s
epoch 128| loss: 139.91995| val_0_rmse: 13.14393|  0:09:58s
epoch 129| loss: 140.66274| val_0_rmse: 12.86712|  0:10:03s
epoch 130| loss: 138.78868| val_0_rmse: 13.4528 |  0:10:07s
epoch 131| loss: 139.92709| val_0_rmse: 12.83033|  0:10:12s
epoch 132| loss: 138.7924| val_0_rmse: 13.0599 |  0:10:16s
epoch 133| loss: 137.3217| val_0_rmse: 13.53032|  0:10:21s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 134| loss: 139.16091| val_0_rmse: 12.89891|  0:10:25s
Early stopping occurred at epoch 134 with best_epoch = 114 and best_val_0_rmse = 12.57141
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 23:46:00,211] Trial 30 finished with value: 12.571405475261583 and parameters: {'lr': 0.0024861729121470654, 'n_steps': 5, 'gamma': 1.071460381928484, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.019918680026082526, 'weight_decay': 0.0005632528458141484, 'batch_size': 256, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 9157.08374| val_0_rmse: 117.68696|  0:00:03s
epoch 1  | loss: 9028.3467| val_0_rmse: 100.62816|  0:00:07s
epoch 2  | loss: 8919.59133| val_0_rmse: 110.28783|  0:00:10s
epoch 3  | loss: 8795.60137| val_0_rmse: 96.05585|  0:00:14s
epoch 4  | loss: 8230.73177| val_0_rmse: 77.94   |  0:00:18s
epoch 5  | loss: 5907.05034| val_0_rmse: 83.12708|  0:00:21s
epoch 6  | loss: 2367.04391| val_0_rmse: 68.47008|  0:00:25s
epoch 7  | loss: 1131.61286| val_0_rmse: 32.9739 |  0:00:28s
epoch 8  | loss: 1004.15434| val_0_rmse: 33.47441|  0:00:32s
epoch 9  | loss: 946.28099| val_0_rmse: 30.99348|  0:00:35s
epoch 10 | loss: 903.27645| val_0_rmse: 30.2805 |  0:00:39s
epoch 11 | loss: 880.89576| val_0_rmse: 30.17875|  0:00:42s
epoch 12 | loss: 857.53586| val_0_rmse: 29.15218|  0:00:46s
epoch 13 | loss: 828.50613| val_0_rmse: 28.90284|  0:00:50s
epoch 14 | loss: 796.33729| val_0_rmse: 28.67832|  0:00:53s
epoch 15 | loss: 777.00634| val_0_rmse: 27.40409|  0:00:57s
epoch 16 | loss: 764.83129| val_0_rmse: 27.74904|  0:01:01s
epoch 17 | loss: 741.9455| val_0_rmse: 26.99765|  0:01:04s
epoch 18 | loss: 707.19385| val_0_rmse: 26.7604 |  0:01:08s
epoch 19 | loss: 670.37307| val_0_rmse: 26.36977|  0:01:11s
epoch 20 | loss: 640.3223| val_0_rmse: 25.02064|  0:01:15s
epoch 21 | loss: 606.85162| val_0_rmse: 25.77974|  0:01:19s
epoch 22 | loss: 581.75647| val_0_rmse: 24.35838|  0:01:22s
epoch 23 | loss: 549.79448| val_0_rmse: 24.07401|  0:01:26s
epoch 24 | loss: 526.90914| val_0_rmse: 23.28357|  0:01:29s
epoch 25 | loss: 496.56537| val_0_rmse: 22.03215|  0:01:33s
epoch 26 | loss: 476.7149| val_0_rmse: 21.82464|  0:01:36s
epoch 27 | loss: 462.09567| val_0_rmse: 21.30917|  0:01:40s
epoch 28 | loss: 444.39414| val_0_rmse: 21.30289|  0:01:43s
epoch 29 | loss: 431.13999| val_0_rmse: 20.76753|  0:01:47s
epoch 30 | loss: 420.14711| val_0_rmse: 20.49823|  0:01:51s
epoch 31 | loss: 410.21518| val_0_rmse: 20.0205 |  0:01:55s
epoch 32 | loss: 391.3871| val_0_rmse: 19.45437|  0:01:58s
epoch 33 | loss: 378.12173| val_0_rmse: 18.86572|  0:02:02s
epoch 34 | loss: 368.06723| val_0_rmse: 18.61693|  0:02:05s
epoch 35 | loss: 356.72305| val_0_rmse: 18.55431|  0:02:09s
epoch 36 | loss: 343.1555| val_0_rmse: 18.05059|  0:02:12s
epoch 37 | loss: 338.92355| val_0_rmse: 17.90013|  0:02:16s
epoch 38 | loss: 323.7131| val_0_rmse: 17.64604|  0:02:20s
epoch 39 | loss: 317.38358| val_0_rmse: 17.42143|  0:02:23s
epoch 40 | loss: 310.52227| val_0_rmse: 17.20572|  0:02:27s
epoch 41 | loss: 295.84308| val_0_rmse: 16.95336|  0:02:30s
epoch 42 | loss: 290.99687| val_0_rmse: 17.18067|  0:02:34s
epoch 43 | loss: 286.74107| val_0_rmse: 16.72537|  0:02:37s
epoch 44 | loss: 284.41256| val_0_rmse: 16.37178|  0:02:41s
epoch 45 | loss: 281.43479| val_0_rmse: 16.52013|  0:02:45s
epoch 46 | loss: 268.96079| val_0_rmse: 16.06928|  0:02:48s
epoch 47 | loss: 260.30076| val_0_rmse: 16.17526|  0:02:52s
epoch 48 | loss: 262.34886| val_0_rmse: 16.04711|  0:02:55s
epoch 49 | loss: 255.05605| val_0_rmse: 15.5499 |  0:02:59s
epoch 50 | loss: 249.83727| val_0_rmse: 15.41679|  0:03:03s
epoch 51 | loss: 243.14569| val_0_rmse: 15.61492|  0:03:06s
epoch 52 | loss: 238.7717| val_0_rmse: 15.33731|  0:03:10s
epoch 53 | loss: 238.68127| val_0_rmse: 15.47548|  0:03:13s
epoch 54 | loss: 229.79959| val_0_rmse: 15.38517|  0:03:17s
epoch 55 | loss: 229.20932| val_0_rmse: 15.55306|  0:03:21s
epoch 56 | loss: 230.28943| val_0_rmse: 15.22574|  0:03:24s
epoch 57 | loss: 226.56469| val_0_rmse: 14.9001 |  0:03:28s
epoch 58 | loss: 211.9703| val_0_rmse: 14.76629|  0:03:31s
epoch 59 | loss: 212.38178| val_0_rmse: 14.68998|  0:03:35s
epoch 60 | loss: 212.3026| val_0_rmse: 14.38956|  0:03:38s
epoch 61 | loss: 210.83139| val_0_rmse: 14.44297|  0:03:42s
epoch 62 | loss: 206.68407| val_0_rmse: 14.50308|  0:03:46s
epoch 63 | loss: 203.49704| val_0_rmse: 14.37068|  0:03:49s
epoch 64 | loss: 198.49242| val_0_rmse: 14.53519|  0:03:53s
epoch 65 | loss: 197.77689| val_0_rmse: 14.28091|  0:03:56s
epoch 66 | loss: 197.30152| val_0_rmse: 14.15872|  0:04:00s
epoch 67 | loss: 196.52543| val_0_rmse: 13.79552|  0:04:03s
epoch 68 | loss: 189.25866| val_0_rmse: 13.99325|  0:04:07s
epoch 69 | loss: 188.96269| val_0_rmse: 14.05529|  0:04:11s
epoch 70 | loss: 183.56729| val_0_rmse: 13.78931|  0:04:14s
epoch 71 | loss: 180.69672| val_0_rmse: 13.65629|  0:04:18s
epoch 72 | loss: 180.39513| val_0_rmse: 13.59039|  0:04:22s
epoch 73 | loss: 178.13987| val_0_rmse: 13.71815|  0:04:25s
epoch 74 | loss: 177.42902| val_0_rmse: 13.57604|  0:04:29s
epoch 75 | loss: 179.90509| val_0_rmse: 13.40492|  0:04:32s
epoch 76 | loss: 176.37652| val_0_rmse: 13.58548|  0:04:36s
epoch 77 | loss: 174.68255| val_0_rmse: 13.79394|  0:04:39s
epoch 78 | loss: 169.88681| val_0_rmse: 13.50223|  0:04:43s
epoch 79 | loss: 167.77345| val_0_rmse: 13.07668|  0:04:46s
epoch 80 | loss: 166.99741| val_0_rmse: 13.32538|  0:04:50s
epoch 81 | loss: 168.33456| val_0_rmse: 13.6004 |  0:04:54s
epoch 82 | loss: 170.47837| val_0_rmse: 12.99464|  0:04:57s
epoch 83 | loss: 161.10968| val_0_rmse: 13.50232|  0:05:01s
epoch 84 | loss: 159.51589| val_0_rmse: 13.10579|  0:05:04s
epoch 85 | loss: 161.8377| val_0_rmse: 13.02109|  0:05:08s
epoch 86 | loss: 160.69102| val_0_rmse: 13.11371|  0:05:11s
epoch 87 | loss: 157.89954| val_0_rmse: 13.20361|  0:05:15s
epoch 88 | loss: 160.09858| val_0_rmse: 12.88748|  0:05:19s
epoch 89 | loss: 154.90729| val_0_rmse: 13.06241|  0:05:22s
epoch 90 | loss: 157.12727| val_0_rmse: 13.04416|  0:05:26s
epoch 91 | loss: 152.29306| val_0_rmse: 12.87063|  0:05:29s
epoch 92 | loss: 152.9112| val_0_rmse: 12.97552|  0:05:33s
epoch 93 | loss: 149.31394| val_0_rmse: 12.90654|  0:05:36s
epoch 94 | loss: 149.72783| val_0_rmse: 12.57143|  0:05:40s
epoch 95 | loss: 149.80845| val_0_rmse: 12.70961|  0:05:43s
epoch 96 | loss: 149.44056| val_0_rmse: 12.99865|  0:05:47s
epoch 97 | loss: 144.84775| val_0_rmse: 12.8719 |  0:05:51s
epoch 98 | loss: 145.28908| val_0_rmse: 12.67213|  0:05:54s
epoch 99 | loss: 147.38924| val_0_rmse: 12.96878|  0:05:58s
epoch 100| loss: 147.82216| val_0_rmse: 12.96727|  0:06:02s
epoch 101| loss: 141.91489| val_0_rmse: 12.69681|  0:06:05s
epoch 102| loss: 141.58794| val_0_rmse: 12.79809|  0:06:09s
epoch 103| loss: 142.8643| val_0_rmse: 12.89028|  0:06:12s
epoch 104| loss: 140.55548| val_0_rmse: 12.74569|  0:06:16s
epoch 105| loss: 139.20649| val_0_rmse: 12.45643|  0:06:20s
epoch 106| loss: 139.06809| val_0_rmse: 12.73062|  0:06:23s
epoch 107| loss: 143.65509| val_0_rmse: 12.85268|  0:06:27s
epoch 108| loss: 138.98368| val_0_rmse: 12.63346|  0:06:30s
epoch 109| loss: 138.27464| val_0_rmse: 12.76632|  0:06:34s
epoch 110| loss: 136.00678| val_0_rmse: 12.82514|  0:06:37s
epoch 111| loss: 141.39228| val_0_rmse: 12.76241|  0:06:41s
epoch 112| loss: 134.06374| val_0_rmse: 12.52915|  0:06:44s
epoch 113| loss: 131.91844| val_0_rmse: 12.4903 |  0:06:48s
epoch 114| loss: 133.49414| val_0_rmse: 12.60323|  0:06:52s
epoch 115| loss: 135.91437| val_0_rmse: 12.67293|  0:06:55s
epoch 116| loss: 135.19814| val_0_rmse: 12.49055|  0:06:59s
epoch 117| loss: 130.5253| val_0_rmse: 12.46771|  0:07:02s
epoch 118| loss: 130.90191| val_0_rmse: 12.6385 |  0:07:06s
epoch 119| loss: 128.24732| val_0_rmse: 12.42775|  0:07:09s
epoch 120| loss: 128.67777| val_0_rmse: 12.85879|  0:07:13s
epoch 121| loss: 133.39281| val_0_rmse: 12.64799|  0:07:16s
epoch 122| loss: 129.84471| val_0_rmse: 12.45666|  0:07:20s
epoch 123| loss: 129.54201| val_0_rmse: 12.4878 |  0:07:23s
epoch 124| loss: 129.75115| val_0_rmse: 13.0316 |  0:07:27s
epoch 125| loss: 128.21196| val_0_rmse: 12.54772|  0:07:30s
epoch 126| loss: 121.56669| val_0_rmse: 13.13432|  0:07:34s
epoch 127| loss: 128.62829| val_0_rmse: 12.6144 |  0:07:37s
epoch 128| loss: 125.5354| val_0_rmse: 12.82092|  0:07:41s
epoch 129| loss: 123.00634| val_0_rmse: 12.84365|  0:07:44s
epoch 130| loss: 125.82481| val_0_rmse: 12.3784 |  0:07:48s
epoch 131| loss: 120.91057| val_0_rmse: 12.36253|  0:07:51s
epoch 132| loss: 122.04115| val_0_rmse: 12.4635 |  0:07:55s
epoch 133| loss: 126.22787| val_0_rmse: 12.28397|  0:07:58s
epoch 134| loss: 121.39653| val_0_rmse: 12.78565|  0:08:02s
epoch 135| loss: 120.30446| val_0_rmse: 12.45495|  0:08:05s
epoch 136| loss: 122.01051| val_0_rmse: 12.71806|  0:08:09s
epoch 137| loss: 122.13487| val_0_rmse: 12.61498|  0:08:12s
epoch 138| loss: 119.49645| val_0_rmse: 12.55126|  0:08:16s
epoch 139| loss: 120.62394| val_0_rmse: 12.33061|  0:08:20s
epoch 140| loss: 120.3267| val_0_rmse: 12.19317|  0:08:23s
epoch 141| loss: 121.65879| val_0_rmse: 11.9512 |  0:08:27s
epoch 142| loss: 117.98365| val_0_rmse: 12.29284|  0:08:30s
epoch 143| loss: 119.91794| val_0_rmse: 12.09429|  0:08:33s
epoch 144| loss: 117.07433| val_0_rmse: 12.20179|  0:08:37s
epoch 145| loss: 116.26083| val_0_rmse: 12.17188|  0:08:40s
epoch 146| loss: 117.06474| val_0_rmse: 12.07855|  0:08:44s
epoch 147| loss: 115.15796| val_0_rmse: 12.22566|  0:08:48s
epoch 148| loss: 118.90544| val_0_rmse: 12.37678|  0:08:51s
epoch 149| loss: 114.12502| val_0_rmse: 12.10157|  0:08:55s
epoch 150| loss: 117.65798| val_0_rmse: 12.34917|  0:08:58s
epoch 151| loss: 112.29966| val_0_rmse: 12.21817|  0:09:02s
epoch 152| loss: 112.68047| val_0_rmse: 12.08863|  0:09:05s
epoch 153| loss: 116.58005| val_0_rmse: 12.23318|  0:09:09s
epoch 154| loss: 116.06147| val_0_rmse: 12.38878|  0:09:12s
epoch 155| loss: 113.90139| val_0_rmse: 12.12023|  0:09:16s
epoch 156| loss: 110.15094| val_0_rmse: 12.14699|  0:09:20s
epoch 157| loss: 114.57569| val_0_rmse: 12.28966|  0:09:23s
epoch 158| loss: 112.47336| val_0_rmse: 11.96839|  0:09:27s
epoch 159| loss: 111.62112| val_0_rmse: 12.3756 |  0:09:30s
epoch 160| loss: 109.59827| val_0_rmse: 12.01422|  0:09:34s
epoch 161| loss: 112.45997| val_0_rmse: 11.90367|  0:09:37s
epoch 162| loss: 109.04676| val_0_rmse: 12.42276|  0:09:41s
epoch 163| loss: 111.93176| val_0_rmse: 12.139  |  0:09:44s
epoch 164| loss: 106.38112| val_0_rmse: 12.37282|  0:09:48s
epoch 165| loss: 108.75728| val_0_rmse: 12.13193|  0:09:51s
epoch 166| loss: 109.92082| val_0_rmse: 12.2028 |  0:09:55s
epoch 167| loss: 108.73  | val_0_rmse: 12.45649|  0:09:58s
epoch 168| loss: 112.00626| val_0_rmse: 12.1536 |  0:10:02s
epoch 169| loss: 104.62418| val_0_rmse: 11.9291 |  0:10:05s
epoch 170| loss: 107.72547| val_0_rmse: 12.57784|  0:10:09s
epoch 171| loss: 106.8902| val_0_rmse: 11.8719 |  0:10:12s
epoch 172| loss: 105.94211| val_0_rmse: 12.18751|  0:10:16s
epoch 173| loss: 107.46364| val_0_rmse: 12.07082|  0:10:19s
epoch 174| loss: 105.03463| val_0_rmse: 12.30144|  0:10:23s
epoch 175| loss: 110.68832| val_0_rmse: 12.197  |  0:10:26s
epoch 176| loss: 109.31701| val_0_rmse: 12.33525|  0:10:30s
epoch 177| loss: 110.05412| val_0_rmse: 12.04569|  0:10:33s
epoch 178| loss: 106.61321| val_0_rmse: 11.92495|  0:10:37s
epoch 179| loss: 103.61157| val_0_rmse: 12.46217|  0:10:40s
epoch 180| loss: 107.60533| val_0_rmse: 12.14903|  0:10:44s
epoch 181| loss: 104.93866| val_0_rmse: 12.20489|  0:10:47s
epoch 182| loss: 105.7195| val_0_rmse: 11.81445|  0:10:51s
epoch 183| loss: 104.09804| val_0_rmse: 11.81474|  0:10:55s
epoch 184| loss: 104.00653| val_0_rmse: 11.95063|  0:10:58s
epoch 185| loss: 99.52997| val_0_rmse: 11.92463|  0:11:01s
epoch 186| loss: 102.58261| val_0_rmse: 11.7727 |  0:11:05s
epoch 187| loss: 102.96597| val_0_rmse: 12.13329|  0:11:08s
epoch 188| loss: 100.59697| val_0_rmse: 11.92148|  0:11:12s
epoch 189| loss: 100.72959| val_0_rmse: 12.00894|  0:11:15s
epoch 190| loss: 103.55059| val_0_rmse: 11.94817|  0:11:19s
epoch 191| loss: 102.64155| val_0_rmse: 11.82641|  0:11:23s
epoch 192| loss: 100.01728| val_0_rmse: 11.90344|  0:11:26s
epoch 193| loss: 99.21146| val_0_rmse: 11.6666 |  0:11:30s
epoch 194| loss: 101.77212| val_0_rmse: 11.47412|  0:11:33s
epoch 195| loss: 104.18889| val_0_rmse: 12.02409|  0:11:37s
epoch 196| loss: 98.79051| val_0_rmse: 12.10674|  0:11:40s
epoch 197| loss: 100.59732| val_0_rmse: 11.62412|  0:11:43s
epoch 198| loss: 100.62867| val_0_rmse: 11.65348|  0:11:47s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 199| loss: 103.68821| val_0_rmse: 11.873  |  0:11:51s
Stop training because you reached max_epochs = 200 with best_epoch = 194 and best_val_0_rmse = 11.47412
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-11 23:58:00,158] Trial 31 finished with value: 11.474122922916827 and parameters: {'lr': 0.0037158896569999653, 'n_steps': 7, 'gamma': 1.1575546542575719, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0447881072406309, 'weight_decay': 0.00027677178328633796, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8953.60443| val_0_rmse: 160.93368|  0:00:03s
epoch 1  | loss: 8891.58259| val_0_rmse: 95.40671|  0:00:07s
epoch 2  | loss: 8838.07094| val_0_rmse: 99.40825|  0:00:11s
epoch 3  | loss: 8764.72332| val_0_rmse: 89.73387|  0:00:15s
epoch 4  | loss: 8589.61145| val_0_rmse: 76.08657|  0:00:19s
epoch 5  | loss: 7458.467| val_0_rmse: 77.53192|  0:00:23s
epoch 6  | loss: 3569.33763| val_0_rmse: 48.51589|  0:00:27s
epoch 7  | loss: 1270.7974| val_0_rmse: 36.73413|  0:00:31s
epoch 8  | loss: 1010.8517| val_0_rmse: 34.71552|  0:00:35s
epoch 9  | loss: 973.09241| val_0_rmse: 32.83981|  0:00:39s
epoch 10 | loss: 949.02418| val_0_rmse: 31.62056|  0:00:42s
epoch 11 | loss: 919.90727| val_0_rmse: 30.33694|  0:00:47s
epoch 12 | loss: 899.44678| val_0_rmse: 29.43687|  0:00:51s
epoch 13 | loss: 870.7047| val_0_rmse: 29.3878 |  0:00:54s
epoch 14 | loss: 849.08414| val_0_rmse: 30.05219|  0:00:58s
epoch 15 | loss: 843.47486| val_0_rmse: 29.38053|  0:01:02s
epoch 16 | loss: 830.83324| val_0_rmse: 29.60821|  0:01:06s
epoch 17 | loss: 827.90667| val_0_rmse: 29.08466|  0:01:10s
epoch 18 | loss: 810.96135| val_0_rmse: 28.5601 |  0:01:14s
epoch 19 | loss: 812.98534| val_0_rmse: 28.28469|  0:01:18s
epoch 20 | loss: 801.14118| val_0_rmse: 28.41629|  0:01:22s
epoch 21 | loss: 792.10599| val_0_rmse: 28.2781 |  0:01:26s
epoch 22 | loss: 773.65403| val_0_rmse: 27.52481|  0:01:30s
epoch 23 | loss: 764.97452| val_0_rmse: 27.87411|  0:01:34s
epoch 24 | loss: 749.39751| val_0_rmse: 27.38355|  0:01:37s
epoch 25 | loss: 711.41802| val_0_rmse: 26.35359|  0:01:41s
epoch 26 | loss: 659.32523| val_0_rmse: 25.52515|  0:01:45s
epoch 27 | loss: 616.19177| val_0_rmse: 25.89531|  0:01:49s
epoch 28 | loss: 584.4026| val_0_rmse: 24.39054|  0:01:53s
epoch 29 | loss: 544.5478| val_0_rmse: 23.21601|  0:01:57s
epoch 30 | loss: 507.71603| val_0_rmse: 23.59379|  0:02:01s
epoch 31 | loss: 474.95159| val_0_rmse: 22.47941|  0:02:05s
epoch 32 | loss: 449.58491| val_0_rmse: 23.47922|  0:02:09s
epoch 33 | loss: 429.79293| val_0_rmse: 21.43647|  0:02:13s
epoch 34 | loss: 411.52988| val_0_rmse: 21.44651|  0:02:17s
epoch 35 | loss: 403.72876| val_0_rmse: 19.95219|  0:02:21s
epoch 36 | loss: 386.49155| val_0_rmse: 19.4221 |  0:02:25s
epoch 37 | loss: 369.09329| val_0_rmse: 19.37207|  0:02:28s
epoch 38 | loss: 358.0204| val_0_rmse: 18.78407|  0:02:32s
epoch 39 | loss: 341.63184| val_0_rmse: 18.24437|  0:02:36s
epoch 40 | loss: 331.75964| val_0_rmse: 17.77341|  0:02:40s
epoch 41 | loss: 322.13592| val_0_rmse: 18.00623|  0:02:44s
epoch 42 | loss: 313.84163| val_0_rmse: 18.42328|  0:02:48s
epoch 43 | loss: 300.69426| val_0_rmse: 17.25833|  0:02:52s
epoch 44 | loss: 289.03981| val_0_rmse: 16.95811|  0:02:56s
epoch 45 | loss: 283.09281| val_0_rmse: 16.68485|  0:03:00s
epoch 46 | loss: 279.33785| val_0_rmse: 16.24433|  0:03:04s
epoch 47 | loss: 265.75931| val_0_rmse: 16.11747|  0:03:07s
epoch 48 | loss: 256.33766| val_0_rmse: 15.89888|  0:03:11s
epoch 49 | loss: 257.73828| val_0_rmse: 15.48761|  0:03:15s
epoch 50 | loss: 249.09993| val_0_rmse: 15.74057|  0:03:20s
epoch 51 | loss: 242.31423| val_0_rmse: 15.72144|  0:03:23s
epoch 52 | loss: 239.37767| val_0_rmse: 15.36945|  0:03:27s
epoch 53 | loss: 231.62963| val_0_rmse: 15.69708|  0:03:31s
epoch 54 | loss: 230.44511| val_0_rmse: 14.84314|  0:03:35s
epoch 55 | loss: 221.90299| val_0_rmse: 14.96087|  0:03:39s
epoch 56 | loss: 216.04521| val_0_rmse: 14.74201|  0:03:43s
epoch 57 | loss: 208.83694| val_0_rmse: 14.64895|  0:03:47s
epoch 58 | loss: 216.63195| val_0_rmse: 14.77506|  0:03:51s
epoch 59 | loss: 207.05904| val_0_rmse: 14.57961|  0:03:55s
epoch 60 | loss: 200.13941| val_0_rmse: 14.41426|  0:03:58s
epoch 61 | loss: 200.45677| val_0_rmse: 14.51239|  0:04:02s
epoch 62 | loss: 200.01034| val_0_rmse: 14.32848|  0:04:06s
epoch 63 | loss: 196.30442| val_0_rmse: 14.37163|  0:04:10s
epoch 64 | loss: 194.87526| val_0_rmse: 14.23364|  0:04:14s
epoch 65 | loss: 190.22902| val_0_rmse: 14.23458|  0:04:18s
epoch 66 | loss: 187.70336| val_0_rmse: 14.15246|  0:04:22s
epoch 67 | loss: 192.13016| val_0_rmse: 14.2884 |  0:04:26s
epoch 68 | loss: 185.66487| val_0_rmse: 13.97871|  0:04:29s
epoch 69 | loss: 187.51201| val_0_rmse: 13.94906|  0:04:33s
epoch 70 | loss: 186.40853| val_0_rmse: 13.9748 |  0:04:37s
epoch 71 | loss: 181.99418| val_0_rmse: 13.73205|  0:04:41s
epoch 72 | loss: 177.49236| val_0_rmse: 13.66477|  0:04:45s
epoch 73 | loss: 180.47033| val_0_rmse: 13.77091|  0:04:49s
epoch 74 | loss: 172.65813| val_0_rmse: 13.24182|  0:04:53s
epoch 75 | loss: 177.17752| val_0_rmse: 13.43538|  0:04:56s
epoch 76 | loss: 171.71922| val_0_rmse: 13.28992|  0:05:00s
epoch 77 | loss: 167.76238| val_0_rmse: 13.25299|  0:05:04s
epoch 78 | loss: 169.58846| val_0_rmse: 13.24264|  0:05:08s
epoch 79 | loss: 170.69916| val_0_rmse: 13.41774|  0:05:12s
epoch 80 | loss: 169.31851| val_0_rmse: 13.31412|  0:05:15s
epoch 81 | loss: 167.17776| val_0_rmse: 13.16022|  0:05:20s
epoch 82 | loss: 164.80221| val_0_rmse: 13.12604|  0:05:23s
epoch 83 | loss: 163.65975| val_0_rmse: 13.08004|  0:05:27s
epoch 84 | loss: 162.71453| val_0_rmse: 13.01469|  0:05:31s
epoch 85 | loss: 162.70013| val_0_rmse: 12.9492 |  0:05:35s
epoch 86 | loss: 159.57599| val_0_rmse: 13.33563|  0:05:39s
epoch 87 | loss: 159.76191| val_0_rmse: 12.82864|  0:05:42s
epoch 88 | loss: 158.78403| val_0_rmse: 12.88092|  0:05:46s
epoch 89 | loss: 157.50438| val_0_rmse: 12.97653|  0:05:50s
epoch 90 | loss: 155.51752| val_0_rmse: 12.81146|  0:05:54s
epoch 91 | loss: 150.58265| val_0_rmse: 12.66228|  0:05:58s
epoch 92 | loss: 154.79586| val_0_rmse: 12.99202|  0:06:02s
epoch 93 | loss: 153.4423| val_0_rmse: 12.88382|  0:06:06s
epoch 94 | loss: 149.19194| val_0_rmse: 13.04483|  0:06:09s
epoch 95 | loss: 149.18113| val_0_rmse: 13.23091|  0:06:13s
epoch 96 | loss: 154.78622| val_0_rmse: 12.63317|  0:06:17s
epoch 97 | loss: 151.42143| val_0_rmse: 12.74108|  0:06:21s
epoch 98 | loss: 147.39581| val_0_rmse: 12.63968|  0:06:25s
epoch 99 | loss: 150.6515| val_0_rmse: 12.68378|  0:06:29s
epoch 100| loss: 144.53217| val_0_rmse: 12.73936|  0:06:32s
epoch 101| loss: 145.50292| val_0_rmse: 12.7431 |  0:06:36s
epoch 102| loss: 145.04572| val_0_rmse: 12.64724|  0:06:40s
epoch 103| loss: 144.31633| val_0_rmse: 12.97157|  0:06:44s
epoch 104| loss: 146.84967| val_0_rmse: 12.64669|  0:06:48s
epoch 105| loss: 142.53963| val_0_rmse: 12.71563|  0:06:52s
epoch 106| loss: 141.65843| val_0_rmse: 12.70138|  0:06:55s
epoch 107| loss: 145.30832| val_0_rmse: 12.64536|  0:06:59s
epoch 108| loss: 138.52318| val_0_rmse: 12.7971 |  0:07:03s
epoch 109| loss: 140.65211| val_0_rmse: 12.40173|  0:07:07s
epoch 110| loss: 139.26629| val_0_rmse: 12.44352|  0:07:10s
epoch 111| loss: 136.62412| val_0_rmse: 12.51222|  0:07:14s
epoch 112| loss: 137.41657| val_0_rmse: 12.09785|  0:07:18s
epoch 113| loss: 136.55639| val_0_rmse: 12.56059|  0:07:22s
epoch 114| loss: 133.80862| val_0_rmse: 12.65283|  0:07:26s
epoch 115| loss: 137.71362| val_0_rmse: 12.57219|  0:07:30s
epoch 116| loss: 136.62419| val_0_rmse: 12.45871|  0:07:34s
epoch 117| loss: 135.14164| val_0_rmse: 12.51569|  0:07:37s
epoch 118| loss: 137.34269| val_0_rmse: 12.32668|  0:07:41s
epoch 119| loss: 133.1671| val_0_rmse: 12.26597|  0:07:45s
epoch 120| loss: 129.08249| val_0_rmse: 12.08623|  0:07:49s
epoch 121| loss: 131.90325| val_0_rmse: 12.22595|  0:07:53s
epoch 122| loss: 131.93632| val_0_rmse: 12.10192|  0:07:57s
epoch 123| loss: 129.07627| val_0_rmse: 12.43592|  0:08:01s
epoch 124| loss: 125.71296| val_0_rmse: 12.64944|  0:08:04s
epoch 125| loss: 126.37647| val_0_rmse: 12.51791|  0:08:08s
epoch 126| loss: 129.90355| val_0_rmse: 12.41375|  0:08:12s
epoch 127| loss: 127.1388| val_0_rmse: 12.08213|  0:08:16s
epoch 128| loss: 125.45015| val_0_rmse: 12.27959|  0:08:20s
epoch 129| loss: 126.64205| val_0_rmse: 12.12201|  0:08:24s
epoch 130| loss: 128.19901| val_0_rmse: 12.27665|  0:08:27s
epoch 131| loss: 124.9553| val_0_rmse: 12.54362|  0:08:31s
epoch 132| loss: 122.82552| val_0_rmse: 12.19109|  0:08:35s
epoch 133| loss: 121.31421| val_0_rmse: 12.41575|  0:08:39s
epoch 134| loss: 126.27056| val_0_rmse: 12.02502|  0:08:43s
epoch 135| loss: 125.12603| val_0_rmse: 12.31737|  0:08:47s
epoch 136| loss: 122.68558| val_0_rmse: 12.31716|  0:08:51s
epoch 137| loss: 126.02289| val_0_rmse: 12.28161|  0:08:54s
epoch 138| loss: 122.69405| val_0_rmse: 11.94877|  0:08:58s
epoch 139| loss: 122.55133| val_0_rmse: 12.10045|  0:09:02s
epoch 140| loss: 121.39852| val_0_rmse: 12.28162|  0:09:06s
epoch 141| loss: 119.47352| val_0_rmse: 12.27492|  0:09:10s
epoch 142| loss: 122.30169| val_0_rmse: 12.15958|  0:09:13s
epoch 143| loss: 120.5517| val_0_rmse: 12.36697|  0:09:18s
epoch 144| loss: 124.15497| val_0_rmse: 12.15201|  0:09:21s
epoch 145| loss: 119.99168| val_0_rmse: 12.18738|  0:09:25s
epoch 146| loss: 119.57833| val_0_rmse: 12.02349|  0:09:29s
epoch 147| loss: 117.11643| val_0_rmse: 12.40667|  0:09:33s
epoch 148| loss: 114.86944| val_0_rmse: 11.99894|  0:09:36s
epoch 149| loss: 114.71378| val_0_rmse: 12.28747|  0:09:40s
epoch 150| loss: 115.84511| val_0_rmse: 12.25702|  0:09:44s
epoch 151| loss: 115.15393| val_0_rmse: 12.2331 |  0:09:48s
epoch 152| loss: 116.53798| val_0_rmse: 11.8892 |  0:09:52s
epoch 153| loss: 117.12384| val_0_rmse: 12.18828|  0:09:56s
epoch 154| loss: 113.9295| val_0_rmse: 11.9844 |  0:10:00s
epoch 155| loss: 113.32791| val_0_rmse: 12.07899|  0:10:03s
epoch 156| loss: 113.60542| val_0_rmse: 12.24076|  0:10:07s
epoch 157| loss: 109.43541| val_0_rmse: 12.57448|  0:10:11s
epoch 158| loss: 114.25962| val_0_rmse: 12.50451|  0:10:15s
epoch 159| loss: 115.5925| val_0_rmse: 12.17207|  0:10:19s
epoch 160| loss: 115.9626| val_0_rmse: 12.40183|  0:10:23s
epoch 161| loss: 108.86151| val_0_rmse: 12.45327|  0:10:26s
epoch 162| loss: 109.29595| val_0_rmse: 11.98674|  0:10:30s
epoch 163| loss: 113.65618| val_0_rmse: 12.14659|  0:10:34s
epoch 164| loss: 109.84603| val_0_rmse: 12.26467|  0:10:38s
epoch 165| loss: 109.56372| val_0_rmse: 12.0314 |  0:10:42s
epoch 166| loss: 108.12055| val_0_rmse: 12.28685|  0:10:46s
epoch 167| loss: 106.89349| val_0_rmse: 12.02489|  0:10:50s
epoch 168| loss: 107.98862| val_0_rmse: 12.11406|  0:10:54s
epoch 169| loss: 109.90839| val_0_rmse: 12.17135|  0:10:57s
epoch 170| loss: 112.54138| val_0_rmse: 12.12059|  0:11:01s
epoch 171| loss: 112.11004| val_0_rmse: 12.20274|  0:11:05s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 172| loss: 112.97419| val_0_rmse: 12.04558|  0:11:09s
Early stopping occurred at epoch 172 with best_epoch = 152 and best_val_0_rmse = 11.8892
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-12 00:09:19,214] Trial 32 finished with value: 11.889199705592196 and parameters: {'lr': 0.0038872941609897726, 'n_steps': 8, 'gamma': 1.1710009384238826, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.03504888657864026, 'weight_decay': 0.00024207254620678127, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8757.98258| val_0_rmse: 93.14083|  0:00:04s
epoch 1  | loss: 8803.62473| val_0_rmse: 95.38344|  0:00:08s
epoch 2  | loss: 8829.33051| val_0_rmse: 94.13206|  0:00:12s
epoch 3  | loss: 8833.72922| val_0_rmse: 94.82288|  0:00:16s
epoch 4  | loss: 8822.11616| val_0_rmse: 94.62259|  0:00:20s
epoch 5  | loss: 8794.22605| val_0_rmse: 94.5213 |  0:00:24s
epoch 6  | loss: 8700.8329| val_0_rmse: 93.9383 |  0:00:29s
epoch 7  | loss: 8298.51218| val_0_rmse: 88.70199|  0:00:33s
epoch 8  | loss: 6349.44237| val_0_rmse: 68.56157|  0:00:37s
epoch 9  | loss: 2752.06746| val_0_rmse: 39.88658|  0:00:42s
epoch 10 | loss: 1271.17257| val_0_rmse: 34.55689|  0:00:46s
epoch 11 | loss: 1090.00536| val_0_rmse: 32.88971|  0:00:50s
epoch 12 | loss: 1043.58083| val_0_rmse: 32.02279|  0:00:54s
epoch 13 | loss: 993.39634| val_0_rmse: 31.85847|  0:00:59s
epoch 14 | loss: 949.42801| val_0_rmse: 31.76523|  0:01:03s
epoch 15 | loss: 941.0552| val_0_rmse: 31.26952|  0:01:07s
epoch 16 | loss: 929.59007| val_0_rmse: 31.48509|  0:01:11s
epoch 17 | loss: 903.93198| val_0_rmse: 31.74222|  0:01:15s
epoch 18 | loss: 887.68134| val_0_rmse: 30.49859|  0:01:20s
epoch 19 | loss: 876.14025| val_0_rmse: 33.17539|  0:01:24s
epoch 20 | loss: 866.75976| val_0_rmse: 30.54839|  0:01:28s
epoch 21 | loss: 850.50933| val_0_rmse: 32.62432|  0:01:32s
epoch 22 | loss: 850.53787| val_0_rmse: 29.75918|  0:01:37s
epoch 23 | loss: 839.29227| val_0_rmse: 29.87461|  0:01:41s
epoch 24 | loss: 815.53696| val_0_rmse: 30.99206|  0:01:45s
epoch 25 | loss: 804.63559| val_0_rmse: 29.77872|  0:01:49s
epoch 26 | loss: 795.03912| val_0_rmse: 28.93233|  0:01:53s
epoch 27 | loss: 773.36995| val_0_rmse: 28.13575|  0:01:57s
epoch 28 | loss: 755.80338| val_0_rmse: 27.58358|  0:02:02s
epoch 29 | loss: 735.66561| val_0_rmse: 27.76479|  0:02:06s
epoch 30 | loss: 714.70344| val_0_rmse: 27.21646|  0:02:10s
epoch 31 | loss: 684.27808| val_0_rmse: 27.11236|  0:02:14s
epoch 32 | loss: 654.20048| val_0_rmse: 25.88964|  0:02:18s
epoch 33 | loss: 624.83857| val_0_rmse: 25.5923 |  0:02:22s
epoch 34 | loss: 585.25792| val_0_rmse: 24.98974|  0:02:27s
epoch 35 | loss: 555.12641| val_0_rmse: 24.31502|  0:02:31s
epoch 36 | loss: 534.62654| val_0_rmse: 23.77177|  0:02:35s
epoch 37 | loss: 515.00192| val_0_rmse: 23.44126|  0:02:39s
epoch 38 | loss: 492.97931| val_0_rmse: 23.56211|  0:02:43s
epoch 39 | loss: 475.51954| val_0_rmse: 22.44814|  0:02:47s
epoch 40 | loss: 450.34532| val_0_rmse: 22.31555|  0:02:52s
epoch 41 | loss: 438.45078| val_0_rmse: 22.07518|  0:02:56s
epoch 42 | loss: 426.519 | val_0_rmse: 21.89623|  0:03:00s
epoch 43 | loss: 412.92218| val_0_rmse: 21.49887|  0:03:04s
epoch 44 | loss: 399.15404| val_0_rmse: 21.42465|  0:03:08s
epoch 45 | loss: 386.81177| val_0_rmse: 20.57356|  0:03:13s
epoch 46 | loss: 374.39898| val_0_rmse: 20.32156|  0:03:17s
epoch 47 | loss: 361.69489| val_0_rmse: 19.46733|  0:03:21s
epoch 48 | loss: 356.06395| val_0_rmse: 19.66437|  0:03:25s
epoch 49 | loss: 346.16987| val_0_rmse: 18.84829|  0:03:29s
epoch 50 | loss: 332.1177| val_0_rmse: 19.44853|  0:03:34s
epoch 51 | loss: 326.76296| val_0_rmse: 18.70774|  0:03:38s
epoch 52 | loss: 316.64913| val_0_rmse: 18.9652 |  0:03:42s
epoch 53 | loss: 306.21933| val_0_rmse: 18.29025|  0:03:46s
epoch 54 | loss: 302.26795| val_0_rmse: 18.23005|  0:03:50s
epoch 55 | loss: 298.29907| val_0_rmse: 17.52936|  0:03:54s
epoch 56 | loss: 286.0379| val_0_rmse: 17.23257|  0:03:59s
epoch 57 | loss: 282.92453| val_0_rmse: 17.00779|  0:04:03s
epoch 58 | loss: 274.94294| val_0_rmse: 17.54933|  0:04:07s
epoch 59 | loss: 271.74653| val_0_rmse: 16.83598|  0:04:11s
epoch 60 | loss: 265.66394| val_0_rmse: 16.2856 |  0:04:15s
epoch 61 | loss: 258.18442| val_0_rmse: 16.18637|  0:04:19s
epoch 62 | loss: 255.44152| val_0_rmse: 16.1825 |  0:04:23s
epoch 63 | loss: 247.62605| val_0_rmse: 16.20565|  0:04:28s
epoch 64 | loss: 245.36608| val_0_rmse: 15.94945|  0:04:32s
epoch 65 | loss: 244.77096| val_0_rmse: 15.94003|  0:04:36s
epoch 66 | loss: 240.70765| val_0_rmse: 15.70242|  0:04:40s
epoch 67 | loss: 236.34985| val_0_rmse: 15.5865 |  0:04:44s
epoch 68 | loss: 228.95982| val_0_rmse: 15.41002|  0:04:48s
epoch 69 | loss: 229.97308| val_0_rmse: 15.47988|  0:04:53s
epoch 70 | loss: 227.4606| val_0_rmse: 15.12652|  0:04:57s
epoch 71 | loss: 219.29708| val_0_rmse: 15.25746|  0:05:01s
epoch 72 | loss: 220.91088| val_0_rmse: 15.34283|  0:05:05s
epoch 73 | loss: 218.33781| val_0_rmse: 15.25617|  0:05:09s
epoch 74 | loss: 214.03889| val_0_rmse: 15.52991|  0:05:14s
epoch 75 | loss: 205.88983| val_0_rmse: 15.12226|  0:05:18s
epoch 76 | loss: 210.54105| val_0_rmse: 14.81532|  0:05:22s
epoch 77 | loss: 205.23703| val_0_rmse: 14.86854|  0:05:26s
epoch 78 | loss: 206.80315| val_0_rmse: 15.01641|  0:05:30s
epoch 79 | loss: 200.95439| val_0_rmse: 15.12724|  0:05:35s
epoch 80 | loss: 196.04804| val_0_rmse: 14.76637|  0:05:39s
epoch 81 | loss: 198.42358| val_0_rmse: 14.85545|  0:05:43s
epoch 82 | loss: 196.70554| val_0_rmse: 14.43004|  0:05:47s
epoch 83 | loss: 189.62606| val_0_rmse: 14.57343|  0:05:51s
epoch 84 | loss: 190.97577| val_0_rmse: 14.55006|  0:05:55s
epoch 85 | loss: 192.29021| val_0_rmse: 14.80822|  0:06:00s
epoch 86 | loss: 183.72889| val_0_rmse: 14.57418|  0:06:04s
epoch 87 | loss: 191.81956| val_0_rmse: 14.41126|  0:06:08s
epoch 88 | loss: 183.06938| val_0_rmse: 14.31287|  0:06:12s
epoch 89 | loss: 179.85363| val_0_rmse: 14.06193|  0:06:16s
epoch 90 | loss: 177.01489| val_0_rmse: 14.16107|  0:06:20s
epoch 91 | loss: 182.34262| val_0_rmse: 13.99061|  0:06:24s
epoch 92 | loss: 174.44155| val_0_rmse: 13.92638|  0:06:29s
epoch 93 | loss: 175.39183| val_0_rmse: 14.15367|  0:06:33s
epoch 94 | loss: 172.50458| val_0_rmse: 13.93356|  0:06:37s
epoch 95 | loss: 171.41053| val_0_rmse: 13.96688|  0:06:41s
epoch 96 | loss: 168.99275| val_0_rmse: 13.63902|  0:06:45s
epoch 97 | loss: 170.97158| val_0_rmse: 14.13907|  0:06:49s
epoch 98 | loss: 168.34435| val_0_rmse: 13.87483|  0:06:53s
epoch 99 | loss: 161.59634| val_0_rmse: 13.63233|  0:06:58s
epoch 100| loss: 167.36317| val_0_rmse: 13.69399|  0:07:02s
epoch 101| loss: 163.319 | val_0_rmse: 13.81499|  0:07:06s
epoch 102| loss: 158.10289| val_0_rmse: 13.35393|  0:07:10s
epoch 103| loss: 159.74855| val_0_rmse: 13.54661|  0:07:14s
epoch 104| loss: 160.14068| val_0_rmse: 13.69186|  0:07:18s
epoch 105| loss: 154.98079| val_0_rmse: 13.50786|  0:07:22s
epoch 106| loss: 153.6987| val_0_rmse: 14.24359|  0:07:27s
epoch 107| loss: 157.63562| val_0_rmse: 13.42855|  0:07:31s
epoch 108| loss: 153.84569| val_0_rmse: 13.32538|  0:07:35s
epoch 109| loss: 155.55518| val_0_rmse: 13.3924 |  0:07:39s
epoch 110| loss: 155.20579| val_0_rmse: 13.34384|  0:07:43s
epoch 111| loss: 153.22868| val_0_rmse: 13.49191|  0:07:47s
epoch 112| loss: 151.69375| val_0_rmse: 13.19392|  0:07:52s
epoch 113| loss: 150.86046| val_0_rmse: 13.27173|  0:07:56s
epoch 114| loss: 148.74142| val_0_rmse: 13.37469|  0:08:00s
epoch 115| loss: 147.96083| val_0_rmse: 13.10193|  0:08:04s
epoch 116| loss: 147.19076| val_0_rmse: 13.28487|  0:08:08s
epoch 117| loss: 145.49132| val_0_rmse: 13.17294|  0:08:12s
epoch 118| loss: 146.15559| val_0_rmse: 13.0067 |  0:08:17s
epoch 119| loss: 143.80934| val_0_rmse: 12.97989|  0:08:21s
epoch 120| loss: 141.01472| val_0_rmse: 13.08189|  0:08:25s
epoch 121| loss: 141.43883| val_0_rmse: 13.12933|  0:08:29s
epoch 122| loss: 138.95728| val_0_rmse: 12.97439|  0:08:34s
epoch 123| loss: 141.69863| val_0_rmse: 13.11486|  0:08:38s
epoch 124| loss: 140.72336| val_0_rmse: 13.05212|  0:08:42s
epoch 125| loss: 137.68185| val_0_rmse: 13.13052|  0:08:46s
epoch 126| loss: 139.97035| val_0_rmse: 12.88778|  0:08:50s
epoch 127| loss: 134.2443| val_0_rmse: 12.65879|  0:08:54s
epoch 128| loss: 134.78579| val_0_rmse: 12.62133|  0:08:58s
epoch 129| loss: 129.56923| val_0_rmse: 12.88265|  0:09:03s
epoch 130| loss: 135.04769| val_0_rmse: 13.09561|  0:09:07s
epoch 131| loss: 128.46786| val_0_rmse: 13.11816|  0:09:11s
epoch 132| loss: 137.98357| val_0_rmse: 12.86797|  0:09:15s
epoch 133| loss: 131.34155| val_0_rmse: 12.50435|  0:09:19s
epoch 134| loss: 131.98425| val_0_rmse: 12.46193|  0:09:23s
epoch 135| loss: 128.54256| val_0_rmse: 12.50488|  0:09:28s
epoch 136| loss: 128.86043| val_0_rmse: 12.66763|  0:09:32s
epoch 137| loss: 125.12139| val_0_rmse: 12.76301|  0:09:36s
epoch 138| loss: 128.42119| val_0_rmse: 12.59605|  0:09:40s
epoch 139| loss: 121.78006| val_0_rmse: 12.75785|  0:09:44s
epoch 140| loss: 130.33074| val_0_rmse: 12.7762 |  0:09:48s
epoch 141| loss: 123.60042| val_0_rmse: 12.67956|  0:09:52s
epoch 142| loss: 121.6992| val_0_rmse: 12.85896|  0:09:57s
epoch 143| loss: 123.87858| val_0_rmse: 12.81705|  0:10:01s
epoch 144| loss: 122.81519| val_0_rmse: 13.09654|  0:10:05s
epoch 145| loss: 122.10632| val_0_rmse: 12.73738|  0:10:09s
epoch 146| loss: 124.47755| val_0_rmse: 12.4475 |  0:10:13s
epoch 147| loss: 119.45409| val_0_rmse: 12.77503|  0:10:17s
epoch 148| loss: 118.94625| val_0_rmse: 12.52106|  0:10:22s
epoch 149| loss: 119.8332| val_0_rmse: 12.67511|  0:10:26s
epoch 150| loss: 118.16551| val_0_rmse: 12.57082|  0:10:30s
epoch 151| loss: 116.36032| val_0_rmse: 12.47658|  0:10:34s
epoch 152| loss: 119.637 | val_0_rmse: 12.47269|  0:10:38s
epoch 153| loss: 117.82513| val_0_rmse: 12.43186|  0:10:43s
epoch 154| loss: 115.41742| val_0_rmse: 12.30332|  0:10:47s
epoch 155| loss: 117.17287| val_0_rmse: 12.35827|  0:10:51s
epoch 156| loss: 115.41305| val_0_rmse: 12.3072 |  0:10:55s
epoch 157| loss: 119.59375| val_0_rmse: 12.45629|  0:10:59s
epoch 158| loss: 111.28129| val_0_rmse: 12.58396|  0:11:03s
epoch 159| loss: 117.24372| val_0_rmse: 12.50776|  0:11:08s
epoch 160| loss: 115.92151| val_0_rmse: 12.51998|  0:11:12s
epoch 161| loss: 114.55047| val_0_rmse: 12.31565|  0:11:16s
epoch 162| loss: 114.70807| val_0_rmse: 12.95379|  0:11:20s
epoch 163| loss: 112.75465| val_0_rmse: 12.37911|  0:11:24s
epoch 164| loss: 111.68494| val_0_rmse: 12.43336|  0:11:28s
epoch 165| loss: 111.87889| val_0_rmse: 12.77067|  0:11:33s
epoch 166| loss: 114.66286| val_0_rmse: 12.4932 |  0:11:37s
epoch 167| loss: 108.87901| val_0_rmse: 12.60236|  0:11:41s
epoch 168| loss: 113.81519| val_0_rmse: 12.80808|  0:11:45s
epoch 169| loss: 111.03081| val_0_rmse: 12.62798|  0:11:49s
epoch 170| loss: 114.71127| val_0_rmse: 12.2739 |  0:11:53s
epoch 171| loss: 110.34419| val_0_rmse: 12.45398|  0:11:57s
epoch 172| loss: 109.41095| val_0_rmse: 12.39202|  0:12:02s
epoch 173| loss: 106.18949| val_0_rmse: 12.64853|  0:12:06s
epoch 174| loss: 112.62399| val_0_rmse: 12.47792|  0:12:10s
epoch 175| loss: 105.69999| val_0_rmse: 12.50229|  0:12:14s
epoch 176| loss: 108.32883| val_0_rmse: 12.50507|  0:12:18s
epoch 177| loss: 111.02809| val_0_rmse: 12.16273|  0:12:22s
epoch 178| loss: 105.95034| val_0_rmse: 12.10219|  0:12:26s
epoch 179| loss: 104.62706| val_0_rmse: 12.18994|  0:12:31s
epoch 180| loss: 106.09571| val_0_rmse: 12.22117|  0:12:35s
epoch 181| loss: 104.84486| val_0_rmse: 12.21255|  0:12:39s
epoch 182| loss: 106.89897| val_0_rmse: 12.28043|  0:12:43s
epoch 183| loss: 105.85656| val_0_rmse: 12.33472|  0:12:47s
epoch 184| loss: 105.86343| val_0_rmse: 12.65811|  0:12:51s
epoch 185| loss: 104.52694| val_0_rmse: 12.38617|  0:12:55s
epoch 186| loss: 107.10543| val_0_rmse: 12.2359 |  0:13:00s
epoch 187| loss: 105.19067| val_0_rmse: 12.15532|  0:13:04s
epoch 188| loss: 103.04452| val_0_rmse: 12.16759|  0:13:08s
epoch 189| loss: 105.40359| val_0_rmse: 12.47186|  0:13:12s
epoch 190| loss: 100.26943| val_0_rmse: 12.67085|  0:13:16s
epoch 191| loss: 101.27498| val_0_rmse: 11.97585|  0:13:20s
epoch 192| loss: 103.59841| val_0_rmse: 12.21204|  0:13:25s
epoch 193| loss: 102.04772| val_0_rmse: 12.20222|  0:13:29s
epoch 194| loss: 104.47719| val_0_rmse: 12.15005|  0:13:33s
epoch 195| loss: 103.04846| val_0_rmse: 12.42123|  0:13:37s
epoch 196| loss: 102.24484| val_0_rmse: 12.32758|  0:13:41s
epoch 197| loss: 102.96061| val_0_rmse: 12.36221|  0:13:46s
epoch 198| loss: 102.4138| val_0_rmse: 12.17031|  0:13:50s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 199| loss: 100.8679| val_0_rmse: 11.83679|  0:13:54s
Stop training because you reached max_epochs = 200 with best_epoch = 199 and best_val_0_rmse = 11.83679
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-12 00:23:24,374] Trial 33 finished with value: 11.836789840644284 and parameters: {'lr': 0.0029922300741942265, 'n_steps': 9, 'gamma': 1.2417549722792531, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.04356625369418514, 'weight_decay': 0.00015081495436588592, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 9180.44484| val_0_rmse: 217.97923|  0:00:03s
epoch 1  | loss: 9097.09323| val_0_rmse: 101.40348|  0:00:07s
epoch 2  | loss: 9024.66048| val_0_rmse: 102.0502|  0:00:10s
epoch 3  | loss: 8974.88327| val_0_rmse: 94.83424|  0:00:14s
epoch 4  | loss: 8917.29142| val_0_rmse: 94.77079|  0:00:17s
epoch 5  | loss: 8852.31439| val_0_rmse: 94.73091|  0:00:21s
epoch 6  | loss: 8774.50909| val_0_rmse: 93.15356|  0:00:25s
epoch 7  | loss: 8576.28674| val_0_rmse: 92.68015|  0:00:28s
epoch 8  | loss: 7853.84306| val_0_rmse: 85.4838 |  0:00:32s
epoch 9  | loss: 6176.73715| val_0_rmse: 72.53226|  0:00:35s
epoch 10 | loss: 3950.81822| val_0_rmse: 55.47042|  0:00:39s
epoch 11 | loss: 2213.66499| val_0_rmse: 43.49328|  0:00:43s
epoch 12 | loss: 1404.99756| val_0_rmse: 36.08275|  0:00:46s
epoch 13 | loss: 1103.2337| val_0_rmse: 33.80871|  0:00:50s
epoch 14 | loss: 1018.47976| val_0_rmse: 31.86874|  0:00:54s
epoch 15 | loss: 954.14601| val_0_rmse: 30.63134|  0:00:57s
epoch 16 | loss: 915.9316| val_0_rmse: 30.10452|  0:01:01s
epoch 17 | loss: 890.97686| val_0_rmse: 30.12951|  0:01:04s
epoch 18 | loss: 863.65569| val_0_rmse: 30.58254|  0:01:08s
epoch 19 | loss: 846.22288| val_0_rmse: 30.07165|  0:01:11s
epoch 20 | loss: 826.35188| val_0_rmse: 28.75284|  0:01:15s
epoch 21 | loss: 812.64893| val_0_rmse: 28.99616|  0:01:19s
epoch 22 | loss: 802.09871| val_0_rmse: 28.34426|  0:01:22s
epoch 23 | loss: 791.52715| val_0_rmse: 28.64652|  0:01:26s
epoch 24 | loss: 783.71968| val_0_rmse: 28.07041|  0:01:30s
epoch 25 | loss: 776.97648| val_0_rmse: 28.27674|  0:01:33s
epoch 26 | loss: 774.57842| val_0_rmse: 27.78558|  0:01:37s
epoch 27 | loss: 771.82546| val_0_rmse: 27.77136|  0:01:40s
epoch 28 | loss: 763.22668| val_0_rmse: 27.79562|  0:01:44s
epoch 29 | loss: 749.96624| val_0_rmse: 27.7378 |  0:01:47s
epoch 30 | loss: 742.36979| val_0_rmse: 27.8184 |  0:01:51s
epoch 31 | loss: 733.61232| val_0_rmse: 27.06386|  0:01:55s
epoch 32 | loss: 719.27075| val_0_rmse: 26.87399|  0:01:59s
epoch 33 | loss: 702.2914| val_0_rmse: 26.59444|  0:02:02s
epoch 34 | loss: 692.16971| val_0_rmse: 26.40893|  0:02:06s
epoch 35 | loss: 685.41998| val_0_rmse: 26.45438|  0:02:09s
epoch 36 | loss: 673.41001| val_0_rmse: 26.0499 |  0:02:13s
epoch 37 | loss: 672.69754| val_0_rmse: 25.95154|  0:02:16s
epoch 38 | loss: 657.42795| val_0_rmse: 25.82625|  0:02:20s
epoch 39 | loss: 655.77957| val_0_rmse: 25.90044|  0:02:24s
epoch 40 | loss: 646.87606| val_0_rmse: 25.90616|  0:02:27s
epoch 41 | loss: 624.04694| val_0_rmse: 25.47523|  0:02:31s
epoch 42 | loss: 606.03776| val_0_rmse: 25.21333|  0:02:34s
epoch 43 | loss: 588.35917| val_0_rmse: 24.93513|  0:02:38s
epoch 44 | loss: 574.11749| val_0_rmse: 25.22156|  0:02:42s
epoch 45 | loss: 552.49715| val_0_rmse: 23.70447|  0:02:45s
epoch 46 | loss: 532.64738| val_0_rmse: 23.48214|  0:02:49s
epoch 47 | loss: 517.69862| val_0_rmse: 23.55245|  0:02:52s
epoch 48 | loss: 491.70688| val_0_rmse: 23.79938|  0:02:56s
epoch 49 | loss: 472.79553| val_0_rmse: 22.24858|  0:03:00s
epoch 50 | loss: 453.84892| val_0_rmse: 23.15348|  0:03:04s
epoch 51 | loss: 429.21468| val_0_rmse: 24.5988 |  0:03:07s
epoch 52 | loss: 420.79718| val_0_rmse: 24.12888|  0:03:11s
epoch 53 | loss: 407.52053| val_0_rmse: 21.51721|  0:03:14s
epoch 54 | loss: 390.03385| val_0_rmse: 22.12894|  0:03:18s
epoch 55 | loss: 378.64543| val_0_rmse: 21.77441|  0:03:21s
epoch 56 | loss: 373.41061| val_0_rmse: 22.24572|  0:03:25s
epoch 57 | loss: 367.02652| val_0_rmse: 23.23102|  0:03:29s
epoch 58 | loss: 344.00064| val_0_rmse: 22.29244|  0:03:32s
epoch 59 | loss: 343.19545| val_0_rmse: 21.20807|  0:03:36s
epoch 60 | loss: 337.85689| val_0_rmse: 21.67291|  0:03:39s
epoch 61 | loss: 329.25919| val_0_rmse: 21.86034|  0:03:43s
epoch 62 | loss: 319.47241| val_0_rmse: 20.0068 |  0:03:46s
epoch 63 | loss: 315.98305| val_0_rmse: 19.83383|  0:03:50s
epoch 64 | loss: 306.32848| val_0_rmse: 21.25709|  0:03:54s
epoch 65 | loss: 302.52264| val_0_rmse: 23.4704 |  0:03:58s
epoch 66 | loss: 295.52955| val_0_rmse: 20.17536|  0:04:01s
epoch 67 | loss: 294.05409| val_0_rmse: 19.46207|  0:04:05s
epoch 68 | loss: 280.99608| val_0_rmse: 20.25042|  0:04:08s
epoch 69 | loss: 278.22838| val_0_rmse: 18.51576|  0:04:12s
epoch 70 | loss: 270.47997| val_0_rmse: 17.90158|  0:04:15s
epoch 71 | loss: 258.77766| val_0_rmse: 17.94047|  0:04:19s
epoch 72 | loss: 261.54471| val_0_rmse: 16.48794|  0:04:23s
epoch 73 | loss: 256.14134| val_0_rmse: 16.52663|  0:04:26s
epoch 74 | loss: 253.4978| val_0_rmse: 16.5213 |  0:04:30s
epoch 75 | loss: 245.96474| val_0_rmse: 16.10392|  0:04:34s
epoch 76 | loss: 247.70477| val_0_rmse: 16.37364|  0:04:37s
epoch 77 | loss: 237.94877| val_0_rmse: 16.55533|  0:04:41s
epoch 78 | loss: 237.55494| val_0_rmse: 16.55737|  0:04:44s
epoch 79 | loss: 234.8693| val_0_rmse: 15.83976|  0:04:48s
epoch 80 | loss: 226.8863| val_0_rmse: 16.11525|  0:04:51s
epoch 81 | loss: 227.36723| val_0_rmse: 15.86626|  0:04:55s
epoch 82 | loss: 226.83211| val_0_rmse: 15.7529 |  0:04:59s
epoch 83 | loss: 224.07797| val_0_rmse: 15.49839|  0:05:02s
epoch 84 | loss: 218.83629| val_0_rmse: 15.26816|  0:05:06s
epoch 85 | loss: 219.65862| val_0_rmse: 15.44134|  0:05:10s
epoch 86 | loss: 207.55288| val_0_rmse: 15.15998|  0:05:13s
epoch 87 | loss: 210.8329| val_0_rmse: 14.90382|  0:05:17s
epoch 88 | loss: 206.69964| val_0_rmse: 14.75644|  0:05:20s
epoch 89 | loss: 205.91283| val_0_rmse: 15.39487|  0:05:24s
epoch 90 | loss: 200.35144| val_0_rmse: 14.86482|  0:05:28s
epoch 91 | loss: 200.85999| val_0_rmse: 14.69831|  0:05:31s
epoch 92 | loss: 201.23393| val_0_rmse: 15.13331|  0:05:35s
epoch 93 | loss: 196.74898| val_0_rmse: 14.73887|  0:05:38s
epoch 94 | loss: 194.8426| val_0_rmse: 14.98988|  0:05:42s
epoch 95 | loss: 195.48752| val_0_rmse: 14.59603|  0:05:45s
epoch 96 | loss: 196.30577| val_0_rmse: 14.6663 |  0:05:49s
epoch 97 | loss: 190.41474| val_0_rmse: 14.47552|  0:05:53s
epoch 98 | loss: 185.07677| val_0_rmse: 14.79008|  0:05:57s
epoch 99 | loss: 187.79363| val_0_rmse: 14.29924|  0:06:00s
epoch 100| loss: 186.4841| val_0_rmse: 14.23838|  0:06:04s
epoch 101| loss: 181.41634| val_0_rmse: 14.02335|  0:06:07s
epoch 102| loss: 185.93556| val_0_rmse: 14.42847|  0:06:11s
epoch 103| loss: 182.4197| val_0_rmse: 14.34156|  0:06:14s
epoch 104| loss: 175.87001| val_0_rmse: 13.99647|  0:06:18s
epoch 105| loss: 175.39645| val_0_rmse: 13.84037|  0:06:22s
epoch 106| loss: 178.3363| val_0_rmse: 14.22111|  0:06:26s
epoch 107| loss: 173.79473| val_0_rmse: 14.3435 |  0:06:29s
epoch 108| loss: 171.34758| val_0_rmse: 14.07988|  0:06:33s
epoch 109| loss: 170.89681| val_0_rmse: 13.79077|  0:06:36s
epoch 110| loss: 171.01834| val_0_rmse: 13.83245|  0:06:40s
epoch 111| loss: 174.00852| val_0_rmse: 14.0055 |  0:06:43s
epoch 112| loss: 166.43847| val_0_rmse: 13.83936|  0:06:47s
epoch 113| loss: 165.36052| val_0_rmse: 13.83925|  0:06:50s
epoch 114| loss: 170.08109| val_0_rmse: 13.72231|  0:06:54s
epoch 115| loss: 170.09037| val_0_rmse: 13.55266|  0:06:57s
epoch 116| loss: 167.21123| val_0_rmse: 13.85235|  0:07:01s
epoch 117| loss: 165.04528| val_0_rmse: 14.12972|  0:07:04s
epoch 118| loss: 164.05928| val_0_rmse: 13.83172|  0:07:08s
epoch 119| loss: 159.31177| val_0_rmse: 13.76686|  0:07:11s
epoch 120| loss: 162.72665| val_0_rmse: 13.65649|  0:07:15s
epoch 121| loss: 161.0032| val_0_rmse: 13.90949|  0:07:18s
epoch 122| loss: 158.02811| val_0_rmse: 13.96955|  0:07:22s
epoch 123| loss: 160.14749| val_0_rmse: 14.1349 |  0:07:26s
epoch 124| loss: 154.28071| val_0_rmse: 14.03961|  0:07:29s
epoch 125| loss: 154.01223| val_0_rmse: 13.50555|  0:07:33s
epoch 126| loss: 150.032 | val_0_rmse: 13.7095 |  0:07:36s
epoch 127| loss: 152.34868| val_0_rmse: 13.85633|  0:07:40s
epoch 128| loss: 151.44732| val_0_rmse: 13.36786|  0:07:43s
epoch 129| loss: 153.09362| val_0_rmse: 13.31904|  0:07:47s
epoch 130| loss: 153.03197| val_0_rmse: 13.63234|  0:07:50s
epoch 131| loss: 149.6664| val_0_rmse: 13.2975 |  0:07:54s
epoch 132| loss: 146.07436| val_0_rmse: 13.59164|  0:07:58s
epoch 133| loss: 143.35691| val_0_rmse: 13.67329|  0:08:01s
epoch 134| loss: 148.61886| val_0_rmse: 14.00819|  0:08:05s
epoch 135| loss: 145.26144| val_0_rmse: 13.58772|  0:08:08s
epoch 136| loss: 146.33266| val_0_rmse: 13.05232|  0:08:12s
epoch 137| loss: 142.34906| val_0_rmse: 14.71259|  0:08:15s
epoch 138| loss: 145.33689| val_0_rmse: 13.30222|  0:08:18s
epoch 139| loss: 145.63026| val_0_rmse: 13.56805|  0:08:22s
epoch 140| loss: 146.64266| val_0_rmse: 13.5196 |  0:08:26s
epoch 141| loss: 143.52415| val_0_rmse: 12.94639|  0:08:29s
epoch 142| loss: 144.16373| val_0_rmse: 13.68458|  0:08:33s
epoch 143| loss: 145.29777| val_0_rmse: 13.09705|  0:08:36s
epoch 144| loss: 139.88622| val_0_rmse: 13.68869|  0:08:40s
epoch 145| loss: 139.47425| val_0_rmse: 13.86435|  0:08:43s
epoch 146| loss: 136.57866| val_0_rmse: 13.0632 |  0:08:47s
epoch 147| loss: 139.51617| val_0_rmse: 13.14511|  0:08:50s
epoch 148| loss: 145.92319| val_0_rmse: 13.38496|  0:08:54s
epoch 149| loss: 133.50865| val_0_rmse: 13.48019|  0:08:58s
epoch 150| loss: 137.63806| val_0_rmse: 13.72031|  0:09:01s
epoch 151| loss: 136.51446| val_0_rmse: 13.37121|  0:09:05s
epoch 152| loss: 137.26319| val_0_rmse: 13.94904|  0:09:08s
epoch 153| loss: 132.42441| val_0_rmse: 13.77362|  0:09:12s
epoch 154| loss: 139.14799| val_0_rmse: 13.95269|  0:09:15s
epoch 155| loss: 133.10327| val_0_rmse: 16.79044|  0:09:19s
epoch 156| loss: 135.10434| val_0_rmse: 14.15585|  0:09:22s
epoch 157| loss: 134.09792| val_0_rmse: 13.33714|  0:09:26s
epoch 158| loss: 132.27561| val_0_rmse: 13.10535|  0:09:29s
epoch 159| loss: 134.07911| val_0_rmse: 12.9959 |  0:09:33s
epoch 160| loss: 127.91947| val_0_rmse: 14.19191|  0:09:36s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 161| loss: 131.37568| val_0_rmse: 13.1793 |  0:09:40s
Early stopping occurred at epoch 161 with best_epoch = 141 and best_val_0_rmse = 12.94639
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-12 00:33:12,947] Trial 34 finished with value: 12.946394675776519 and parameters: {'lr': 0.002233335085184568, 'n_steps': 7, 'gamma': 1.1528651828285752, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.029064801935374265, 'weight_decay': 0.0004769316477503371, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 3195.32405| val_0_rmse: 35.70976|  0:00:12s
epoch 1  | loss: 948.8987| val_0_rmse: 28.12473|  0:00:24s
epoch 2  | loss: 562.48289| val_0_rmse: 22.61637|  0:00:37s
epoch 3  | loss: 420.05286| val_0_rmse: 21.62173|  0:00:49s
epoch 4  | loss: 358.97745| val_0_rmse: 19.50341|  0:01:01s
epoch 5  | loss: 322.95504| val_0_rmse: 19.96154|  0:01:14s
epoch 6  | loss: 306.12845| val_0_rmse: 18.71311|  0:01:26s
epoch 7  | loss: 289.03261| val_0_rmse: 18.95025|  0:01:39s
epoch 8  | loss: 267.93283| val_0_rmse: 17.77389|  0:01:51s
epoch 9  | loss: 257.50525| val_0_rmse: 17.40013|  0:02:04s
epoch 10 | loss: 237.40073| val_0_rmse: 17.31508|  0:02:16s
epoch 11 | loss: 230.20915| val_0_rmse: 17.80598|  0:02:28s
epoch 12 | loss: 219.48071| val_0_rmse: 18.25576|  0:02:41s
epoch 13 | loss: 224.01116| val_0_rmse: 16.52104|  0:02:53s
epoch 14 | loss: 216.61641| val_0_rmse: 16.31393|  0:03:06s
epoch 15 | loss: 217.08848| val_0_rmse: 16.76448|  0:03:18s
epoch 16 | loss: 210.76475| val_0_rmse: 17.35107|  0:03:30s
epoch 17 | loss: 205.26967| val_0_rmse: 17.21107|  0:03:43s
epoch 18 | loss: 202.90505| val_0_rmse: 16.45192|  0:03:55s
epoch 19 | loss: 197.73209| val_0_rmse: 17.3747 |  0:04:08s
epoch 20 | loss: 203.07382| val_0_rmse: 17.11392|  0:04:20s
epoch 21 | loss: 188.96681| val_0_rmse: 16.54307|  0:04:33s
epoch 22 | loss: 191.35269| val_0_rmse: 18.10379|  0:04:45s
epoch 23 | loss: 197.59956| val_0_rmse: 16.41494|  0:04:57s
epoch 24 | loss: 190.80434| val_0_rmse: 16.64866|  0:05:10s
epoch 25 | loss: 184.23817| val_0_rmse: 17.6158 |  0:05:22s
epoch 26 | loss: 184.40283| val_0_rmse: 16.82076|  0:05:35s
epoch 27 | loss: 179.46535| val_0_rmse: 16.61095|  0:05:48s
epoch 28 | loss: 181.70018| val_0_rmse: 18.19854|  0:06:00s
epoch 29 | loss: 177.70517| val_0_rmse: 17.04864|  0:06:13s
epoch 30 | loss: 177.9112| val_0_rmse: 17.19355|  0:06:25s
epoch 31 | loss: 175.30986| val_0_rmse: 16.13369|  0:06:38s
epoch 32 | loss: 178.62665| val_0_rmse: 17.31996|  0:06:50s
epoch 33 | loss: 177.52528| val_0_rmse: 16.19091|  0:07:02s
epoch 34 | loss: 173.39909| val_0_rmse: 16.89126|  0:07:14s
epoch 35 | loss: 182.33216| val_0_rmse: 16.59205|  0:07:26s
epoch 36 | loss: 172.76067| val_0_rmse: 2413.19755|  0:07:39s
epoch 37 | loss: 166.77708| val_0_rmse: 226.65982|  0:07:51s
epoch 38 | loss: 172.14724| val_0_rmse: 22.14739|  0:08:04s
epoch 39 | loss: 168.33098| val_0_rmse: 167.42469|  0:08:16s
epoch 40 | loss: 168.22417| val_0_rmse: 194.7184|  0:08:28s
epoch 41 | loss: 167.94511| val_0_rmse: 165.29  |  0:08:41s
epoch 42 | loss: 169.62147| val_0_rmse: 981.30362|  0:08:53s
epoch 43 | loss: 160.31059| val_0_rmse: 18.28884|  0:09:06s
epoch 44 | loss: 168.74351| val_0_rmse: 17.12648|  0:09:18s
epoch 45 | loss: 164.93559| val_0_rmse: 18.27348|  0:09:30s
epoch 46 | loss: 163.18005| val_0_rmse: 18.02686|  0:09:43s
epoch 47 | loss: 160.84941| val_0_rmse: 18.64252|  0:09:55s
epoch 48 | loss: 161.64389| val_0_rmse: 16.89377|  0:10:08s
epoch 49 | loss: 164.5745| val_0_rmse: 16.58846|  0:10:20s
epoch 50 | loss: 164.10191| val_0_rmse: 16.16147|  0:10:32s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 51 | loss: 160.41414| val_0_rmse: 17.86717|  0:10:45s
Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_rmse = 16.13369
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-12 00:44:42,440] Trial 35 finished with value: 16.133690396781226 and parameters: {'lr': 0.008327901243350987, 'n_steps': 6, 'gamma': 1.496014410882327, 'n_independent': 1, 'n_shared': 3, 'momentum': 0.06102370581714043, 'weight_decay': 0.00011447913345015589, 'batch_size': 64, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8567.53798| val_0_rmse: 92.29977|  0:00:09s
epoch 1  | loss: 7979.16276| val_0_rmse: 83.94463|  0:00:18s
epoch 2  | loss: 5297.13651| val_0_rmse: 57.21516|  0:00:26s
epoch 3  | loss: 2049.16232| val_0_rmse: 38.14736|  0:00:36s
epoch 4  | loss: 1102.49545| val_0_rmse: 32.97894|  0:00:45s
epoch 5  | loss: 986.29103| val_0_rmse: 33.48128|  0:00:53s
epoch 6  | loss: 950.53692| val_0_rmse: 31.84413|  0:01:02s
epoch 7  | loss: 923.89567| val_0_rmse: 31.73761|  0:01:11s
epoch 8  | loss: 912.92144| val_0_rmse: 30.4356 |  0:01:20s
epoch 9  | loss: 900.33389| val_0_rmse: 30.78472|  0:01:29s
epoch 10 | loss: 889.67916| val_0_rmse: 31.71815|  0:01:38s
epoch 11 | loss: 882.43376| val_0_rmse: 29.53227|  0:01:47s
epoch 12 | loss: 865.37385| val_0_rmse: 34.85275|  0:01:56s
epoch 13 | loss: 860.9367| val_0_rmse: 34.35283|  0:02:05s
epoch 14 | loss: 844.62861| val_0_rmse: 30.80445|  0:02:14s
epoch 15 | loss: 834.24694| val_0_rmse: 29.95156|  0:02:23s
epoch 16 | loss: 825.29334| val_0_rmse: 29.45215|  0:02:32s
epoch 17 | loss: 816.16644| val_0_rmse: 28.16246|  0:02:41s
epoch 18 | loss: 807.70475| val_0_rmse: 27.77671|  0:02:50s
epoch 19 | loss: 790.75679| val_0_rmse: 27.74542|  0:02:59s
epoch 20 | loss: 779.33326| val_0_rmse: 27.76286|  0:03:08s
epoch 21 | loss: 775.91424| val_0_rmse: 27.79868|  0:03:17s
epoch 22 | loss: 773.37768| val_0_rmse: 27.88267|  0:03:26s
epoch 23 | loss: 766.3628| val_0_rmse: 27.51751|  0:03:35s
epoch 24 | loss: 760.28783| val_0_rmse: 27.73838|  0:03:44s
epoch 25 | loss: 756.92778| val_0_rmse: 28.68294|  0:03:53s
epoch 26 | loss: 751.02072| val_0_rmse: 27.31818|  0:04:01s
epoch 27 | loss: 751.4965| val_0_rmse: 27.21209|  0:04:11s
epoch 28 | loss: 743.255 | val_0_rmse: 27.44405|  0:04:19s
epoch 29 | loss: 736.23483| val_0_rmse: 26.95067|  0:04:28s
epoch 30 | loss: 727.10519| val_0_rmse: 26.398  |  0:04:38s
epoch 31 | loss: 716.09958| val_0_rmse: 26.96733|  0:04:46s
epoch 32 | loss: 706.40453| val_0_rmse: 28.32155|  0:04:55s
epoch 33 | loss: 684.82105| val_0_rmse: 30.76641|  0:05:04s
epoch 34 | loss: 668.07142| val_0_rmse: 26.88773|  0:05:13s
epoch 35 | loss: 642.51607| val_0_rmse: 28.41363|  0:05:22s
epoch 36 | loss: 614.331 | val_0_rmse: 32.49337|  0:05:31s
epoch 37 | loss: 578.5925| val_0_rmse: 39.4452 |  0:05:40s
epoch 38 | loss: 552.14285| val_0_rmse: 24.03248|  0:05:49s
epoch 39 | loss: 524.33997| val_0_rmse: 31.11807|  0:05:58s
epoch 40 | loss: 500.46497| val_0_rmse: 28.98163|  0:06:07s
epoch 41 | loss: 480.87634| val_0_rmse: 24.60942|  0:06:16s
epoch 42 | loss: 457.60162| val_0_rmse: 25.84033|  0:06:25s
epoch 43 | loss: 442.64584| val_0_rmse: 26.12008|  0:06:34s
epoch 44 | loss: 428.83702| val_0_rmse: 25.52753|  0:06:43s
epoch 45 | loss: 413.68752| val_0_rmse: 25.75803|  0:06:52s
epoch 46 | loss: 402.49127| val_0_rmse: 23.20476|  0:07:00s
epoch 47 | loss: 389.83365| val_0_rmse: 21.97672|  0:07:10s
epoch 48 | loss: 381.05314| val_0_rmse: 28.53238|  0:07:18s
epoch 49 | loss: 360.80439| val_0_rmse: 32.83625|  0:07:27s
epoch 50 | loss: 355.23619| val_0_rmse: 22.28619|  0:07:36s
epoch 51 | loss: 346.69554| val_0_rmse: 25.36557|  0:07:45s
epoch 52 | loss: 336.66223| val_0_rmse: 23.28113|  0:07:54s
epoch 53 | loss: 323.05156| val_0_rmse: 22.95837|  0:08:03s
epoch 54 | loss: 320.22779| val_0_rmse: 25.05918|  0:08:12s
epoch 55 | loss: 309.10866| val_0_rmse: 22.75515|  0:08:21s
epoch 56 | loss: 298.87233| val_0_rmse: 19.87749|  0:08:29s
epoch 57 | loss: 294.6805| val_0_rmse: 19.27064|  0:08:39s
epoch 58 | loss: 287.5636| val_0_rmse: 20.13788|  0:08:48s
epoch 59 | loss: 278.35604| val_0_rmse: 19.86825|  0:08:56s
epoch 60 | loss: 274.46115| val_0_rmse: 18.73712|  0:09:05s
epoch 61 | loss: 266.79471| val_0_rmse: 18.18338|  0:09:14s
epoch 62 | loss: 255.21779| val_0_rmse: 18.42116|  0:09:23s
epoch 63 | loss: 256.42027| val_0_rmse: 17.83246|  0:09:32s
epoch 64 | loss: 252.92577| val_0_rmse: 18.05392|  0:09:41s
epoch 65 | loss: 245.88941| val_0_rmse: 17.38066|  0:09:50s
epoch 66 | loss: 237.15584| val_0_rmse: 17.41153|  0:09:58s
epoch 67 | loss: 233.57048| val_0_rmse: 17.6581 |  0:10:08s
epoch 68 | loss: 231.47615| val_0_rmse: 16.92866|  0:10:16s
epoch 69 | loss: 224.8263| val_0_rmse: 16.56036|  0:10:25s
epoch 70 | loss: 222.9204| val_0_rmse: 17.14151|  0:10:34s
epoch 71 | loss: 219.19566| val_0_rmse: 17.24949|  0:10:43s
epoch 72 | loss: 217.41156| val_0_rmse: 16.76569|  0:10:52s
epoch 73 | loss: 213.95061| val_0_rmse: 16.79659|  0:11:01s
epoch 74 | loss: 208.24105| val_0_rmse: 16.64998|  0:11:10s
epoch 75 | loss: 209.74264| val_0_rmse: 16.29063|  0:11:19s
epoch 76 | loss: 208.47447| val_0_rmse: 16.56035|  0:11:27s
epoch 77 | loss: 201.8576| val_0_rmse: 16.23116|  0:11:37s
epoch 78 | loss: 194.50087| val_0_rmse: 15.92096|  0:11:46s
epoch 79 | loss: 195.92962| val_0_rmse: 16.25924|  0:11:54s
epoch 80 | loss: 196.96766| val_0_rmse: 16.37456|  0:12:03s
epoch 81 | loss: 187.6703| val_0_rmse: 16.13626|  0:12:12s
epoch 82 | loss: 195.40584| val_0_rmse: 15.93021|  0:12:21s
epoch 83 | loss: 186.62852| val_0_rmse: 16.28399|  0:12:30s
epoch 84 | loss: 187.07429| val_0_rmse: 15.60775|  0:12:39s
epoch 85 | loss: 181.64938| val_0_rmse: 15.82598|  0:12:48s
epoch 86 | loss: 181.97224| val_0_rmse: 15.79408|  0:12:57s
epoch 87 | loss: 176.66456| val_0_rmse: 16.06828|  0:13:06s
epoch 88 | loss: 173.03907| val_0_rmse: 16.03953|  0:13:15s
epoch 89 | loss: 175.20038| val_0_rmse: 16.3548 |  0:13:23s
epoch 90 | loss: 170.84573| val_0_rmse: 16.28089|  0:13:32s
epoch 91 | loss: 173.05467| val_0_rmse: 15.38684|  0:13:41s
epoch 92 | loss: 168.68721| val_0_rmse: 15.83866|  0:13:50s
epoch 93 | loss: 163.89338| val_0_rmse: 15.46832|  0:13:59s
epoch 94 | loss: 166.66026| val_0_rmse: 15.23888|  0:14:08s
epoch 95 | loss: 166.87575| val_0_rmse: 15.26562|  0:14:17s
epoch 96 | loss: 160.1981| val_0_rmse: 15.62532|  0:14:26s
epoch 97 | loss: 164.17758| val_0_rmse: 15.55474|  0:14:35s
epoch 98 | loss: 160.97993| val_0_rmse: 15.61208|  0:14:44s
epoch 99 | loss: 156.39967| val_0_rmse: 15.65431|  0:14:52s
epoch 100| loss: 154.54324| val_0_rmse: 15.18795|  0:15:01s
epoch 101| loss: 153.64863| val_0_rmse: 15.62265|  0:15:10s
epoch 102| loss: 151.04477| val_0_rmse: 15.53409|  0:15:19s
epoch 103| loss: 152.66982| val_0_rmse: 15.41854|  0:15:28s
epoch 104| loss: 152.19672| val_0_rmse: 15.02393|  0:15:37s
epoch 105| loss: 147.72311| val_0_rmse: 15.13578|  0:15:46s
epoch 106| loss: 145.87562| val_0_rmse: 15.62564|  0:15:55s
epoch 107| loss: 147.03248| val_0_rmse: 15.30455|  0:16:04s
epoch 108| loss: 149.06011| val_0_rmse: 15.48607|  0:16:13s
epoch 109| loss: 145.80653| val_0_rmse: 15.26419|  0:16:21s
epoch 110| loss: 145.12824| val_0_rmse: 15.1387 |  0:16:30s
epoch 111| loss: 142.93569| val_0_rmse: 15.25334|  0:16:40s
epoch 112| loss: 143.4652| val_0_rmse: 15.13108|  0:16:48s
epoch 113| loss: 141.52666| val_0_rmse: 15.816  |  0:16:57s
epoch 114| loss: 137.74559| val_0_rmse: 16.07516|  0:17:06s
epoch 115| loss: 146.37574| val_0_rmse: 16.5814 |  0:17:15s
epoch 116| loss: 135.23998| val_0_rmse: 16.3988 |  0:17:24s
epoch 117| loss: 136.50035| val_0_rmse: 17.06735|  0:17:33s
epoch 118| loss: 134.38621| val_0_rmse: 17.2953 |  0:17:42s
epoch 119| loss: 130.55132| val_0_rmse: 15.90583|  0:17:50s
epoch 120| loss: 135.81988| val_0_rmse: 15.44735|  0:17:59s
epoch 121| loss: 132.14989| val_0_rmse: 16.59123|  0:18:09s
epoch 122| loss: 133.40133| val_0_rmse: 16.95606|  0:18:17s
epoch 123| loss: 130.48084| val_0_rmse: 18.16608|  0:18:26s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 124| loss: 129.29387| val_0_rmse: 17.61337|  0:18:35s
Early stopping occurred at epoch 124 with best_epoch = 104 and best_val_0_rmse = 15.02393
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-12 01:03:45,969] Trial 36 finished with value: 15.023933826054124 and parameters: {'lr': 0.0017103250928624857, 'n_steps': 7, 'gamma': 1.0836025751235812, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.07611746517487668, 'weight_decay': 3.1431421589758e-05, 'batch_size': 128, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8738.95356| val_0_rmse: 94.37056|  0:00:04s
epoch 1  | loss: 8647.47923| val_0_rmse: 94.1713 |  0:00:08s
epoch 2  | loss: 8204.41335| val_0_rmse: 81.21183|  0:00:13s
epoch 3  | loss: 5162.93715| val_0_rmse: 54.98514|  0:00:17s
epoch 4  | loss: 1462.39365| val_0_rmse: 37.59502|  0:00:21s
epoch 5  | loss: 1087.86498| val_0_rmse: 35.18826|  0:00:26s
epoch 6  | loss: 1048.52597| val_0_rmse: 33.93308|  0:00:30s
epoch 7  | loss: 1021.63927| val_0_rmse: 35.52514|  0:00:35s
epoch 8  | loss: 1000.4311| val_0_rmse: 32.18371|  0:00:39s
epoch 9  | loss: 983.99242| val_0_rmse: 33.82408|  0:00:43s
epoch 10 | loss: 941.33568| val_0_rmse: 31.68031|  0:00:48s
epoch 11 | loss: 922.7704| val_0_rmse: 31.7195 |  0:00:52s
epoch 12 | loss: 906.25359| val_0_rmse: 30.93388|  0:00:56s
epoch 13 | loss: 869.70363| val_0_rmse: 30.63733|  0:01:01s
epoch 14 | loss: 795.32235| val_0_rmse: 28.6794 |  0:01:05s
epoch 15 | loss: 728.01572| val_0_rmse: 26.26411|  0:01:10s
epoch 16 | loss: 653.10367| val_0_rmse: 24.76984|  0:01:14s
epoch 17 | loss: 589.9254| val_0_rmse: 22.98089|  0:01:18s
epoch 18 | loss: 539.54867| val_0_rmse: 22.23591|  0:01:22s
epoch 19 | loss: 487.78066| val_0_rmse: 21.58816|  0:01:27s
epoch 20 | loss: 470.73399| val_0_rmse: 20.9204 |  0:01:31s
epoch 21 | loss: 448.54126| val_0_rmse: 20.63219|  0:01:36s
epoch 22 | loss: 437.22446| val_0_rmse: 20.93297|  0:01:40s
epoch 23 | loss: 419.28405| val_0_rmse: 20.03371|  0:01:45s
epoch 24 | loss: 412.79484| val_0_rmse: 19.40309|  0:01:49s
epoch 25 | loss: 391.10621| val_0_rmse: 18.84749|  0:01:53s
epoch 26 | loss: 376.16222| val_0_rmse: 18.5535 |  0:01:57s
epoch 27 | loss: 358.98115| val_0_rmse: 18.41949|  0:02:02s
epoch 28 | loss: 349.09711| val_0_rmse: 17.83975|  0:02:07s
epoch 29 | loss: 333.33171| val_0_rmse: 17.75399|  0:02:11s
epoch 30 | loss: 326.9311| val_0_rmse: 17.02925|  0:02:15s
epoch 31 | loss: 314.03504| val_0_rmse: 16.97423|  0:02:20s
epoch 32 | loss: 307.80327| val_0_rmse: 16.50401|  0:02:24s
epoch 33 | loss: 292.43536| val_0_rmse: 16.10197|  0:02:28s
epoch 34 | loss: 290.01844| val_0_rmse: 16.25178|  0:02:33s
epoch 35 | loss: 281.74937| val_0_rmse: 15.94862|  0:02:37s
epoch 36 | loss: 272.3902| val_0_rmse: 15.90341|  0:02:42s
epoch 37 | loss: 266.35423| val_0_rmse: 15.518  |  0:02:46s
epoch 38 | loss: 257.01256| val_0_rmse: 15.46358|  0:02:50s
epoch 39 | loss: 253.09135| val_0_rmse: 15.37662|  0:02:54s
epoch 40 | loss: 249.5678| val_0_rmse: 15.18714|  0:02:59s
epoch 41 | loss: 249.22196| val_0_rmse: 15.20563|  0:03:04s
epoch 42 | loss: 241.04619| val_0_rmse: 14.74106|  0:03:08s
epoch 43 | loss: 235.9867| val_0_rmse: 14.98024|  0:03:12s
epoch 44 | loss: 227.75047| val_0_rmse: 14.75994|  0:03:17s
epoch 45 | loss: 227.16552| val_0_rmse: 14.96739|  0:03:21s
epoch 46 | loss: 222.90345| val_0_rmse: 14.81016|  0:03:25s
epoch 47 | loss: 213.22489| val_0_rmse: 14.44647|  0:03:29s
epoch 48 | loss: 211.6242| val_0_rmse: 14.59856|  0:03:34s
epoch 49 | loss: 210.03139| val_0_rmse: 14.55174|  0:03:38s
epoch 50 | loss: 206.96294| val_0_rmse: 14.29877|  0:03:43s
epoch 51 | loss: 204.2141| val_0_rmse: 14.13553|  0:03:47s
epoch 52 | loss: 202.9756| val_0_rmse: 14.10453|  0:03:51s
epoch 53 | loss: 196.0673| val_0_rmse: 14.09149|  0:03:56s
epoch 54 | loss: 192.1837| val_0_rmse: 14.02989|  0:04:00s
epoch 55 | loss: 191.33288| val_0_rmse: 14.16474|  0:04:05s
epoch 56 | loss: 190.79729| val_0_rmse: 14.68827|  0:04:09s
epoch 57 | loss: 191.09412| val_0_rmse: 13.8861 |  0:04:13s
epoch 58 | loss: 187.18259| val_0_rmse: 13.71063|  0:04:18s
epoch 59 | loss: 180.29412| val_0_rmse: 13.78313|  0:04:22s
epoch 60 | loss: 181.43502| val_0_rmse: 14.01614|  0:04:26s
epoch 61 | loss: 182.81013| val_0_rmse: 13.57603|  0:04:31s
epoch 62 | loss: 180.11817| val_0_rmse: 13.42828|  0:04:35s
epoch 63 | loss: 182.694 | val_0_rmse: 13.92189|  0:04:40s
epoch 64 | loss: 180.65593| val_0_rmse: 13.64992|  0:04:44s
epoch 65 | loss: 179.26093| val_0_rmse: 13.51957|  0:04:48s
epoch 66 | loss: 171.98709| val_0_rmse: 13.34631|  0:04:53s
epoch 67 | loss: 176.23163| val_0_rmse: 13.73368|  0:04:57s
epoch 68 | loss: 176.13175| val_0_rmse: 13.14691|  0:05:01s
epoch 69 | loss: 168.37457| val_0_rmse: 13.07459|  0:05:06s
epoch 70 | loss: 167.52089| val_0_rmse: 13.39692|  0:05:10s
epoch 71 | loss: 167.46844| val_0_rmse: 13.37004|  0:05:15s
epoch 72 | loss: 160.39115| val_0_rmse: 13.2458 |  0:05:19s
epoch 73 | loss: 160.73655| val_0_rmse: 13.31682|  0:05:23s
epoch 74 | loss: 159.60858| val_0_rmse: 13.08299|  0:05:27s
epoch 75 | loss: 165.77258| val_0_rmse: 13.6852 |  0:05:32s
epoch 76 | loss: 161.87404| val_0_rmse: 13.08323|  0:05:37s
epoch 77 | loss: 156.13966| val_0_rmse: 13.11671|  0:05:41s
epoch 78 | loss: 158.26428| val_0_rmse: 13.14496|  0:05:45s
epoch 79 | loss: 155.07131| val_0_rmse: 12.8916 |  0:05:49s
epoch 80 | loss: 152.23937| val_0_rmse: 12.97371|  0:05:54s
epoch 81 | loss: 149.57673| val_0_rmse: 13.05647|  0:05:58s
epoch 82 | loss: 155.08319| val_0_rmse: 13.08556|  0:06:03s
epoch 83 | loss: 153.1911| val_0_rmse: 13.17831|  0:06:07s
epoch 84 | loss: 153.1344| val_0_rmse: 12.75325|  0:06:11s
epoch 85 | loss: 150.03567| val_0_rmse: 13.09405|  0:06:16s
epoch 86 | loss: 156.94294| val_0_rmse: 12.78035|  0:06:20s
epoch 87 | loss: 150.06936| val_0_rmse: 12.75683|  0:06:24s
epoch 88 | loss: 148.35741| val_0_rmse: 13.09373|  0:06:29s
epoch 89 | loss: 146.06959| val_0_rmse: 12.89478|  0:06:33s
epoch 90 | loss: 144.69145| val_0_rmse: 13.25127|  0:06:38s
epoch 91 | loss: 147.39823| val_0_rmse: 12.73209|  0:06:42s
epoch 92 | loss: 146.41774| val_0_rmse: 13.02879|  0:06:46s
epoch 93 | loss: 146.20728| val_0_rmse: 12.92434|  0:06:51s
epoch 94 | loss: 142.39  | val_0_rmse: 12.84432|  0:06:55s
epoch 95 | loss: 141.19545| val_0_rmse: 13.02124|  0:06:59s
epoch 96 | loss: 140.26192| val_0_rmse: 13.1326 |  0:07:04s
epoch 97 | loss: 147.9757| val_0_rmse: 13.25093|  0:07:08s
epoch 98 | loss: 141.59026| val_0_rmse: 12.78278|  0:07:12s
epoch 99 | loss: 143.44156| val_0_rmse: 13.32382|  0:07:17s
epoch 100| loss: 141.55666| val_0_rmse: 13.1076 |  0:07:21s
epoch 101| loss: 136.49381| val_0_rmse: 12.67431|  0:07:25s
epoch 102| loss: 141.14265| val_0_rmse: 12.76615|  0:07:30s
epoch 103| loss: 137.98233| val_0_rmse: 12.86201|  0:07:34s
epoch 104| loss: 137.66438| val_0_rmse: 12.78489|  0:07:39s
epoch 105| loss: 138.07013| val_0_rmse: 12.93411|  0:07:43s
epoch 106| loss: 140.31113| val_0_rmse: 13.00108|  0:07:47s
epoch 107| loss: 137.69386| val_0_rmse: 12.89546|  0:07:52s
epoch 108| loss: 139.30309| val_0_rmse: 13.02512|  0:07:56s
epoch 109| loss: 140.28533| val_0_rmse: 12.32617|  0:08:00s
epoch 110| loss: 138.72801| val_0_rmse: 12.31754|  0:08:05s
epoch 111| loss: 140.56126| val_0_rmse: 13.23216|  0:08:09s
epoch 112| loss: 138.82476| val_0_rmse: 12.44325|  0:08:14s
epoch 113| loss: 135.5237| val_0_rmse: 12.60972|  0:08:18s
epoch 114| loss: 134.76601| val_0_rmse: 12.63575|  0:08:22s
epoch 115| loss: 134.28232| val_0_rmse: 12.62234|  0:08:26s
epoch 116| loss: 138.42818| val_0_rmse: 12.32526|  0:08:31s
epoch 117| loss: 134.64728| val_0_rmse: 12.50904|  0:08:35s
epoch 118| loss: 132.43614| val_0_rmse: 12.43361|  0:08:40s
epoch 119| loss: 132.92996| val_0_rmse: 12.61786|  0:08:44s
epoch 120| loss: 135.75085| val_0_rmse: 12.77606|  0:08:48s
epoch 121| loss: 138.56125| val_0_rmse: 12.47537|  0:08:52s
epoch 122| loss: 131.20826| val_0_rmse: 12.49794|  0:08:57s
epoch 123| loss: 132.66361| val_0_rmse: 12.54601|  0:09:01s
epoch 124| loss: 132.73635| val_0_rmse: 12.66816|  0:09:05s
epoch 125| loss: 131.72388| val_0_rmse: 12.66287|  0:09:10s
epoch 126| loss: 132.74303| val_0_rmse: 12.47954|  0:09:14s
epoch 127| loss: 131.88664| val_0_rmse: 12.41523|  0:09:18s
epoch 128| loss: 131.00599| val_0_rmse: 12.85331|  0:09:22s
epoch 129| loss: 132.72504| val_0_rmse: 12.35707|  0:09:26s
epoch 130| loss: 130.68745| val_0_rmse: 11.8699 |  0:09:31s
epoch 131| loss: 127.42385| val_0_rmse: 12.31166|  0:09:35s
epoch 132| loss: 133.12874| val_0_rmse: 13.22784|  0:09:40s
epoch 133| loss: 134.77912| val_0_rmse: 12.01542|  0:09:44s
epoch 134| loss: 130.80001| val_0_rmse: 12.2043 |  0:09:48s
epoch 135| loss: 128.99929| val_0_rmse: 12.44149|  0:09:52s
epoch 136| loss: 132.57147| val_0_rmse: 12.20815|  0:09:57s
epoch 137| loss: 134.59777| val_0_rmse: 12.14119|  0:10:01s
epoch 138| loss: 127.48229| val_0_rmse: 12.40353|  0:10:06s
epoch 139| loss: 131.26718| val_0_rmse: 12.58244|  0:10:10s
epoch 140| loss: 132.1692| val_0_rmse: 12.65165|  0:10:14s
epoch 141| loss: 127.62407| val_0_rmse: 12.38501|  0:10:18s
epoch 142| loss: 129.99567| val_0_rmse: 12.2359 |  0:10:22s
epoch 143| loss: 124.71834| val_0_rmse: 12.30223|  0:10:27s
epoch 144| loss: 130.77444| val_0_rmse: 12.65337|  0:10:31s
epoch 145| loss: 131.12819| val_0_rmse: 12.23358|  0:10:36s
epoch 146| loss: 126.13668| val_0_rmse: 12.55023|  0:10:40s
epoch 147| loss: 128.88714| val_0_rmse: 12.16912|  0:10:44s
epoch 148| loss: 127.06715| val_0_rmse: 12.48322|  0:10:48s
epoch 149| loss: 125.20026| val_0_rmse: 12.42303|  0:10:53s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 150| loss: 124.55421| val_0_rmse: 12.54914|  0:10:57s
Early stopping occurred at epoch 150 with best_epoch = 130 and best_val_0_rmse = 11.8699
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-12 01:14:51,502] Trial 37 finished with value: 11.869901689965658 and parameters: {'lr': 0.005669057653366005, 'n_steps': 6, 'gamma': 1.4047205464341015, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.02685837881872884, 'weight_decay': 0.0003011625652697746, 'batch_size': 512, 'virtual_batch_size': 32}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8793.76791| val_0_rmse: 94.29849|  0:00:14s
epoch 1  | loss: 8497.16047| val_0_rmse: 90.86673|  0:00:29s
epoch 2  | loss: 5071.5381| val_0_rmse: 47.58087|  0:00:43s
epoch 3  | loss: 1439.27492| val_0_rmse: 34.70952|  0:00:58s
epoch 4  | loss: 1082.00401| val_0_rmse: 33.20948|  0:01:12s
epoch 5  | loss: 1010.27392| val_0_rmse: 32.73392|  0:01:27s
epoch 6  | loss: 977.04088| val_0_rmse: 31.61134|  0:01:41s
epoch 7  | loss: 952.61552| val_0_rmse: 32.27837|  0:01:56s
epoch 8  | loss: 928.61272| val_0_rmse: 31.07276|  0:02:11s
epoch 9  | loss: 901.0989| val_0_rmse: 29.97761|  0:02:25s
epoch 10 | loss: 883.26854| val_0_rmse: 29.46429|  0:02:40s
epoch 11 | loss: 867.76362| val_0_rmse: 30.67563|  0:02:55s
epoch 12 | loss: 837.02083| val_0_rmse: 29.03736|  0:03:09s
epoch 13 | loss: 813.71924| val_0_rmse: 30.01831|  0:03:24s
epoch 14 | loss: 800.36573| val_0_rmse: 29.14904|  0:03:39s
epoch 15 | loss: 786.55229| val_0_rmse: 30.15955|  0:03:53s
epoch 16 | loss: 780.42511| val_0_rmse: 29.48423|  0:04:08s
epoch 17 | loss: 767.84559| val_0_rmse: 29.3133 |  0:04:22s
epoch 18 | loss: 749.06977| val_0_rmse: 31.98251|  0:04:37s
epoch 19 | loss: 734.01501| val_0_rmse: 29.70318|  0:04:51s
epoch 20 | loss: 678.74018| val_0_rmse: 26.07804|  0:05:06s
epoch 21 | loss: 627.35302| val_0_rmse: 26.18022|  0:05:20s
epoch 22 | loss: 566.44897| val_0_rmse: 27.08253|  0:05:35s
epoch 23 | loss: 520.2163| val_0_rmse: 24.43926|  0:05:49s
epoch 24 | loss: 490.38941| val_0_rmse: 24.30734|  0:06:04s
epoch 25 | loss: 452.45336| val_0_rmse: 23.11573|  0:06:19s
epoch 26 | loss: 431.98348| val_0_rmse: 22.80225|  0:06:34s
epoch 27 | loss: 403.16222| val_0_rmse: 22.11387|  0:06:48s
epoch 28 | loss: 380.90633| val_0_rmse: 22.57533|  0:07:03s
epoch 29 | loss: 368.42049| val_0_rmse: 21.83904|  0:07:17s
epoch 30 | loss: 349.04536| val_0_rmse: 20.72327|  0:07:32s
epoch 31 | loss: 336.72252| val_0_rmse: 20.28743|  0:07:46s
epoch 32 | loss: 321.45694| val_0_rmse: 19.72182|  0:08:01s
epoch 33 | loss: 316.7918| val_0_rmse: 20.06469|  0:08:16s
epoch 34 | loss: 302.44054| val_0_rmse: 19.75117|  0:08:31s
epoch 35 | loss: 289.98981| val_0_rmse: 19.38932|  0:08:45s
epoch 36 | loss: 282.26581| val_0_rmse: 19.3724 |  0:09:00s
epoch 37 | loss: 272.82318| val_0_rmse: 19.7827 |  0:09:14s
epoch 38 | loss: 265.13037| val_0_rmse: 19.39813|  0:09:29s
epoch 39 | loss: 255.779 | val_0_rmse: 19.34606|  0:09:43s
epoch 40 | loss: 252.30554| val_0_rmse: 18.93992|  0:09:58s
epoch 41 | loss: 241.82431| val_0_rmse: 19.29161|  0:10:13s
epoch 42 | loss: 239.97827| val_0_rmse: 19.10676|  0:10:27s
epoch 43 | loss: 235.43772| val_0_rmse: 19.01095|  0:10:42s
epoch 44 | loss: 232.9412| val_0_rmse: 19.48209|  0:10:56s
epoch 45 | loss: 225.07395| val_0_rmse: 19.54713|  0:11:11s
epoch 46 | loss: 224.38301| val_0_rmse: 18.20488|  0:11:25s
epoch 47 | loss: 222.74858| val_0_rmse: 18.01445|  0:11:40s
epoch 48 | loss: 217.49556| val_0_rmse: 20.01473|  0:11:54s
epoch 49 | loss: 212.95776| val_0_rmse: 18.63319|  0:12:09s
epoch 50 | loss: 210.26991| val_0_rmse: 18.8324 |  0:12:23s
epoch 51 | loss: 205.79253| val_0_rmse: 18.76825|  0:12:38s
epoch 52 | loss: 204.12258| val_0_rmse: 17.82327|  0:12:52s
epoch 53 | loss: 200.99043| val_0_rmse: 18.91183|  0:13:07s
epoch 54 | loss: 195.0861| val_0_rmse: 18.40613|  0:13:21s
epoch 55 | loss: 195.86177| val_0_rmse: 18.88756|  0:13:36s
epoch 56 | loss: 194.03  | val_0_rmse: 18.77697|  0:13:51s
epoch 57 | loss: 187.02915| val_0_rmse: 18.99487|  0:14:06s
epoch 58 | loss: 192.07586| val_0_rmse: 17.83921|  0:14:20s
epoch 59 | loss: 184.31099| val_0_rmse: 19.37464|  0:14:35s
epoch 60 | loss: 181.27573| val_0_rmse: 19.32202|  0:14:49s
epoch 61 | loss: 178.89069| val_0_rmse: 19.74536|  0:15:04s
epoch 62 | loss: 179.62853| val_0_rmse: 18.01153|  0:15:18s
epoch 63 | loss: 178.63297| val_0_rmse: 18.30219|  0:15:33s
epoch 64 | loss: 178.03033| val_0_rmse: 18.45998|  0:15:47s
epoch 65 | loss: 178.32519| val_0_rmse: 19.8235 |  0:16:02s
epoch 66 | loss: 175.31752| val_0_rmse: 19.1015 |  0:16:17s
epoch 67 | loss: 174.18424| val_0_rmse: 18.45212|  0:16:32s
epoch 68 | loss: 175.07311| val_0_rmse: 18.53732|  0:16:46s
epoch 69 | loss: 171.65531| val_0_rmse: 18.19008|  0:17:01s
epoch 70 | loss: 166.43889| val_0_rmse: 18.16762|  0:17:15s
epoch 71 | loss: 163.99608| val_0_rmse: 18.73161|  0:17:30s
epoch 72 | loss: 163.4101| val_0_rmse: 19.23052|  0:17:44s
Early stopping occurred at epoch 72 with best_epoch = 52 and best_val_0_rmse = 17.82327
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-12 01:33:41,991] Trial 38 finished with value: 17.82326515959428 and parameters: {'lr': 0.0013276358392031442, 'n_steps': 9, 'gamma': 1.2988397993576952, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.05287732186913685, 'weight_decay': 8.873854112953019e-05, 'batch_size': 64, 'virtual_batch_size': 64}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  lr = trial.suggest_loguniform('lr', 0.001, 0.01)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  gamma = trial.suggest_uniform('gamma', 1.0, 1.5)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:50: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  momentum = trial.suggest_uniform('momentum', 0.01, 0.1)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/train_tabnet.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform('weight_decay', 0.00001, 0.001)  # Reduced range
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
epoch 0  | loss: 8799.59311| val_0_rmse: 90.93893|  0:00:05s
epoch 1  | loss: 8838.73538| val_0_rmse: 94.97191|  0:00:11s
epoch 2  | loss: 8853.25262| val_0_rmse: 116.17398|  0:00:16s
epoch 3  | loss: 8841.29961| val_0_rmse: 92.92067|  0:00:21s
epoch 4  | loss: 8799.55403| val_0_rmse: 143.97333|  0:00:26s
epoch 5  | loss: 8711.15883| val_0_rmse: 100.14964|  0:00:32s
epoch 6  | loss: 8541.17587| val_0_rmse: 91.29454|  0:00:37s
epoch 7  | loss: 7490.80861| val_0_rmse: 79.25546|  0:00:43s
epoch 8  | loss: 4884.51154| val_0_rmse: 55.71219|  0:00:48s
epoch 9  | loss: 1992.82791| val_0_rmse: 36.52408|  0:00:53s
epoch 10 | loss: 1127.84074| val_0_rmse: 33.68119|  0:00:58s
epoch 11 | loss: 1008.81468| val_0_rmse: 32.04514|  0:01:04s
epoch 12 | loss: 926.24265| val_0_rmse: 30.61375|  0:01:09s
epoch 13 | loss: 876.4707| val_0_rmse: 29.71853|  0:01:15s
epoch 14 | loss: 849.80711| val_0_rmse: 29.60291|  0:01:20s
epoch 15 | loss: 834.89648| val_0_rmse: 29.14063|  0:01:25s
epoch 16 | loss: 824.15011| val_0_rmse: 28.84667|  0:01:30s
epoch 17 | loss: 805.52321| val_0_rmse: 28.49428|  0:01:36s
epoch 18 | loss: 780.50392| val_0_rmse: 28.35293|  0:01:41s
epoch 19 | loss: 763.69098| val_0_rmse: 27.89364|  0:01:46s
epoch 20 | loss: 762.30675| val_0_rmse: 28.19877|  0:01:51s
epoch 21 | loss: 745.86599| val_0_rmse: 28.48473|  0:01:57s
epoch 22 | loss: 738.00245| val_0_rmse: 27.51796|  0:02:02s
epoch 23 | loss: 737.88752| val_0_rmse: 27.55949|  0:02:07s
epoch 24 | loss: 732.99201| val_0_rmse: 27.70211|  0:02:13s
epoch 25 | loss: 723.99107| val_0_rmse: 27.90205|  0:02:18s
epoch 26 | loss: 722.93201| val_0_rmse: 27.45891|  0:02:23s
epoch 27 | loss: 701.39839| val_0_rmse: 27.4859 |  0:02:28s
epoch 28 | loss: 700.78073| val_0_rmse: 27.23028|  0:02:33s
epoch 29 | loss: 691.78283| val_0_rmse: 26.90225|  0:02:39s
epoch 30 | loss: 689.47049| val_0_rmse: 26.81032|  0:02:44s
epoch 31 | loss: 680.79551| val_0_rmse: 26.65213|  0:02:50s
epoch 32 | loss: 677.00228| val_0_rmse: 26.37548|  0:02:55s
epoch 33 | loss: 671.96745| val_0_rmse: 26.3556 |  0:03:00s
epoch 34 | loss: 663.68341| val_0_rmse: 26.44508|  0:03:05s
epoch 35 | loss: 658.92998| val_0_rmse: 25.85746|  0:03:11s
epoch 36 | loss: 650.55359| val_0_rmse: 26.23398|  0:03:16s
epoch 37 | loss: 653.34563| val_0_rmse: 26.84359|  0:03:21s
epoch 38 | loss: 647.17783| val_0_rmse: 26.17048|  0:03:26s
epoch 39 | loss: 636.35934| val_0_rmse: 26.42048|  0:03:31s
epoch 40 | loss: 633.55288| val_0_rmse: 25.71749|  0:03:37s
epoch 41 | loss: 623.0082| val_0_rmse: 25.95936|  0:03:42s
epoch 42 | loss: 609.09437| val_0_rmse: 25.83089|  0:03:48s
epoch 43 | loss: 588.56061| val_0_rmse: 28.0076 |  0:03:53s
epoch 44 | loss: 571.50557| val_0_rmse: 24.84384|  0:03:58s
epoch 45 | loss: 549.14689| val_0_rmse: 25.58873|  0:04:03s
epoch 46 | loss: 537.90104| val_0_rmse: 26.68135|  0:04:09s
epoch 47 | loss: 519.61277| val_0_rmse: 23.9058 |  0:04:14s
epoch 48 | loss: 505.71472| val_0_rmse: 24.65074|  0:04:19s
epoch 49 | loss: 491.79963| val_0_rmse: 22.94684|  0:04:24s
epoch 50 | loss: 482.29496| val_0_rmse: 23.02628|  0:04:30s
epoch 51 | loss: 465.30205| val_0_rmse: 22.68707|  0:04:35s
epoch 52 | loss: 460.57406| val_0_rmse: 23.89308|  0:04:41s
epoch 53 | loss: 455.02474| val_0_rmse: 22.80312|  0:04:46s
epoch 54 | loss: 442.63758| val_0_rmse: 22.36144|  0:04:51s
epoch 55 | loss: 436.6413| val_0_rmse: 20.8563 |  0:04:56s
epoch 56 | loss: 427.2302| val_0_rmse: 22.55387|  0:05:01s
epoch 57 | loss: 424.98408| val_0_rmse: 21.09761|  0:05:07s
epoch 58 | loss: 409.15537| val_0_rmse: 21.2849 |  0:05:12s
epoch 59 | loss: 404.9195| val_0_rmse: 20.98926|  0:05:17s
epoch 60 | loss: 395.93462| val_0_rmse: 20.29667|  0:05:23s
epoch 61 | loss: 393.33001| val_0_rmse: 20.87201|  0:05:28s
epoch 62 | loss: 382.58607| val_0_rmse: 20.47294|  0:05:33s
epoch 63 | loss: 384.09811| val_0_rmse: 20.02902|  0:05:39s
epoch 64 | loss: 379.98638| val_0_rmse: 21.47262|  0:05:44s
epoch 65 | loss: 367.2594| val_0_rmse: 19.691  |  0:05:49s
epoch 66 | loss: 365.30341| val_0_rmse: 21.33379|  0:05:54s
epoch 67 | loss: 367.14732| val_0_rmse: 21.15543|  0:05:59s
epoch 68 | loss: 357.71216| val_0_rmse: 19.75225|  0:06:05s
epoch 69 | loss: 354.99079| val_0_rmse: 21.51127|  0:06:10s
epoch 70 | loss: 352.55785| val_0_rmse: 19.41197|  0:06:16s
epoch 71 | loss: 345.37486| val_0_rmse: 19.20976|  0:06:21s
epoch 72 | loss: 339.89296| val_0_rmse: 19.1815 |  0:06:26s
epoch 73 | loss: 339.86327| val_0_rmse: 18.55861|  0:06:31s
epoch 74 | loss: 333.26828| val_0_rmse: 20.04564|  0:06:37s
epoch 75 | loss: 330.73079| val_0_rmse: 20.18348|  0:06:42s
epoch 76 | loss: 330.13089| val_0_rmse: 18.02305|  0:06:47s
epoch 77 | loss: 318.53886| val_0_rmse: 21.49551|  0:06:52s
epoch 78 | loss: 316.55748| val_0_rmse: 19.24075|  0:06:58s
epoch 79 | loss: 312.66033| val_0_rmse: 20.93825|  0:07:03s
epoch 80 | loss: 308.3243| val_0_rmse: 19.17935|  0:07:08s
epoch 81 | loss: 302.65942| val_0_rmse: 18.48933|  0:07:14s
epoch 82 | loss: 298.21763| val_0_rmse: 19.25981|  0:07:19s
epoch 83 | loss: 293.64233| val_0_rmse: 22.35855|  0:07:24s
epoch 84 | loss: 293.79179| val_0_rmse: 19.36115|  0:07:29s
epoch 85 | loss: 290.27167| val_0_rmse: 19.06854|  0:07:34s
epoch 86 | loss: 285.80512| val_0_rmse: 17.8923 |  0:07:40s
epoch 87 | loss: 283.46966| val_0_rmse: 18.03543|  0:07:45s
epoch 88 | loss: 280.08208| val_0_rmse: 17.80848|  0:07:50s
epoch 89 | loss: 278.19955| val_0_rmse: 16.23282|  0:07:55s
epoch 90 | loss: 272.5432| val_0_rmse: 16.27617|  0:08:01s
epoch 91 | loss: 263.98323| val_0_rmse: 15.79622|  0:08:06s
epoch 92 | loss: 268.02377| val_0_rmse: 16.50881|  0:08:12s
epoch 93 | loss: 262.19717| val_0_rmse: 16.50174|  0:08:17s
epoch 94 | loss: 255.74919| val_0_rmse: 16.82348|  0:08:22s
epoch 95 | loss: 264.34761| val_0_rmse: 16.47235|  0:08:27s
epoch 96 | loss: 253.28228| val_0_rmse: 15.76824|  0:08:32s
epoch 97 | loss: 249.33779| val_0_rmse: 16.55846|  0:08:38s
epoch 98 | loss: 244.37806| val_0_rmse: 15.95775|  0:08:43s
epoch 99 | loss: 247.6685| val_0_rmse: 15.89254|  0:08:48s
epoch 100| loss: 237.8835| val_0_rmse: 15.57281|  0:08:53s
epoch 101| loss: 237.38631| val_0_rmse: 15.93515|  0:08:58s
epoch 102| loss: 237.61441| val_0_rmse: 15.72231|  0:09:04s
epoch 103| loss: 228.99884| val_0_rmse: 15.38608|  0:09:10s
epoch 104| loss: 235.08332| val_0_rmse: 15.48213|  0:09:15s
epoch 105| loss: 226.39467| val_0_rmse: 14.78511|  0:09:20s
epoch 106| loss: 227.9612| val_0_rmse: 14.5923 |  0:09:25s
epoch 107| loss: 222.34248| val_0_rmse: 14.5284 |  0:09:30s
epoch 108| loss: 220.82618| val_0_rmse: 14.51106|  0:09:36s
epoch 109| loss: 222.60232| val_0_rmse: 14.39125|  0:09:41s
epoch 110| loss: 214.82394| val_0_rmse: 14.27307|  0:09:46s
epoch 111| loss: 214.47209| val_0_rmse: 14.15019|  0:09:52s
epoch 112| loss: 210.53654| val_0_rmse: 14.47688|  0:09:57s
epoch 113| loss: 205.62496| val_0_rmse: 14.43184|  0:10:02s
epoch 114| loss: 204.08817| val_0_rmse: 14.03741|  0:10:08s
epoch 115| loss: 207.85153| val_0_rmse: 14.07739|  0:10:13s
epoch 116| loss: 199.36312| val_0_rmse: 14.36819|  0:10:18s
epoch 117| loss: 200.3355| val_0_rmse: 14.162  |  0:10:23s
epoch 118| loss: 197.38007| val_0_rmse: 13.97129|  0:10:28s
epoch 119| loss: 194.93731| val_0_rmse: 13.93586|  0:10:34s
epoch 120| loss: 197.54941| val_0_rmse: 13.88182|  0:10:39s
epoch 121| loss: 192.99257| val_0_rmse: 13.89066|  0:10:44s
epoch 122| loss: 192.83796| val_0_rmse: 14.17665|  0:10:50s
epoch 123| loss: 190.02315| val_0_rmse: 13.9318 |  0:10:55s
epoch 124| loss: 192.58727| val_0_rmse: 13.98963|  0:11:00s
epoch 125| loss: 186.57819| val_0_rmse: 13.90416|  0:11:05s
epoch 126| loss: 186.30732| val_0_rmse: 13.96812|  0:11:11s
epoch 127| loss: 187.29026| val_0_rmse: 13.53748|  0:11:16s
epoch 128| loss: 184.76668| val_0_rmse: 13.67064|  0:11:21s
epoch 129| loss: 181.77995| val_0_rmse: 13.9749 |  0:11:26s
epoch 130| loss: 181.05186| val_0_rmse: 13.76821|  0:11:32s
epoch 131| loss: 176.42729| val_0_rmse: 13.5857 |  0:11:37s
epoch 132| loss: 179.98197| val_0_rmse: 13.56077|  0:11:42s
epoch 133| loss: 182.9385| val_0_rmse: 13.72906|  0:11:48s
epoch 134| loss: 172.7392| val_0_rmse: 14.60715|  0:11:53s
epoch 135| loss: 178.003 | val_0_rmse: 13.83521|  0:11:58s
epoch 136| loss: 175.55653| val_0_rmse: 13.97694|  0:12:03s
epoch 137| loss: 177.06445| val_0_rmse: 13.92962|  0:12:09s
epoch 138| loss: 167.83517| val_0_rmse: 13.7821 |  0:12:14s
epoch 139| loss: 169.87508| val_0_rmse: 13.25588|  0:12:19s
epoch 140| loss: 169.36964| val_0_rmse: 13.5962 |  0:12:24s
epoch 141| loss: 164.40447| val_0_rmse: 13.35495|  0:12:30s
epoch 142| loss: 169.35988| val_0_rmse: 13.37148|  0:12:35s
epoch 143| loss: 166.207 | val_0_rmse: 13.47107|  0:12:40s
epoch 144| loss: 168.1674| val_0_rmse: 13.59518|  0:12:46s
epoch 145| loss: 166.57512| val_0_rmse: 13.50187|  0:12:51s
epoch 146| loss: 162.43151| val_0_rmse: 13.2446 |  0:12:56s
epoch 147| loss: 159.00907| val_0_rmse: 13.34969|  0:13:01s
epoch 148| loss: 159.83744| val_0_rmse: 13.61918|  0:13:07s
epoch 149| loss: 163.40865| val_0_rmse: 13.09483|  0:13:12s
epoch 150| loss: 161.04529| val_0_rmse: 13.189  |  0:13:17s
epoch 151| loss: 155.21486| val_0_rmse: 13.51178|  0:13:22s
epoch 152| loss: 158.37917| val_0_rmse: 13.03585|  0:13:28s
epoch 153| loss: 156.05283| val_0_rmse: 13.49831|  0:13:33s
epoch 154| loss: 158.97804| val_0_rmse: 13.67741|  0:13:38s
epoch 155| loss: 161.46561| val_0_rmse: 12.67031|  0:13:44s
epoch 156| loss: 153.39363| val_0_rmse: 13.23321|  0:13:49s
epoch 157| loss: 153.41069| val_0_rmse: 13.36404|  0:13:54s
epoch 158| loss: 156.92573| val_0_rmse: 13.1252 |  0:13:59s
epoch 159| loss: 152.26647| val_0_rmse: 13.21074|  0:14:04s
epoch 160| loss: 149.52876| val_0_rmse: 13.03254|  0:14:10s
epoch 161| loss: 154.0659| val_0_rmse: 13.01705|  0:14:15s
epoch 162| loss: 156.71198| val_0_rmse: 12.8452 |  0:14:20s
epoch 163| loss: 152.8281| val_0_rmse: 13.68109|  0:14:25s
epoch 164| loss: 145.48611| val_0_rmse: 13.42505|  0:14:31s
epoch 165| loss: 149.75161| val_0_rmse: 13.22556|  0:14:36s
epoch 166| loss: 147.8944| val_0_rmse: 13.00452|  0:14:41s
epoch 167| loss: 150.54327| val_0_rmse: 12.80439|  0:14:47s
epoch 168| loss: 148.61184| val_0_rmse: 13.08436|  0:14:52s
epoch 169| loss: 146.27518| val_0_rmse: 12.79846|  0:14:57s
epoch 170| loss: 147.42827| val_0_rmse: 13.65605|  0:15:02s
epoch 171| loss: 146.40373| val_0_rmse: 13.09512|  0:15:08s
epoch 172| loss: 147.37322| val_0_rmse: 13.47233|  0:15:13s
epoch 173| loss: 146.36167| val_0_rmse: 12.97803|  0:15:18s
epoch 174| loss: 149.16081| val_0_rmse: 13.73019|  0:15:23s
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
epoch 175| loss: 143.64742| val_0_rmse: 13.28949|  0:15:28s
Early stopping occurred at epoch 175 with best_epoch = 155 and best_val_0_rmse = 12.67031
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib64/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(
[I 2024-07-12 01:49:20,982] Trial 39 finished with value: 12.67031096270709 and parameters: {'lr': 0.002885087811701696, 'n_steps': 8, 'gamma': 1.1214127880650957, 'n_independent': 3, 'n_shared': 1, 'momentum': 0.04550383398274609, 'weight_decay': 0.00017516533409597085, 'batch_size': 512, 'virtual_batch_size': 32}. Best is trial 21 with value: 11.422998560747544.
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
Best hyperparameters:  {'lr': 0.003125799102640622, 'n_steps': 6, 'gamma': 1.4272449155206772, 'n_independent': 2, 'n_shared': 3, 'momentum': 0.010619535976877537, 'weight_decay': 7.260077124984901e-05, 'batch_size': 512, 'virtual_batch_size': 64}
epoch 0  | loss: 8617.45801| val_0_rmse: 94.41712|  0:00:03s
epoch 1  | loss: 8633.28258| val_0_rmse: 94.27827|  0:00:07s
epoch 2  | loss: 8555.95984| val_0_rmse: 93.42304|  0:00:10s
epoch 3  | loss: 8225.78574| val_0_rmse: 89.67424|  0:00:14s
epoch 4  | loss: 7192.76404| val_0_rmse: 78.56631|  0:00:18s
epoch 5  | loss: 5029.67964| val_0_rmse: 59.17723|  0:00:21s
epoch 6  | loss: 2783.88249| val_0_rmse: 45.00668|  0:00:25s
epoch 7  | loss: 1396.92052| val_0_rmse: 34.30856|  0:00:29s
epoch 8  | loss: 1110.74535| val_0_rmse: 33.09319|  0:00:33s
epoch 9  | loss: 1044.21688| val_0_rmse: 32.19412|  0:00:36s
epoch 10 | loss: 1023.83733| val_0_rmse: 32.54897|  0:00:40s
epoch 11 | loss: 1011.95735| val_0_rmse: 32.1916 |  0:00:44s
epoch 12 | loss: 1007.33341| val_0_rmse: 31.61095|  0:00:47s
epoch 13 | loss: 993.28652| val_0_rmse: 31.64246|  0:00:51s
epoch 14 | loss: 992.69368| val_0_rmse: 31.39663|  0:00:55s
epoch 15 | loss: 980.38842| val_0_rmse: 31.21938|  0:00:59s
epoch 16 | loss: 972.30391| val_0_rmse: 31.43467|  0:01:03s
epoch 17 | loss: 963.97389| val_0_rmse: 31.38725|  0:01:06s
epoch 18 | loss: 955.94469| val_0_rmse: 31.47854|  0:01:10s
epoch 19 | loss: 946.12657| val_0_rmse: 31.13995|  0:01:13s
epoch 20 | loss: 928.75958| val_0_rmse: 31.1229 |  0:01:17s
epoch 21 | loss: 930.41496| val_0_rmse: 31.51865|  0:01:21s
epoch 22 | loss: 922.09311| val_0_rmse: 30.87536|  0:01:24s
epoch 23 | loss: 912.79897| val_0_rmse: 30.70374|  0:01:28s
epoch 24 | loss: 912.25428| val_0_rmse: 30.62976|  0:01:32s
epoch 25 | loss: 900.42846| val_0_rmse: 31.31819|  0:01:36s
epoch 26 | loss: 892.06705| val_0_rmse: 30.84714|  0:01:39s
epoch 27 | loss: 878.8101| val_0_rmse: 30.6133 |  0:01:43s
epoch 28 | loss: 863.57778| val_0_rmse: 30.2997 |  0:01:47s
epoch 29 | loss: 852.71873| val_0_rmse: 29.61317|  0:01:50s
epoch 30 | loss: 836.12066| val_0_rmse: 29.02911|  0:01:54s
epoch 31 | loss: 818.20967| val_0_rmse: 29.6094 |  0:01:58s
epoch 32 | loss: 804.86037| val_0_rmse: 29.41651|  0:02:02s
epoch 33 | loss: 784.44312| val_0_rmse: 28.93329|  0:02:05s
epoch 34 | loss: 773.1598| val_0_rmse: 28.03259|  0:02:09s
epoch 35 | loss: 753.15555| val_0_rmse: 28.0242 |  0:02:13s
epoch 36 | loss: 742.2733| val_0_rmse: 27.78613|  0:02:16s
epoch 37 | loss: 721.99156| val_0_rmse: 26.7737 |  0:02:20s
epoch 38 | loss: 713.14592| val_0_rmse: 26.66394|  0:02:23s
epoch 39 | loss: 687.96576| val_0_rmse: 26.26368|  0:02:27s
epoch 40 | loss: 656.76903| val_0_rmse: 26.04051|  0:02:31s
epoch 41 | loss: 636.16772| val_0_rmse: 25.59703|  0:02:35s
epoch 42 | loss: 607.42504| val_0_rmse: 25.49814|  0:02:39s
epoch 43 | loss: 583.24587| val_0_rmse: 24.98823|  0:02:42s
epoch 44 | loss: 560.44452| val_0_rmse: 24.5001 |  0:02:46s
epoch 45 | loss: 535.14662| val_0_rmse: 24.15976|  0:02:49s
epoch 46 | loss: 524.4843| val_0_rmse: 24.02164|  0:02:53s
epoch 47 | loss: 501.89321| val_0_rmse: 23.86411|  0:02:57s
epoch 48 | loss: 487.69707| val_0_rmse: 23.24777|  0:03:01s
epoch 49 | loss: 464.95873| val_0_rmse: 22.77889|  0:03:05s
epoch 50 | loss: 454.23422| val_0_rmse: 22.97964|  0:03:08s
epoch 51 | loss: 439.66871| val_0_rmse: 22.61072|  0:03:12s
epoch 52 | loss: 426.59639| val_0_rmse: 22.13867|  0:03:15s
epoch 53 | loss: 419.11426| val_0_rmse: 22.00842|  0:03:19s
epoch 54 | loss: 404.04625| val_0_rmse: 21.95442|  0:03:23s
epoch 55 | loss: 395.94346| val_0_rmse: 21.74316|  0:03:27s
epoch 56 | loss: 380.11016| val_0_rmse: 21.56275|  0:03:31s
epoch 57 | loss: 373.59147| val_0_rmse: 20.95056|  0:03:34s
epoch 58 | loss: 363.17376| val_0_rmse: 21.03675|  0:03:38s
epoch 59 | loss: 354.28371| val_0_rmse: 21.16067|  0:03:41s
epoch 60 | loss: 346.50619| val_0_rmse: 20.458  |  0:03:45s
epoch 61 | loss: 343.15006| val_0_rmse: 20.14475|  0:03:49s
epoch 62 | loss: 328.7658| val_0_rmse: 20.80044|  0:03:52s
epoch 63 | loss: 320.13172| val_0_rmse: 19.87026|  0:03:56s
epoch 64 | loss: 316.70211| val_0_rmse: 19.94611|  0:04:00s
epoch 65 | loss: 310.88395| val_0_rmse: 19.28171|  0:04:04s
epoch 66 | loss: 303.04702| val_0_rmse: 18.63947|  0:04:07s
epoch 67 | loss: 299.3779| val_0_rmse: 18.25332|  0:04:11s
epoch 68 | loss: 293.63779| val_0_rmse: 18.37887|  0:04:15s
epoch 69 | loss: 287.32816| val_0_rmse: 17.85945|  0:04:18s
epoch 70 | loss: 280.27766| val_0_rmse: 18.17348|  0:04:22s
epoch 71 | loss: 274.17845| val_0_rmse: 17.71381|  0:04:26s
epoch 72 | loss: 273.26843| val_0_rmse: 17.76501|  0:04:30s
epoch 73 | loss: 261.65482| val_0_rmse: 17.35564|  0:04:33s
epoch 74 | loss: 258.4072| val_0_rmse: 17.59774|  0:04:37s
epoch 75 | loss: 259.28167| val_0_rmse: 17.28506|  0:04:41s
epoch 76 | loss: 252.50067| val_0_rmse: 16.62301|  0:04:44s
epoch 77 | loss: 251.40586| val_0_rmse: 16.54918|  0:04:48s
epoch 78 | loss: 243.28204| val_0_rmse: 16.57965|  0:04:51s
epoch 79 | loss: 239.85266| val_0_rmse: 16.74497|  0:04:55s
epoch 80 | loss: 237.31578| val_0_rmse: 16.42737|  0:04:59s
epoch 81 | loss: 235.75218| val_0_rmse: 15.90341|  0:05:03s
epoch 82 | loss: 233.64786| val_0_rmse: 15.76048|  0:05:06s
epoch 83 | loss: 232.01876| val_0_rmse: 15.77423|  0:05:10s
epoch 84 | loss: 223.42667| val_0_rmse: 15.48862|  0:05:14s
epoch 85 | loss: 224.85969| val_0_rmse: 15.13133|  0:05:17s
epoch 86 | loss: 221.06898| val_0_rmse: 15.07728|  0:05:21s
epoch 87 | loss: 210.71675| val_0_rmse: 15.00967|  0:05:24s
epoch 88 | loss: 213.27914| val_0_rmse: 15.1649 |  0:05:28s
epoch 89 | loss: 208.88202| val_0_rmse: 15.06193|  0:05:32s
epoch 90 | loss: 210.16598| val_0_rmse: 14.98176|  0:05:36s
epoch 91 | loss: 205.09785| val_0_rmse: 15.12525|  0:05:39s
epoch 92 | loss: 201.52391| val_0_rmse: 14.43209|  0:05:43s
epoch 93 | loss: 201.00322| val_0_rmse: 14.70171|  0:05:46s
epoch 94 | loss: 202.43433| val_0_rmse: 14.75271|  0:05:50s
epoch 95 | loss: 196.8408| val_0_rmse: 14.46173|  0:05:53s
epoch 96 | loss: 200.54705| val_0_rmse: 14.20048|  0:05:57s
epoch 97 | loss: 194.59804| val_0_rmse: 14.22731|  0:06:01s
epoch 98 | loss: 183.22551| val_0_rmse: 14.29652|  0:06:05s
epoch 99 | loss: 189.02174| val_0_rmse: 13.88129|  0:06:08s
epoch 100| loss: 189.86539| val_0_rmse: 13.56297|  0:06:12s
epoch 101| loss: 186.33696| val_0_rmse: 14.14624|  0:06:15s
epoch 102| loss: 179.77001| val_0_rmse: 13.42707|  0:06:19s
epoch 103| loss: 182.72403| val_0_rmse: 13.45659|  0:06:23s
epoch 104| loss: 180.47614| val_0_rmse: 13.46191|  0:06:26s
epoch 105| loss: 174.99812| val_0_rmse: 13.58407|  0:06:30s
epoch 106| loss: 177.95026| val_0_rmse: 13.66223|  0:06:34s
epoch 107| loss: 175.75904| val_0_rmse: 13.98048|  0:06:38s
epoch 108| loss: 173.07912| val_0_rmse: 13.58656|  0:06:41s
epoch 109| loss: 168.62796| val_0_rmse: 13.42972|  0:06:45s
epoch 110| loss: 169.8706| val_0_rmse: 13.48013|  0:06:48s
epoch 111| loss: 167.66521| val_0_rmse: 15.1004 |  0:06:52s
epoch 112| loss: 164.04054| val_0_rmse: 14.97358|  0:06:55s
epoch 113| loss: 168.90035| val_0_rmse: 14.04696|  0:06:59s
epoch 114| loss: 162.38003| val_0_rmse: 13.16232|  0:07:03s
epoch 115| loss: 160.30264| val_0_rmse: 13.48676|  0:07:06s
epoch 116| loss: 160.79205| val_0_rmse: 12.85487|  0:07:10s
epoch 117| loss: 157.78414| val_0_rmse: 13.1572 |  0:07:14s
epoch 118| loss: 160.29442| val_0_rmse: 13.13185|  0:07:17s
epoch 119| loss: 158.94078| val_0_rmse: 13.18801|  0:07:21s
epoch 120| loss: 155.52709| val_0_rmse: 12.99246|  0:07:24s
epoch 121| loss: 152.53618| val_0_rmse: 12.88683|  0:07:28s
epoch 122| loss: 150.85702| val_0_rmse: 13.07765|  0:07:32s
epoch 123| loss: 146.93209| val_0_rmse: 12.84794|  0:07:36s
epoch 124| loss: 148.10728| val_0_rmse: 12.83669|  0:07:39s
epoch 125| loss: 150.17785| val_0_rmse: 12.99362|  0:07:43s
epoch 126| loss: 147.18098| val_0_rmse: 12.74263|  0:07:46s
epoch 127| loss: 147.95308| val_0_rmse: 12.7292 |  0:07:50s
epoch 128| loss: 148.39077| val_0_rmse: 12.93636|  0:07:53s
epoch 129| loss: 144.56896| val_0_rmse: 13.12292|  0:07:57s
epoch 130| loss: 144.18946| val_0_rmse: 14.06115|  0:08:01s
epoch 131| loss: 144.95334| val_0_rmse: 12.41594|  0:08:05s
epoch 132| loss: 139.25144| val_0_rmse: 12.63276|  0:08:08s
epoch 133| loss: 139.47741| val_0_rmse: 12.58587|  0:08:12s
epoch 134| loss: 137.85617| val_0_rmse: 12.7955 |  0:08:15s
epoch 135| loss: 136.69459| val_0_rmse: 12.68115|  0:08:19s
epoch 136| loss: 134.72263| val_0_rmse: 12.69846|  0:08:22s
epoch 137| loss: 132.83824| val_0_rmse: 12.4586 |  0:08:26s
epoch 138| loss: 130.60977| val_0_rmse: 12.34758|  0:08:30s
epoch 139| loss: 133.03096| val_0_rmse: 12.25852|  0:08:34s
epoch 140| loss: 131.22642| val_0_rmse: 12.69591|  0:08:37s
epoch 141| loss: 132.92443| val_0_rmse: 12.33032|  0:08:41s
epoch 142| loss: 132.40737| val_0_rmse: 12.71162|  0:08:44s
epoch 143| loss: 128.63108| val_0_rmse: 12.63638|  0:08:48s
epoch 144| loss: 132.74353| val_0_rmse: 12.50905|  0:08:51s
epoch 145| loss: 124.48422| val_0_rmse: 12.26163|  0:08:55s
epoch 146| loss: 124.726 | val_0_rmse: 12.30836|  0:08:59s
epoch 147| loss: 124.83269| val_0_rmse: 12.19707|  0:09:03s
epoch 148| loss: 122.87178| val_0_rmse: 12.41319|  0:09:06s
epoch 149| loss: 123.69097| val_0_rmse: 12.19121|  0:09:10s
epoch 150| loss: 125.41898| val_0_rmse: 12.2493 |  0:09:14s
epoch 151| loss: 123.81537| val_0_rmse: 12.24547|  0:09:17s
epoch 152| loss: 123.32942| val_0_rmse: 12.29798|  0:09:21s
epoch 153| loss: 119.30275| val_0_rmse: 12.04627|  0:09:24s
epoch 154| loss: 119.36325| val_0_rmse: 11.99349|  0:09:28s
epoch 155| loss: 120.31798| val_0_rmse: 12.02239|  0:09:32s
epoch 156| loss: 119.80515| val_0_rmse: 12.02351|  0:09:36s
epoch 157| loss: 118.07829| val_0_rmse: 11.96173|  0:09:39s
epoch 158| loss: 114.64421| val_0_rmse: 12.10371|  0:09:43s
epoch 159| loss: 117.27586| val_0_rmse: 12.33345|  0:09:46s
epoch 160| loss: 115.77976| val_0_rmse: 12.30999|  0:09:50s
epoch 161| loss: 116.67596| val_0_rmse: 12.42488|  0:09:53s
epoch 162| loss: 116.77335| val_0_rmse: 12.11696|  0:09:57s
epoch 163| loss: 114.43713| val_0_rmse: 12.17359|  0:10:01s
epoch 164| loss: 113.17436| val_0_rmse: 12.14714|  0:10:05s
epoch 165| loss: 118.18083| val_0_rmse: 11.748  |  0:10:08s
epoch 166| loss: 114.30082| val_0_rmse: 11.86685|  0:10:12s
epoch 167| loss: 110.72943| val_0_rmse: 11.88143|  0:10:15s
epoch 168| loss: 114.7381| val_0_rmse: 12.04776|  0:10:19s
epoch 169| loss: 115.35736| val_0_rmse: 11.57972|  0:10:22s
epoch 170| loss: 107.65216| val_0_rmse: 11.50205|  0:10:26s
epoch 171| loss: 112.72971| val_0_rmse: 11.86559|  0:10:30s
epoch 172| loss: 111.27311| val_0_rmse: 11.61258|  0:10:34s
epoch 173| loss: 110.10432| val_0_rmse: 12.12808|  0:10:37s
epoch 174| loss: 114.66704| val_0_rmse: 11.59266|  0:10:41s
epoch 175| loss: 110.17636| val_0_rmse: 11.94384|  0:10:44s
epoch 176| loss: 108.52878| val_0_rmse: 11.7009 |  0:10:48s
epoch 177| loss: 110.28905| val_0_rmse: 11.75346|  0:10:51s
epoch 178| loss: 106.63654| val_0_rmse: 11.54644|  0:10:55s
epoch 179| loss: 109.37137| val_0_rmse: 11.49401|  0:10:59s
epoch 180| loss: 109.21728| val_0_rmse: 11.58172|  0:11:03s
epoch 181| loss: 104.84037| val_0_rmse: 11.53145|  0:11:06s
epoch 182| loss: 105.49577| val_0_rmse: 11.79831|  0:11:10s
epoch 183| loss: 105.35321| val_0_rmse: 11.73018|  0:11:13s
epoch 184| loss: 109.7641| val_0_rmse: 12.14215|  0:11:17s
epoch 185| loss: 102.52504| val_0_rmse: 11.8289 |  0:11:20s
epoch 186| loss: 111.33172| val_0_rmse: 11.76749|  0:11:24s
epoch 187| loss: 103.27834| val_0_rmse: 11.66304|  0:11:28s
epoch 188| loss: 102.04151| val_0_rmse: 11.75663|  0:11:32s
epoch 189| loss: 100.57662| val_0_rmse: 11.74336|  0:11:35s
epoch 190| loss: 101.13226| val_0_rmse: 11.81259|  0:11:39s
epoch 191| loss: 101.87198| val_0_rmse: 11.69618|  0:11:42s
epoch 192| loss: 105.15449| val_0_rmse: 11.423  |  0:11:46s
epoch 193| loss: 100.95403| val_0_rmse: 11.51051|  0:11:49s
epoch 194| loss: 100.13005| val_0_rmse: 11.81464|  0:11:53s
epoch 195| loss: 105.53982| val_0_rmse: 11.59934|  0:11:57s
epoch 196| loss: 100.94519| val_0_rmse: 11.48361|  0:12:01s
epoch 197| loss: 99.23211| val_0_rmse: 11.58652|  0:12:04s
epoch 198| loss: 100.96146| val_0_rmse: 11.48489|  0:12:08s
epoch 199| loss: 99.46053| val_0_rmse: 11.64768|  0:12:11s
Stop training because you reached max_epochs = 200 with best_epoch = 192 and best_val_0_rmse = 11.423
/scratch/izar/mmorvan/EnergyEfficiencyPredictionMatthew/text/venv/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
Train MSE: 68.21038670899836
Validation MSE: 130.48489611884048
Test MSE: 904.4071677694869